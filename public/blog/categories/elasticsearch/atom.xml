<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ElasticSearch | OpenTable Tech UK Blog]]></title>
  <link href="http://tech.opentable.co.uk/blog/categories/elasticsearch/atom.xml" rel="self"/>
  <link href="http://tech.opentable.co.uk/"/>
  <updated>2015-02-07T14:21:51+00:00</updated>
  <id>http://tech.opentable.co.uk/</id>
  <author>
    <name><![CDATA[OpenTable]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On Strongly Typed Logging]]></title>
    <link href="http://tech.opentable.co.uk/blog/2015/01/23/on-strongly-typed-logging/"/>
    <updated>2015-01-23T13:13:13+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2015/01/23/on-strongly-typed-logging</id>
    <content type="html"><![CDATA[<p>Logging is a crucial element of monitoring highly available systems. It allows not only to find out about errors but also quickly identify their cause. Logs are often used to generate metrics that help business and engineering make informative decisions on future development directions.</p>

<p>At OpenTable we have a central logging infrastructure, that means all logs are stored in the same shared database (ElasticSearch for us). And everybody can access any logs they want without having very specialized knowledge (thanks Kibana!).</p>

<p>ElasticSearch, though living in a NoSQL world, is not actually a schema-free database. Sure, you do not need to provide schema to it but instead ES will infer schema for you from documents you send to it. This is very similar to type inference you can find in many programming languages. You do not need to specify type of field, but if you later on try to assign inappropriate value to it you will get an exception.</p>

<p>This trait of our database goes all the way to the root of our logging system design. Let me explain why I say that we have &lsquo;strongly typed logs&rsquo;.</p>

<h2>In The Beginning There Was String</h2>

<p>Before centralization we just logged a single message along with its importance. In code it looked something like:
<code>
logger.ERROR(“Kaboom!”)
</code>
which resulted in logline on disk having timestamp, severity and message.
<code>
{2014-10-10T07:33:04Z [ERROR] Kaboom!}
</code>
That worked pretty well. As time passed we often started making log messages more generic to hold relevant data:
<code>
logger.INFO(string.Format(“Received {0} from {1}. Status: {2}. Took {3}”, httpMethod, sourceIp, statusCode, durationms));
</code></p>

<p>When we decided to centralize logs we moved the same logs from local disk to a central database. Suddenly things that used to live on single server in a file called &lsquo;application.log&rsquo; become part of one huge lump of data. Instead of easing access to logs they were really hard to filter, without even speaking about aggregation, or any simple form of operations to find the source of the problem. ElasticSearch is really good at free text searching, but frankly speaking FTS is never as precise as a good filter.</p>

<h2>Then There Was Dictionary Of Strings</h2>

<p>Wherever there is problem there is also a solution. So we changed the way our logging works. We created a custom logger and started sending logs more like documents than single string.
<code>
customLogger.send(‘info’, new Dictionary&lt;string, string&gt; {
{‘method’, httpMethod.ToString()},
{‘sourceIp’, sourceIp.ToString()},
{‘statusCode’, statusCode.ToString()},
{‘duration’, durationms.ToString()},
{‘requestId’, requestId.ToString()},
{‘service’, ‘myservice’}
{‘message’, string.Format(“Received {0} from {1}. Status: {2}. Took {3}”, httpMethod, sourceIp, statusCode, durationms)}
}
</code></p>

<p><strong>That helped a lot.</strong></p>

<p>You might wonder why we serialized everything to string? The answer is ElasticSearch mapping as I described above. Mapping, once it is inferred, cannot be changed. So from time to time we used to have conflicts (e.g. one application logging requestId as number, other as guid). Those conflicts were costly &ndash; logs were lost &ndash; so we simply applied the simplest solution available and serialized everything.</p>

<p>Now filtering was working fine. We were even able to group requests based on a single field and count them. You cannot imagine how useful it is to simply count the different status codes returned by a service. Also you may have noticed we introduced some extra fields like &lsquo;service&rsquo; which helped us group logs coming from a single application. We did the same with hostname etc.</p>

<p>With this easy success our appetite has grown and we wanted to log more. And being lazy programmers we found a way to do it quickly so our logs often included just relevant objects.
<code>
customLogger.log(‘info’, request)
customLogger.log(‘error’, exception)
</code></p>

<p>Our custom logging library did all the serialization for us. This worked really well. Now we were actually logging whole things that mattered without having to worry about serialization at all. What&rsquo;s even better, whenever the object in question changed (e.g. a new field was added to request), it was automagically logged.</p>

<p>However one thing was still missing. We really wanted to see performance of our application in real time or do range queries (e.g. &ldquo;show me all requests that have 5xx status code&rdquo;). We also were aware that both ES and Kibana can deliver it but our logging is not yet good enough.</p>

<h2>Strongly Typed Logs</h2>

<p>So we looked at our logging and infrastructure and at what needs to be done to allow different types of fields to live in ElasticSearch. And you can imagine that it was a pretty simple fix; we just started using types. Each log format was assigned its own type. This type was then used by ElasticSearch to put different logs into separate buckets with separate mapping. The type is equivalent in meaning to classes in OO programming. If we take this comparison further then each log entry would be an object in OO programming. ElasticSearch supports searches across multiple types, which is very convenient when you don&rsquo;t know what you are looking for. On the other hand, when you know, you can limit your query to single type and take advantage of fields types.</p>

<p>It was a big application change as we needed to completely change our transport mechanism to LogStash. We started with Gelf and switched to Redis, which allowed us to better control format of our logs.</p>

<p>We also agreed on a first standard. The standard defined that type will consist of three parts:
<code>
&lt;serviceName&gt;-&lt;logName&gt;-&lt;version&gt;
</code>
This ensures that each team can use any logs they want to (thus serviceName). Each log will have its own format (thus logName). But they can also change in the future (thus version). One little word of caution, ES doesn&rsquo;t like dots in type name, so don&rsquo;t use them.</p>

<p>So our logs look now like this:
<code>
customLogger.log(new RequestLog {
Request = request,
Headers = headers,
Status = status})
</code></p>

<p>RequestLog is responsible for providing valid type to the logging library.</p>

<p>With sending serialized objects as logs and assigning each class unique type our logs have become strongly typed.</p>

<p>We are already couple steps further down the path of improving our logs. We standardized some common fields and logtypes. That, however, is a completely different tale. ​</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interacting with ElasticSearch using Hubot]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/11/08/interacting-with-elasticsearch-using-hubot/"/>
    <updated>2014-11-08T10:32:42+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/11/08/interacting-with-elasticsearch-using-hubot</id>
    <content type="html"><![CDATA[<p>At OpenTable, we use a few <a href="">ElasticSearch</a> clusters. Our aim was to be able to interact with our ElasticSearch clusters via <a href="http://www.hipchat.com">HipChat</a> so that we could troubleshoot easily and without having to log into our VPN. We already use <a href="http://hubot.github.com">Hubot</a> as part of our systems workflow, so it made sense to be able to interact with ElasticSearch with it.</p>

<h3>Setting a cluster alias</h3>

<p>When a pager wakes me at 3am, I really do not want to have to try and type the cluster URL into my mobile hipchat client. So the first thing that was added to the script was the ability to give a cluster an alias.</p>

<p><code>
elasticsearch add alias my-test-alias http://my-cluster.com:9200
</code></p>

<p><img src="/images/posts/elasticsearch-add-alias.png" alt="add-alias" /></p>

<p>This allows us to use that alias for all commands going forward. Please note that you can remove and query aliases as well:</p>

<p><code>
elasticsearch show aliases
</code></p>

<p><img src="/images/posts/elasticsearch-show-aliases.png" alt="show-alias" /></p>

<p><code>
elasticsearch clear alias my-test-alias
</code></p>

<p><img src="/images/posts/elasticsearch-clear-alias.png" alt="clear-alias" /></p>

<h3>Using the ElasticSearch Cat API</h3>

<p>A lot of what we do with ElasticSearch can be done via the <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat.html">cat</a> API. This has proved extremely useful to get node status, cluster health and index status.</p>

<h4>Cat Health</h4>

<p>As documented <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-health.html#cat-health">here</a></p>

<p><code>
elasticsearch cluster health my-test-alias
</code></p>

<h4>Cat Nodes</h4>

<p>As documented <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-nodes.html">here</a></p>

<p><code>
elasticsearch cat nodes my-test-alias
</code></p>

<h4>Cat Indices</h4>

<p>As documented <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-indices.html">here</a></p>

<p><code>
elasticsearch cat indexes my-test-alias
</code></p>

<h4>Cat Allocation</h4>

<p>As documented <a href="">here</a></p>

<p><code>
elasticsearch cat allocation my-test-alias
</code></p>

<h3>Getting the Cluster Settings</h3>

<p>Sometimes when we are rebalancing shards or recycling nodes, we want to be able to control the cluster settings. By using the cluster settings API, can have some insight into the settings currently set on the cluster:</p>

<p><code>
elasticsearch cluster settings my-test-alias
</code>
More information about the cluster settings API can be found <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-update-settings.html#cluster-settings">here</a></p>

<h3>Getting the Settings for an Index</h3>

<p>Should we want to start to understand the actual settings that are attributed to an index, we can use the Cat Indices settings API. More information can be found <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-get-settings.html">here</a></p>

<p><code>
elasticsearch index settings my-test-alias my-index-name-2014-11-07
</code></p>

<h3>Clearing the cluster Cache</h3>

<p>The last piece of the puzzle we are able to do, is to clear the cache of the ElasticSearch cluster. This can be done as follows:</p>

<p><code>
hubot elasticsearch clear cache my-test-alias
</code></p>

<h3>Where can I find the code?</h3>

<p>The code is available on <a href="https://github.com/stack72/hubot-elasticsearch">github</a> or also as an <a href="https://www.npmjs.org/package/hubot-elasticsearch">NPM package</a>. Please feel free to send PRs or create issues on our repository. All feedback is useful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Counting in ElasticSearch]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/09/11/counting-in-elastic-search/"/>
    <updated>2013-09-11T15:48:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/09/11/counting-in-elastic-search</id>
    <content type="html"><![CDATA[<blockquote><p>Counting is the religion of this generation it is its hope and its salvation.
Gertrude Stein</p></blockquote>

<p>In our NeverEnding quest to provide better experience to the users we utilise user behaviour logs to influence future results. One particular case is restaurant popularity, which is indicated by many factors, for example how often it is searched and viewed.</p>

<p>In this blog post we will look into multiple ways of counting documents in Elastic Search which is crucial for this kind of activity. All examples here are provided using Elastic Search HTTP interface and code examples implemented with <a href="https://github.com/Yegoroff/PlainElastic.Net">PlainElastic.NET</a> are <a href="https://gist.github.com/gondar/6320578">available here</a></p>

<p>Before we make a deep dive into Elastic Search Counting options let&rsquo;s define our expectations:
<code>
So that I can order restaurants by those that are most searched
As a potential diner
I want the most searched statistics from the logs to be part of the search database
</code>
Okay, that&rsquo;s not exactly how our story was defined but as we don&rsquo;t want to discuss the whole search infrastructure here, let&rsquo;s assume this is sufficient.</p>

<p>Because we are eager engineers, we will quickly build some mock data against which to test our assumptions. Our restaurant name search logs look something like this:
```
{</p>

<pre><code>"RestaurantId" : 2,
"RestaurantName" : "Restaurant Brian",
"DateTime" : "2013-08-16T15:13:47.4833748+01:00"
</code></pre>

<p>}
<code>
So we will populate our mock database with appropriate commands and check that all is in place:
</code>
curl <a href="http://localhost:9200/store/item/">http://localhost:9200/store/item/</a> -XPOST -d &lsquo;{&ldquo;RestaurantId&rdquo;:2,&ldquo;RestaurantName&rdquo;:&ldquo;Restaurant Brian&rdquo;,&ldquo;DateTime&rdquo;:&ldquo;2013-08-16T15:13:47.4833748+01:00&rdquo;}&rsquo;
curl <a href="http://localhost:9200/store/item/">http://localhost:9200/store/item/</a> -XPOST -d &lsquo;{&ldquo;RestaurantId&rdquo;:1,&ldquo;RestaurantName&rdquo;:&ldquo;Restaurant Cecil&rdquo;,&ldquo;DateTime&rdquo;:&ldquo;2013-08-16T15:13:47.4833748+01:00&rdquo;}&rsquo;
curl <a href="http://localhost:9200/store/item/">http://localhost:9200/store/item/</a> -XPOST -d &lsquo;{&ldquo;RestaurantId&rdquo;:1,&ldquo;RestaurantName&rdquo;:&ldquo;Restaurant Cecil&rdquo;,&ldquo;DateTime&rdquo;:&ldquo;2013-08-16T15:13:47.4833748+01:00&rdquo;}&rsquo;
curl <a href="http://localhost:9200/store/item/_search?q=*&amp;pretty">http://localhost:9200/store/item/_search?q=*&amp;pretty</a>
<code>
Our expected output is a count of documents for each restaurant. For example:
</code>
{</p>

<pre><code>"Restaurant Brian" : 1
"Restaurant Cecil" : 2
</code></pre>

<p>}
```
There are three ways this can be achieved in Elastic Search; using count API (which seems like the most obvious way), a search with type set to count, or using facets to generate counts of all objects grouped by given property. Let&rsquo;s compare them:</p>

<h3>Count API</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/count/">See documentation here</a>)
```
curl -XPOST <a href="http://localhost:9200/store/item/_count">http://localhost:9200/store/item/_count</a> -d &lsquo;{</p>

<pre><code>"field": {
    "RestaurantName": {
        "query": "Restaurant Cecil",
        "default_operator": "AND"
    }
}
</code></pre>

<p>}&lsquo;
{</p>

<pre><code>"count":2,
"_shards":{"total":5,"successful":5,"failed":0}
</code></pre>

<p>}
```
Count is nice little feature which solves our problem. However, if we need count for multiple restaurants we need to execute similar queries multiple times, which may hugely influence both performance of our query and usage of our ElasticSeach cluster.</p>

<h3>Search</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/search/search-type/">See documentation here</a>)
```
curl -XPOST <a href="http://localhost:9200/store/item/_search?search_type=count">http://localhost:9200/store/item/_search?search_type=count</a> -d &lsquo; {</p>

<pre><code>"query": {
    "field": {
        "RestaurantName": {
            "query": "Restaurant Cecil",
            "default_operator": "AND"
        }
    }
}
</code></pre>

<p>}&lsquo;
{</p>

<pre><code>"took":5,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},
"hits": {
    "total": 2,
    "max_score": 0.0,
    "hits":[]
}
</code></pre>

<p>}
```
Using search type set to count is the same as executing a search request with size set to zero, but it&rsquo;s internally optimised for performance. The nice thing about search is that we can use multi_search interface to execute many count queries at once.</p>

<p>On the other hand, the query still will be executed multiple times, so it is only feasible if we want to get popularity for a small subset of all the restaurants we have.</p>

<p>Comparing two previous requests highlights that the query language is slightly different. The DSL for <em>count API</em> is basically the same as for the <em>search API</em>, but you are immediately inside the &lsquo;query&rsquo; part. That inconsistency on the ElasticSearch side is only a minor inconvenience.</p>

<h3>Facets</h3>

<p>(<a href="http://www.elasticsearch.org/guide/reference/api/search/facets/">See documentation here</a>)</p>

<p>```
curl -XPOST <a href="http://localhost:9200/store/item/_search?search_type=count">http://localhost:9200/store/item/_search?search_type=count</a> -d &lsquo;
{</p>

<pre><code>"query": {
    "match_all": {

    }
},
"facets": {
    "ItemsPerCategoryCount": {
        "terms": {
            "field": "RestaurantId",
            "size": 100
        }
    }
}
</code></pre>

<p>}&lsquo;
{</p>

<pre><code>"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":132,"max_score":0.0,"hits":[]},
"facets": {
    "ItemsPerCategoryCount": {
        "_type": "terms",
        "missing":0,
        "total":3,
        "other":0,
        "terms": [
            {"term": 2, "count": 1},
            {"term": 1, "count":2}
        ]
    }
}
</code></pre>

<p>}
```</p>

<p>Facets is a means to obtain grouping by a given field together with count in a group. It is designed to ease creation of filters which are often naturally part of search results interface.</p>

<p>This is nice feature which grabs for us all counts grouped by given field. That&rsquo;s more then we need if we only care for a count of single type, but it&rsquo;s invaluable if you want to have counts for all terms in a field. Also note that we are using search type count again, but facets work equally well for all types of searches including those which actually return results.</p>

<p>In the example above we used &lsquo;RestaurantId&rsquo; field instead of restaurant name, as this field is not analysed. If we used restaurant name it would give us facets for each term e.g. [{&ldquo;term&rdquo;: &ldquo;Restaurant&rdquo;, &ldquo;count&rdquo;: 3}, {&ldquo;term&rdquo;:&ldquo;Cecil&rdquo;, &ldquo;count&rdquo;:2},{&ldquo;term&rdquo;:&ldquo;Brian&rdquo;, &ldquo;count&rdquo;:1}], which is not what we exactly want.</p>

<h3>Conclusion</h3>

<p>It&rsquo;s hard to discuss which one is better. Count API is slightly faster then Search of type count. On the other hand search is more flexible, and its queries are consistent with normal search queries. Facets is a different beast altogether as it always grabs all the results. Still, it&rsquo;s fun that ElasticSearch is elastic in this aspect giving us variety of approaches.</p>

<p>We are really curious about your experiences in ElasticSearch. If you have any questions, proposals or comments feel free to <a href="mailto:mbazydlo@opentable.com">email me</a>.</p>

<h3>Acknowledgement</h3>

<p>This blog post benefited thanks to invaluable comments from my team (Andrew Metcalfe, Michael Wallett and Tom Harvey), <a href="https://github.com/Yegoroff/PlainElastic.Net">PlainElastic.Net</a> author (Yegoroff) and <a href="https://github.com/pbazydlo">my brother</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Vagrant to work with ElasticSearch on your local machine]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/08/05/using-vagrant-to-work-with-elasticsearch-on-your-local-machine/"/>
    <updated>2013-08-05T08:45:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/08/05/using-vagrant-to-work-with-elasticsearch-on-your-local-machine</id>
    <content type="html"><![CDATA[<p>Recently, I have started to work a lot more with <a href="http://www.vagrantup.com/">Vagrant</a> as a tool for creating a standard development environment across my team. This essentially means that regardless what the developers' machine is set up or running as, they can still reproduce the same environment as their colleagues just by entering a command.</p>

<p>Configuration managgement is something we have had to embrace to help us maintain an ever changing world of technologies. The hardest thing is knowing what we actually have to build in these environments. We use Vagrant to help us understand this. The simple flow is as follows:</p>

<ul>
<li>Developer starts a new project</li>
<li>Developer creates a Vagrantfile to spin up a local VM</li>
<li>Vagrantfile gets iterated on as the development process goes forward</li>
</ul>


<p>Once the developer understands what they need to actually run their software, we would then go about creating an environment to which this software will actually be deployed for end-to-end testing. I won&rsquo;t go any further into the details of our Vagrant flow in this post, if you want to read more about how to get started with Vagrant, then I would suggest reading <a href="http://shop.oreilly.com/product/0636920026358.do">Vagrant Up and Running</a> by <a href="https://twitter.com/mitchellh">Mitchell Hashimoto</a>.</p>

<h2>Vagrant and ElasticSearch</h2>

<p>Whilst reviewing a book on <a href="http://www.elasticsearch.org/">ElasticSearch</a>, I noticed how simple the instructions were to get up and running with ElasticSearch. Please note, that there are already lots of Puppet modules for configuring ElasticSearch on <a href="http://forge.puppetlabs.com/modules?q=elasticsearch">Puppetlabs Forge</a>. This post only talks about how I was able to quickly spin up some local instances. I didn&rsquo;t want to manually do this, so I decided to use Vagrant (and Puppet) to take care of it for me. The instructions can be summarised as follows:</p>

<ul>
<li>Download and install the JavaSDK</li>
<li>Download the specific ElasticSearch package</li>
<li>Install ElasticSearch</li>
<li>Download and install curl (to be able to interact with ElasticSearch)</li>
<li>Make sure the service is started</li>
</ul>


<p>I hate doing this manually. Luckily, with the correct script, I am able to automate all of this as follows:</p>

<pre><code>Vagrant.configure("2") do |config|
    config.vm.box = "Ubuntu precise 64 VMWare"
    config.vm.box_url = "http://files.vagrantup.com/precise64_vmware.box"
    config.vm.network :forwarded_port, guest: 9200, host: 9200
    config.vm.provision :puppet do |puppet|
        puppet.module_path = '../setup/modules'
        puppet.manifests_path = '../setup/manifests'
        puppet.manifest_file = 'default.pp'
        puppet.options = '--verbose --debug'
    end
end
</code></pre>

<p>Essentially, this script says to create a clone of a VM from a predefined box, forward port 9200 on the vm to 9200 on my local machine and then provision the server using Puppet. The Puppet script works as follows:</p>

<pre><code>exec { "apt-get-update":
    command =&gt; "/usr/bin/apt-get update",
}

package {'curl':
    provider =&gt; apt,
    ensure   =&gt; latest,
    require  =&gt; Exec['apt-get-update']
}

class {'elasticsearch':
    version =&gt; '0.90.0',
    require =&gt; Exec['apt-get-update'],
}
</code></pre>

<p>This defines that the command apt-get-update gets applied (due to both the class and the package requiring it) and then will install curl and ElasticSearch in no particular order. Once the script runs, I will be able to open a browser on my local machine, go to <a href="http://localhost:9200">http://localhost:9200</a> and see the newly provisioned ElasticSearch node. The result of the JSON was something similar to this:</p>

<pre><code>{
    "ok" : true,
    "status" : 200,
    "name" : "Gibborim",
    "version" : {
        "number" : "0.90.0",
        "snapshot_build" : false,
    },
    "tagline" : "You Know, for Search"
}
</code></pre>

<p>By entering the URL, &lsquo;**<a href="http://localhost:9200/_cluster/health?pretty**">http://localhost:9200/_cluster/health?pretty**</a>&rsquo;, you can see the state of the ElasticSearch cluster. It should show something like this:</p>

<pre><code>{
    "cluster_name" : "elasticsearch",
    "status" : "yellow",
    "timed_out" : false,
    "number_of_nodes" : 1,              
    "number_of_data_nodes" : 1,         
    "active_primary_shards" : 5,        
    "active_shards" : 5,                
    "relocating_shards" : 0,            
    "initializing_shards" : 0,          
    "unassigned_shards" : 5             
}
</code></pre>

<p>I wanted to be able to provision multiple nodes and then let them create a cluster. I was able to take the existing Vagrantfile and then using the multi-environment features of Vagrant. This created a new Vagrantfile as follows:</p>

<pre><code>Vagrant::Config.run do |config|
    config.vm.box = "Ubuntu precise 64 VMWare"
    config.vm.box_url = "http://files.vagrantup.com/precise64_vmware.box"

    config.vm.define "es1" do |es1|
        es1.vm.network :hostonly, "192.168.1.10"
        es1.vm.provision :puppet do |puppet|
            puppet.module_path = '../setup/modules'
            puppet.manifests_path = '../setup/manifests'
            puppet.manifest_file = 'default.pp'
            puppet.options = '--verbose --debug'
        end
    end

    config.vm.define "es2" do |es2|
        es2.vm.network :hostonly, "192.168.1.11"
        es2.vm.provision :puppet do |puppet|
            puppet.module_path = '../setup/modules'
            puppet.manifests_path = '../setup/manifests'
            puppet.manifest_file = 'default.pp'
            puppet.options = '--verbose --debug'
        end
    end

    config.vm.define "es3" do |es3|
        es3.vm.network :hostonly, "192.168.1.12"
        es3.vm.provision :puppet do |puppet|
            puppet.module_path = '../setup/modules'
            puppet.manifests_path = '../setup/manifests'
            puppet.manifest_file = 'default.pp'
            puppet.options = '--verbose --debug'
        end
    end
end
</code></pre>

<p>This effectively tells Vagrant to create three instances of ElasticSearch using the Puppet configuration (as above). Each ElasticSearch node is given its own IP. Thanks to ElasticSearch using Multicast and Unicast discovery, it is able to find other nodes on the network and create a cluster. By running a similar url as before, &lsquo;**<a href="http://192.168.1.10:9200/_cluster/health?pretty**">http://192.168.1.10:9200/_cluster/health?pretty**</a>&rsquo;, we can now see that the cluster looks as follows:</p>

<pre><code>{
    "cluster_name" : "elasticsearch",
    "status" : "green",
    "timed_out" : false,
    "number_of_nodes" : 3,              
    "number_of_data_nodes" : 3,         
    "active_primary_shards" : 5,        
    "active_shards" : 15,                
    "relocating_shards" : 0,            
    "initializing_shards" : 0,          
    "unassigned_shards" : 0             
}
</code></pre>

<p>Using this method, we can continue to spin up as many instances as we need to replicate different scenarios or testing conditions. Vagrant has made this very easy to do. If you want a copy of the Vagrantfiles and Puppet modules to try this yourself, then you can find them on my <a href="https://github.com/stack72/vagrant-examples/tree/master/elasticsearch">github repository</a>. The scripts are available under the <a href="http://opensource.org/licenses/MIT">MIT</a> license.</p>
]]></content>
  </entry>
  
</feed>
