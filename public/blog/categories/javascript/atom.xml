<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: JavaScript | OpenTable Tech UK Blog]]></title>
  <link href="http://tech.opentable.co.uk/blog/categories/javascript/atom.xml" rel="self"/>
  <link href="http://tech.opentable.co.uk/"/>
  <updated>2015-02-07T14:21:51+00:00</updated>
  <id>http://tech.opentable.co.uk/</id>
  <author>
    <name><![CDATA[OpenTable]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Explaining Flux architecture with macgyver.js]]></title>
    <link href="http://tech.opentable.co.uk/blog/2015/01/01/explaining-flux-architecture-with-macgyver-dot-js/"/>
    <updated>2015-01-01T15:33:46+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2015/01/01/explaining-flux-architecture-with-macgyver-dot-js</id>
    <content type="html"><![CDATA[<h2>What is Flux?</h2>

<p><a href="https://github.com/facebook/flux">Flux</a> is an application architectural pattern developed by Facebook. It was developed to solve some of the complexities of the MVC pattern when used at scale by favouring a uni-directional approach. It is a pattern and not a technology or framework.</p>

<p><img src="/images/posts/mvc-scale.png" alt="MVC scale issue" /></p>

<p>When applications that use the model-view-controller (MVC) pattern at any scale it becomes difficult to maintain consistent data across multiple views. In particular the case whereby flow between models and views is not uni-directional and may require increasing logic to maintain parity between views when model data is updated. Facebook hit this issue several times and in particular with their unseen count (an incremented value of unseen messages which is updated by several UI chat components). It wasn&rsquo;t until they realised that the MVC pattern accomodated the complexity that they stepped back from the problem and addressed the architecture.</p>

<p>Flux is intentionally unidirectional.</p>

<p><img src="/images/posts/flux.png" alt="flux" /></p>

<p>Key to this architecture is the dispatcher. The dispatcher forms the gatekeeper that all actions must go through. When a view, or views, wish to do something they fire an action which the dispatcher correctly routes via registered callbacks made by the stores.</p>

<p>Stores are responsible for the data and respond to callbacks from the dispatcher. When data is changed they emit change events that views listen to to notify them that data has changed. The view can then respond accordingly (for example to update/rebind).</p>

<p>This will become more obvious when we go through the macgyver.js example.</p>

<h2>What is macgyver.js?</h2>

<p><a href="https://github.com/stevejhiggs/macgyver">Macgyver</a> is a project fork of <a href="http://mullet.io/">mullet.io</a> by <a href="https://github.com/stevejhiggs">Steve Higgs</a>. Mullet is an aggregate stack to get started using Node.js with Facebook&rsquo;s <a href="http://facebook.github.io/react/">React</a> framework on the client and Walmart&rsquo;s <a href="http://walmartlabs.github.io/hapi/">hapi.js</a> on the server.</p>

<p>Steve initially swapped out Grunt for Gulp, updated hapi and React and fixed some issues with the React dev tools. I then added another example to incorporate the Flux architecture, which you can see <a href="https://github.com/stevejhiggs/macgyver/tree/master/reactPlusFlux">here</a>. As React was also developed by Facebook you can begin to see how flux compliments its design and component based model.</p>

<h2>The macgyver.js Flux example</h2>

<p>The demo is a very simple quiz. In true Macgyver style he is faced with abnormally unrealistic situations armed with impossibly useless &ldquo;every-day&rdquo; items to escape the situation. If you select the correct tool, you proceed to the next situation.</p>

<p><img class="left" src="/images/posts/structure.png" width="200"></p>

<p>Let&rsquo;s start by going through the uni-directional flow above and at the same time look at the code and its structure.</p>

<p>When the game is first loaded the view fires an action to get the next situation. This is then fired off to the dispatcher, as are all actions.</p>

<p>```javascript
receiveSituations: function(data) {</p>

<pre><code>AppDispatcher.handleViewAction({
    actionType: MacgyverConstants.RECEIVE_SITUATIONS_DATA,
        data: data
});
</code></pre>

<p>},</p>

<p>```</p>

<p>The store registers to listen for events from the dispatcher with a registered callback. It has the job of loading the situation data and emitting an event when this data is changed. In this case the SituationStore.js has the job of setting the current situation for the view to render.</p>

<p>```javascript
AppDispatcher.register(function(payload){</p>

<pre><code>var action = payload.action;

switch(action.actionType) {
    case MacgyverConstants.RECEIVE_SITUATIONS_DATA:
        loadSituationsData(action.data);
        break;
    case MacgyverConstants.CHECK_ANSWER:
        checkAnswer(action.data);
        break;
    default:
        return true;
}

SituationStore.emitChange();

return true;
</code></pre>

<p>});
```</p>

<p>The React view (in this case Game.jsx) registers an event listener for these changes in the SituationStore using the React &ldquo;componentDidMount&rdquo; function. When the situation is received by the component it rebinds to the data by loading the sitution and the possible answers.</p>

<p>```javascript
var Game = React.createClass({</p>

<pre><code>componentDidMount: function () {
    SituationStore.addChangeListener(this._onChange);
    ToolStore.addChangeListener(this._onChange);
},
componentWillUnmount: function() {
    SituationStore.removeChangeListener(this._onChange);
    ToolStore.removeChangeListener(this._onChange);
},
render: ...
</code></pre>

<p>});
```</p>

<p>When the user selects an answer this fires off another &ldquo;CHECK_ANSWER&rdquo; event to the dispatcher. The situation store recieves this event with the answer in the payload and checks whether the answer selected is the correct one. If it is it updates the situation and emits a changes event to which the view receives and rebinds the view to the new situation.</p>

<h2>Conclusion</h2>

<p>Flux can be quite difficult to fathom eventhough it is quite a simple architectural pattern. In this small example it does initially feel overly complex and indeed it probably is. The pattern was designed to solve issues that occur at large scale in MVC applications due to the increased amound of bi-directional dependencies between views and models. For smaller applications it could be seen as over-engineered, however I really like the simplicity in the uni-directional flow and the assurance that unit tests are almost always going to mimic the state changes possible in your application because of the guarantee of a simple flow of data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Grunt your deployments too]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/08/08/grunt-your-deployments-too/"/>
    <updated>2013-08-08T15:27:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/08/08/grunt-your-deployments-too</id>
    <content type="html"><![CDATA[<p>We&rsquo;ve been using <a href="http://www.gruntjs.com">Grunt</a> as a build tool for our nodejs apps, and it&rsquo;s brilliant. It lints, it configures, it minifies, it tests and it packages.</p>

<p>As we move towards getting our first node app into production, we were looking at ways to deploy it. First we thought of <a href="http://www.capistranorb.com">Capistrano</a>.</p>

<p><strong><em>Capistrano</em></strong> is a fully featured deployment framework written in ruby and levering rake style tasks. It&rsquo;s extremely powerful and very robust, plus there is a <a href="https://github.com/loopj/capistrano-node-deploy">gem for node deployments</a>. Alas, it was not to be. After half a day of tail chasing and hoop jumping, it occurred to me that there must be an easier way. Capistrano was encouraging me to make my project fit their template, rather than allowing me to configure the deployment to match my project. When I dug down into the Capistrano source, I found that it was just using ssh and sftp to run remote commands and copy files. But we can simplify this process.</p>

<p><strong><em>Grunt</em></strong> has been great so far, so I started looking at deploying directly through grunt. We would be deploying to Ubuntu server boxes, so the only tools necessary are ssh and sftp.</p>

<p>There are Grunt modules for nearly <a href="https://npmjs.org/search?q=grunt">everything</a> (linting, minifying, testing, waiting, packaging, shell-exec'ing, tagging, etc.), and rather predictably, sshing (with sftp).</p>

<p><a href="https://github.com/andrewrjones/grunt-ssh">Grunt-ssh</a> provides tasks for executing remote ssh commands, and for copying files using ssh. Let&rsquo;s dive into some code.</p>

<p><strong><em>SSH commands</em></strong></p>

<p>This is going to go over some old ground (available on the Grunt-ssh <a href="https://github.com/andrewrjones/grunt-ssh">readme</a>), but we can build up the commands pretty quick.</p>

<p>This is the basic config for executing ssh commands from your Gruntfile:</p>

<p>```
module.exports = function(grunt) {</p>

<pre><code>grunt.initConfig({
    sshexec: {
  uptime: {
    command: "uptime",
    options: {
      host: "127.0.0.1",
      port: 22
      username: "myuser",
      password: "mypass"
    }
  }
}  
</code></pre>

<p>  });</p>

<p>  // Load the plugin that provides the &ldquo;sshexec&rdquo; task.
  grunt.loadNpmTasks(&lsquo;grunt-ssh&rsquo;);</p>

<p>  // Default task.
  grunt.registerTask(&lsquo;default&rsquo;, [&lsquo;sshexec:uptime&rsquo;]);</p>

<p>};
```</p>

<p>We&rsquo;ve registered a command, which we can invoke with:
<code>
grunt sshexec:uptime
</code></p>

<p>The Grunt-ssh module also provides the ability to specify multiple host configurations (shared between commands), and to select one at runtime:</p>

<p>```
grunt.initConfig({</p>

<pre><code>sshconfig: {
  qa: {
    host: "my.qa.server",
    port: 22,
    username: "user", 
    password: "password"
  },
  staging: {
    host: "my.staging.server",
    port: 22,
    username: "user",
    password: "password"
  }    
},
sshexec: {
  uptime: {
    command: "uptime"
  }
}  
</code></pre>

<p>});
<code>
So when we invoke the grunt task, we can specify a config:
</code>
grunt sshexec:uptime &mdash;config qa
<code>
Or we can set it programmatically (inside the Gruntfile)
</code>
grunt.option(&lsquo;config&rsquo;, &lsquo;qa&rsquo;);
```</p>

<p><strong><em>SFTP Tasks</em></strong></p>

<p>Grunt-ssh allows you to upload files via S<a href="FTP:">FTP:</a>
```
grunt.initConfig({</p>

<pre><code>sshconfig: {
  qa: {
    host: "my.qa.server",
    port: 22,
    username: "user", 
    password: "password",
    path: "/path/on/server"
  },
  staging: {
    host: "my.staging.server",
    port: 22,
    username: "user",
    password: "password",
    path: "/path/on/server"
  }    
},
sshexec: {
  uptime: {
    command: "uptime"
  }
}
sftp: {
  deploy: {
    files: {
      "./": "package/**"
    },
    options: {
      srcBasePath: "package/",
      createDirectories: true
    }
  }
}  
</code></pre>

<p>});
```</p>

<p>There are a couple of options here, so let&rsquo;s break it down:
<code>
files: {
  "./": "package/**"
}
</code></p>

<p>This will copy all files from the &ldquo;package/&rdquo; folder locally. If you want to specify only certain types of files, you can use grunt&rsquo;s standard <a href="https://github.com/gruntjs/grunt/wiki/grunt.file#globbing-patterns">file globbing</a>.</p>

<p><code>
srcBasePath: "package/"
</code>
Optionally strip off an initial part of the path (without it, files would upload to &ldquo;/path/on/server/package/&rdquo;).</p>

<p><strong><em>Putting it all together</em></strong>
We&rsquo;ve got all the component parts, now lets put it together (plus a few other cool bits).</p>

<p><em>Note: at the time of writing, there is a bug in Grunt-ssh where the sftp task does not use the shared sshconfig, so if you want the fixed code, use <a href="https://github.com/andyroyle/grunt-ssh">my fork</a> (there is a pull request outstanding)</em></p>

<p>This snippet assumes that:</p>

<ul>
<li>You can connect to your deployment server using ssh</li>
<li>You are deploying to /var/www/myapp</li>
<li>You are using <a href="https://github.com/nodejitsu/forever">forever</a> to run your app</li>
<li>Your application files are copied to ./package/</li>
</ul>


<p>(but, since we&rsquo;re just using bash commands, this is easily configurable)</p>

<p>```
var dirname = (new Date()).toISOString();</p>

<p>module.exports = function(grunt){
  grunt.initConfig({</p>

<pre><code>// our shared sshconfig
sshconfig: {
  qa: {
    host: "my.qa.server",
    port: 22,
    username: "user",
    password: "password",
    path: "/path/on/server"
  },
  staging: {
    host: "my.staging.server",
    port: 22,
    username: "user",
    password: "password",
    path: "/path/on/server"
  },
  production: {
    host: "&lt;%= grunt.option('server') %&gt;",
    port: 22,
    username: "&lt;%= grunt.option('username') %&gt;",
    password: "&lt;%= grunt.option('password') %&gt;",
    path: "/path/on/server"
  }
},
// define our ssh commands
sshexec: {
  start: {
    command: "cd /var/www/myapp/current &amp;&amp; forever start -o /var/www/myapp/current/logs/forever.out -e /var/www/myapp/current/logs/forever.err --append app.js"
  },
  stop: {
    command: "forever stop app.js",
    options: {
      ignoreErrors: true
   }
  },
  'make-release-dir': {
    command: "mkdir -m 777 -p /var/www/myapp/releases/" + dirname + "/logs"
  },
  'update-symlinks': {
    command: "rm -rf /var/www/myapp/current &amp;&amp; ln -s /var/www/myapp/releases/" + dirname + " /var/www/myapp/current"
  },
  'npm-update': {
    command: "cd /var/www/myapp/current &amp;&amp; npm update"
  },
  'set-config': {
    command: "mv -f /var/www/myapp/current/config/&lt;%= grunt.option('config') %&gt;.yml /var/www/myapp/current/config/default.yml"
  }
},
// our sftp file copy config
sftp: {
  deploy: {
    files: {
      "./": "package/**"
    },
    options: {
      srcBasePath: "package/",
      createDirectories: true
    }
  }
}
</code></pre>

<p>  });</p>

<p>  grunt.loadNpmTasks(&lsquo;grunt-ssh&rsquo;);
  grunt.registerTask(&lsquo;deploy&rsquo;, [</p>

<pre><code>'sshexec:stop',
'sshexec:make-release-dir',
'sshexec:update-symlinks',
'sftp:deploy',
'sshexec:npm-update',
'sshexec:set-config',
'sshexec:start'
</code></pre>

<p>  ]);
});
```</p>

<p>It should all make sense, the sshexec is just running remote ssh commands (making directories, starting and stopping using forever etc). Let&rsquo;s just re-iterate what this is doing:</p>

<ol>
<li><code>sshexec:stop</code>: stops the app (assumes you&rsquo;re using forever)</li>
<li><code>sshexec:make-release-dir</code>: this will create the folder /var/www/myapp/releases/[current-date-time]</li>
<li><code>sshexec:update-symlinks</code>: this will create a symlink from /var/www/myapp/current to the release folder we just created (this means that rolling back is just a case of changing the symlink back).</li>
<li><code>sftp:deploy</code>: copy the files into place</li>
<li><code>sshexec:npm-update</code>: installs any missing node modules</li>
<li><code>sshexec:set-config</code>: copy the environment configuration into place</li>
<li><code>sshexec:start</code>: start the application using forever, pointing the logs to /var/www/myapp/current/logs/</li>
</ol>


<p><strong><em>Deploying with one command</em></strong>
<code>
grunt deploy --config qa
</code></p>

<p>Also, if you noticed the <em>production</em> config I specified in that snippet, you&rsquo;ll see that I didn&rsquo;t include any host, username or password configs. The <code>grunt.option('value')</code> allows us to access the command line switches, which means we don&rsquo;t have to keep any sensitive passwords in source control; we can specify them on the command line.</p>

<p><code>
grunt deploy --config production --server my.production.server --username user --password password
</code></p>

<p>There are lots of other solutions to the problem of credentials, but this is by far the simplest. It&rsquo;s worth remembering the Grunt-ssh uses the ssh2 module, so by default it will look to <code>~/.ssh/</code> for keys when connecting without a password.</p>

<p><strong>But wait, there&rsquo;s more</strong></p>

<p>Basically any task you can think of is scriptable using grunt (and some combination of tools). Extra things that we&rsquo;ve added to our deployment process include:</p>

<ul>
<li>Removing the application server from the load-balancer before deploying (and pushing it back when the deployment is complete).</li>
<li>Making a http request to check the health of the service before going live.</li>
<li>Rollback from a single command</li>
</ul>


<p><strong>Oink, Oink &hellip;..</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MapReduce in MongoDB]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/08/07/mapreduce-in-mongodb/"/>
    <updated>2013-08-07T10:40:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/08/07/mapreduce-in-mongodb</id>
    <content type="html"><![CDATA[<p>One of the first things I took on when joining OpenTable was building a new endpoint in our reviews API to aggregate and summarise restaurant review data. Thankfully, at the time, all the data I needed was cached in memory so building the response object was a simple set of linq queries over the cached reviews.</p>

<h2>The problem</h2>

<p>Over time the number of reviews grow, and grow, and grow!  In fact it is inevitable that, in time, it will reach a point where caching all this data in memory would be madness.  One option to mitigate this would be to limit the cache to a fixed date range but this won&rsquo;t work in this instance because the summary logic supports custom date ranges.  Another option would be to pull the data from the persistence store each and every time it&rsquo;s required however this would seriously impact load on the infrastructure and degrade performance of the API.</p>

<p>We like to be proactive at OpenTable, so during innovation time (yes! we get time to innovate on our development) I looked at finding an alternate solution that would meet the requirements of the logic and wouldn&rsquo;t drastically increase load or degrade performance.</p>

<h2>The solution</h2>

<p>MongoDB supports <a href="http://en.wikipedia.org/wiki/MapReduce">MapReduce</a> which allows processing large volumes of data (Map), running arbitrary logic to summarise (Reduce) and producing some results.  In MongoDB the MapReduce functionality uses JavaScript functions to perform the map and reduce steps and the syntax is relatively simple to understand:&ndash;</p>

<pre><code>db.largeDataset.mapReduce(mapFunction, 
                          reduceFunction, 
                          { out: "summary" }
</code></pre>

<p>In the above example the largeDataset collection is mapped using the predefined mapFunction, the results are passed to the reduceFunction and the reduced data is finally stored in the summary collection. On top of this you can specify queries as well as a finalize function to &ldquo;tweak&rdquo; the reduce results.</p>

<p>Because map, reduce &amp; finalize are functions that only operate on their inputs the workload can be parallelized although the final results would be stored in one location.</p>

<h4>Map</h4>

<p>The key purpose of the map function is to take the complex documents and produce a structure conducive to summarising whilst at the same time defining the granularity of the results using grouping.</p>

<p>For example if you wanted to get the number of reviews by restaurant then you&rsquo;d group by the restaurant&rsquo;s unique identifier meaning that the reduce function would produce a single result for each restaurant:&ndash;</p>

<pre><code>var mapFunction = function() {
                    emit(this.RestaurantId, 1);
                  };
</code></pre>

<p>This function, called on each review, maps the review to the value 1 and groups by the RestaurantId.  The value 1 was chosen because, as you will see in the continuation of this example below, it&rsquo;s the easiest way to calculate the count of reviews per restaurant.</p>

<h4>Reduce</h4>

<p>The key purpose of the reduce function is to take a batch of mapped values for a given group and return a single result.  All values emited from the map function are passed to the reduce function although since the batch size is decided by MongoDB there may be multiple calls to reduce.  In fact given a sufficiently large source dataset the reduce function will be passed results from previous reduce function calls as well.  For example if there were 250 mapped results in a group and batches of 100 were reduced by each call then four calls would be needed, three to reduce the initial 250 results and a final call to reduce these results into the final value for the group.</p>

<p>To continue the example from above:&ndash;</p>

<pre><code>var reduceFunction = function(group, values) {
                       return Array.sum(values);
                     };
</code></pre>

<p>The results from this function would be stored in the collection defined in the mapReduce call with an <em>id and value.  The </em>id property is populated with the group id and the value property will contain the final reduce result for that group.</p>

<h2>Getting Started in C#</h2>

<p>The following are a list of projects/resources to look at if you want to implement Mongo MapReduce in your scenario with emphasis on C#:&ndash;</p>

<ul>
<li><a href="http://docs.mongodb.org/manual/tutorial/map-reduce-examples">Mongo Map-Reduce Examples</a> is a useful primer document.</li>
<li><a href="http://cookbook.mongodb.org">Mongo Cookbook</a> has a number of &ldquo;real world&rdquo; examples of MapReduce</li>
<li><a href="http://docs.mongodb.org/ecosystem/drivers/csharp">Mongo C# driver</a> has some logic to perform MapReduce however it is only a thin layer over the underlying syntax and uses JavaScript functions passed as strings.</li>
<li><a href="http://github.com/craiggwilson/fluent-mongo/wiki/Map-Reduce">Fluent-Mongo</a> provides a linq syntax over simple map reduce functions.  Interestingly it can perform multiple calculations on a set of data although at present it doesn&rsquo;t support more complex logic.</li>
<li><a href="http://twitter.com/odetocode">K.Scott Allen</a> has written two articles on MapReduce, <a href="http://odetocode.com/blogs/scott/archive/2012/03/19/a-simple-mapreduce-with-mongodb-and-c.aspx">A Simple MapReduce&hellip;</a> and <a href="http://odetocode.com/blogs/scott/archive/2012/03/29/a-simpler-mapreduce-with-mongodb-and-c.aspx">A Simpler MapReduce&hellip;</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[One Gun - Many Enemies]]></title>
    <link href="http://tech.opentable.co.uk/blog/2013/07/24/one-gun-many-enemies/"/>
    <updated>2013-07-24T14:32:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2013/07/24/one-gun-many-enemies</id>
    <content type="html"><![CDATA[<p>I spent some time, re-investigating Javascript. After a few months of intensive TDD and SOLID training at OpenTable I was curious how those principles apply in a slightly different environment. Guess what, they do not differ that much&hellip; I planned to write about all this sometime in the future after gaining more experience from real battles ahead, however <a href="http://watirmelon.com/2013/02/09/why-i-dont-like-Jasmine/">Watirmelon post</a> invited me to attack this subject immediately.</p>

<p>I went through the available testing frameworks for Javascript and decided on Jasmine. Why? Mainly because of its BDD syntax which is fashionable this season. The second reason is that it has really cool documentation. Finally, the basic set up of just running an HTML file in a browser got me up and running fast.</p>

<p>Let&rsquo;s look at the basics &ndash; what I expect from different types of tests and how Jasmine fits into those requirements:</p>

<h2>Unit Testing</h2>

<h3>Advice from senior craftsman</h3>

<ul>
<li>Shoot them one by one
(structure your code so that it is easier to maintain)</li>
<li>Be sure you kill with every shot
(verify small bits of code to nail down issues)</li>
<li>Kill them all
(verify edge cases)</li>
<li>Kill them quick
(provide fast feedback on issues)</li>
<li>Choose a fast shooting gun &ndash; the faster you shoot, the more enemies you will kill
(unit tests must be blazing fast)</li>
<li>Choose the most reliable gun &ndash; if it is stuck then you are dead
(when unit tests are brittle you will stop depending on them)</li>
<li>Your gun needs to be light enough to carry everywhere
(unit tests are your gun, so you must be able to run them all locally)</li>
</ul>


<p>Jasmine works great for all those aims, as it is really fast and it allows you to stub both objects and DOM elements (especially combined with a Jasmine-jQuery plugin). Keep in mind that Unit Testing is useful only if you either write new code or refactor old code. Do not ever try to write Unit Tests for existing code which you don&rsquo;t intend to refactor. It&rsquo;s like shooting the dead. You simply cannot kill them with another bullet.</p>

<h2>Integration Testing</h2>

<h3>Advice from senior craftsman</h3>

<ul>
<li>Beware of the hidden sniper
(test against external dependencies like files, databases, network services)</li>
<li>You are part of your squad
(verify that components of the application work well together)</li>
<li>Observe the environment
(try to use depenedencies as close to the real problem as possible e.g. example file, database with test data, test version of an external service)</li>
<li>Point in the right direction
(do not try to test your stack top-down, instead concentrate on interfaces and adapters that you could not test in unit tests)</li>
</ul>


<p>Integration tests will naturally be harder to quickly setup &ndash; they might break due to configuration problems or inaccessible external services. That&rsquo;s why you don&rsquo;t want to have that many of them. Still, you want to know that all your problems are due to your partner changing protocol. Once again nothing stops us from using Jasmine here.</p>

<p>Jasmine just executes your tests, so the tests that point towards external services, send example files to your code etc. will all be a breeze to implement with Jasmine.</p>

<h2>Acceptance Testing</h2>

<h3>Aims</h3>

<ul>
<li>Confirm you are fighting the right war
(show specification to your product owner/manager/client)</li>
<li>Keep clean supply routes
(test the functionality of your app with full set up and all dependencies)</li>
<li>Find hidden mines
(test your application in all environments to which you deploy)</li>
</ul>


<h3>Advice from senior craftsman</h3>

<ul>
<li>Speak in their language
(use a tool which allows writing tests in natural language such as Cucumber or SpecFlow)</li>
<li>Take your time
(those tests will be slower, so accept that you will not always run them locally and that you will not run them every minute)</li>
<li>Spend your resources wisely
(acceptance tests are naturally much more brittle then other types of tests, so try to keep their number reasonably small)</li>
</ul>


<p>This is where I would not recommend Jasmine, simply because it doesn&rsquo;t fit this job. Its syntax is based on programming language, so it is harder to read by non-engineers. Jasmine allows you to execute events and call your code. However, your users will be most likely be using a browser to interact with your code, so it is much better to use a tool that also uses a browser to test your web page.</p>

<h2>Conclusion</h2>

<p>I find Jasmine extremely well suited for unit and integration testing. I wouldn&rsquo;t use it for acceptance tests as there are quite few better tools for that job. And I guess that is the issue that Alister Scott has with Jasmine on his blog &ndash; he tried to use it for acceptance testing. In that case I wouldn&rsquo;t choose Jasmine either. On the other hand I don&rsquo;t like using a screwdriver to hammer a nail.</p>

<h2>Post Scriptum (on DOM dependency)</h2>

<p>Alister also complains about integration issues between his server-side code and client-side code. His experience is that changes to IDs and classes in MVC applications result in failing Javascript. The issue is serious and shows one important mistake, which is hardcoding dependencies. IDs and class names are configurable details and Javascript code should be agnostic of them. To illustrate let&rsquo;s look at a simple test case from my pet-project:</p>

<pre><code>describe("GameController", function(){
  describe("During Initialize", function() {
    var view;
    var subject;

    beforeEach(function() {    
        setFixtures("&lt;div id="myid"&gt;&lt;/div&gt;");
        subject = new GameView();

        subject.CreateFabricInDiv("#myid");
    });

    it("creates fabric in div", function(){
        expect($("#myid")).not.toBeEmpty();
    });

    it("holds pointer to canvas", function(){
        expect(subject._canvas).not.toBe(undefined);
    });
 }); 
</code></pre>

<p>Do you see how &ldquo;#myid&rdquo; parameter is passed into the method call? We moved the configuration detail out from the Javascript code, which in my opinion it is the simplest solution to Alister&rsquo;s problem.</p>

<p>If code is designed in such a way that IDs/class names are not embedded all around the codebase you can figure out quite few nice ways to keep them consistent between Javascript and server-side code.</p>

<p>It also helps with code re-usability as you can use the same code on two separate pages with differing IDs!</p>
]]></content>
  </entry>
  
</feed>
