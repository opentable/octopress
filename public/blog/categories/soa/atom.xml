<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: SOA | OpenTable Tech UK Blog]]></title>
  <link href="http://tech.opentable.co.uk/blog/categories/soa/atom.xml" rel="self"/>
  <link href="http://tech.opentable.co.uk/"/>
  <updated>2016-04-28T09:43:24+01:00</updated>
  <id>http://tech.opentable.co.uk/</id>
  <author>
    <name><![CDATA[OpenTable]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OpenComponents - microservices in the front-end world]]></title>
    <link href="http://tech.opentable.co.uk/blog/2016/04/27/opencomponents-microservices-in-the-front-end-world/"/>
    <updated>2016-04-27T11:10:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2016/04/27/opencomponents-microservices-in-the-front-end-world</id>
    <content type="html"><![CDATA[<p>Many engineers work every day on opentable.com from our offices located in Europe, America, and Asia, pushing changes to production multiple times a day. Usually, this is very hard to achieve, in fact it took years for us to get to this point. <a href="http://tech.opentable.co.uk/blog/2015/02/09/dismantling-the-monolith-microsites-at-opentable/">I described in a previous article</a> how we dismantled our monolith in favour of a Microsites architecture. Since the publication of that blog post we have been working on something I believe to be quite unique, called <strong>OpenComponents</strong>.</p>

<h3>Another front-end framework?</h3>

<p>OpenComponents is a system to facilitate code sharing, reduce dependencies, and easily approach new features and experiments from the back-end to the front-end. To achieve this, it is based on the concept of using services as interfaces &ndash; enabling pages to render partial content that is located, executed and deployed independently.</p>

<p>OpenComponents is not <em>another SPA JS framework</em>; it is a set of conventions, patterns and tools to develop and quickly deploy fragments of front-end. In this perspective, it plays nicely with any existing architecture and framework in terms of front-end and back-end. Its purpose is to <strong>serve as delivery mechanism for a more modularised end-result in the front-end</strong>.</p>

<p>OC is been in production for more than a year at OpenTable and it is <a href="https://github.com/opentable/oc">fully open-sourced</a>.</p>

<h2>Â Overview</h2>

<p>OpenComponents involves two parts:</p>

<ul>
<li>The <strong>consumers</strong> are web pages that need fragments of HTML for rendering partial contents. Sometimes they need some content during server-side rendering, somethings when executing code in the browser.</li>
<li>The <strong>components</strong> are small units of isomorphic code mainly consisting of HTML, Javascript and CSS. They can optionally contain some logic, allowing a server-side Node.js closure to compose a model that is used to render the view. When rendered they are pieces of HTML, ready to be injected in any web page.</li>
</ul>


<p>The framework consists of three parts:</p>

<ul>
<li>The <strong>cli</strong> allows developers to create, develop, test, and publish components.</li>
<li>The <strong>library</strong> is where the components are stored after the publishing. When components depend on static resources (such as images, CSS files, etc.) these are stored, during packaging and publishing, in a publicly-exposed part of the library that serves as a CDN.</li>
<li>The <strong>registry</strong> is a REST API that is used to consume components. It is the entity that handles the traffic between the library and the consumers.</li>
</ul>


<p>In the following example, you can see how a web page looks like when including both a server-side rendered component (<em>header</em>) and client-side (still) unrendered component (<em>advert</em>):</p>

<p>```html
&lt;!DOCTYPE html>
  &hellip;
  &lt;oc-component href=&ldquo;//oc-registry.com/header/1.X.X&rdquo; data-rendered=&ldquo;true&rdquo;></p>

<pre><code>&lt;a href="http://tech.opentable.co.uk/"&gt;
  &lt;img src="//cdn.com/oc/header/1.2.3/img/logo.png" /&gt;
&lt;/a&gt;
</code></pre>

<p>  &lt;/oc-component>
  &hellip;
  <p>page content</p>
  &lt;oc-component href=&ldquo;//oc-registry.com/advert/~1.3.5/?type=bottom&rdquo;>
  &lt;/oc-component>
  &hellip;
  <script src="//oc-registry/oc-client/client.js"></script>
```</p>

<h3>Getting started</h3>

<p>The only prerequisite for creating a component is Node.js:</p>

<p><code>sh
$ npm install -g oc
$ mkdir components &amp;&amp; cd components
$ oc init my-component
</code></p>

<p>Components are folders containing the following files:</p>

<table style="margin-bottom:16px;">
    <tr>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">File</th>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Description</th>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">package.json</td>
        <td style="padding:5px 10px;font-weight: inherit;">A common <a href="https://docs.npmjs.com/files/package.json" target="_blank">node's package.json</a>. An "oc" property contains some additional configuration.</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">view.html</td>
        <td style="padding:5px 10px;font-weight: inherit;">The view containing the markup. Currently Handlebars and Jade view engines are supported. It can contain some CSS under the &lt;style&gt; tag and client-side Javascript under the &lt;script&gt; tag.</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">server.js (optional)</td>
        <td style="padding:5px 10px;font-weight: inherit;">If the component has some logic, including consuming services, this is the entity that will produce the view-model to compile the view.</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">static files (optional)</td>
        <td style="padding:5px 10px;font-weight: inherit;">Images, Javascript, and files that will be referenced in the HTML markup.</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">*</td>
        <td style="padding:5px 10px;font-weight: inherit;">Any other files that will be useful for the development such as tests, docs, etc.</td>
    </tr>
</table>


<h2>Editing, debugging, testing</h2>

<p>To start a local test registry using a components' folder as a library with a watcher:
<code>sh
$ oc dev . 3030
</code></p>

<p>To see how the component looks like when consuming it:
<code>sh
$ oc preview http://localhost:3030/hello-world
</code></p>

<p>As soon as you make changes on the component, you will be able to refresh this page and see how it looks. This an example for a component that handles some minimal logic:</p>

<p>
```</p>

<!-- view.html -->


<div>Hello {{ name }}</div>


<p>```
</p>

<p>```js
// server.js
module.exports.data = function(context, callback){
  callback(null, {</p>

<pre><code>name: context.params.name || 'John Doe'
</code></pre>

<p>  });
};
```</p>

<p>To test this component, we can curl <code>http://localhost:3030/my-component/?name=Jack</code>.</p>

<h3>Publishing to a registry</h3>

<p>You will need an online registry connected to a library. A component with the same name and version cannot already exist on that registry.</p>

<p>```sh</p>

<h1>just once we create a link between the current folder and a registry endpoint</h1>

<p>$ oc registry add <a href="http://my-components-registry.mydomain.com">http://my-components-registry.mydomain.com</a></p>

<h1>then, ship it</h1>

<p>$ oc publish my-component/
```</p>

<p>Now, it should be available at <code>http://my-components-registry.mydomain.com/my-component</code>.</p>

<h2>Consuming components</h2>

<p>From a consumer&rsquo;s perspective, a component is an HTML fragment. You can render components just on the client-side, just on the server-side, or use the client-side rendering as failover strategy for when the server-side rendering fails (for example because the registry is not responding quickly or it is down).</p>

<p>You don&rsquo;t need Node.js to consume components on the server-side. The registry can provide rendered components so that you can consume them using any tech stack.</p>

<p>When published, components are immutable and semantic versioned. The registry allows consumers to get any version of the component: the latest patch, or minor version, etc. For instance, <code>http://registry.com/component</code> serves the latest version, and <code>http://registry.com/component/^1.2.5</code> serves the most recent major version for v1.</p>

<h3>Client-side rendering</h3>

<p>To make this happen, a components' registry has to be publicly available.
<code>html
&lt;!DOCTYPE html&gt;
  ...
  &lt;oc-component href="//my-components-registry.mydomain.com/hello-world/1.X.X"&gt;&lt;/oc-component&gt;
  ...
  &lt;script src="//my-components-registry.mydomain.com/oc-client/client.js" /&gt;
</code></p>

<h3>Server-side rendering</h3>

<p>You can get rendered components via the registry REST API.
```sh
curl <a href="http://my-components-registry.mydomain.com/hello-world">http://my-components-registry.mydomain.com/hello-world</a></p>

<p>{
  &ldquo;href&rdquo;: &ldquo;<a href="https://my-components-registry.mydomain.com/hello-world">https://my-components-registry.mydomain.com/hello-world</a>&rdquo;,
  &ldquo;version&rdquo;: &ldquo;1.0.0&rdquo;,
  &ldquo;requestVersion&rdquo;: &ldquo;&rdquo;,
  &ldquo;html&rdquo;: &ldquo;&lt;oc-component href=\"<a href="https://my-components-registry.mydomain.com/hello-world%22">https://my-components-registry.mydomain.com/hello-world%22</a> data-hash=\"cad2a9671257d5033d2abfd739b1660993021d02\&rdquo; data-name=\&ldquo;hello-world\&rdquo; data-rendered=\&ldquo;true\&rdquo; data-version=\&ldquo;1.0.13\&rdquo;>Hello John doe!&lt;/oc-component>&ldquo;,
  "type&rdquo;: &ldquo;oc-component&rdquo;,
  &ldquo;renderMode&rdquo;: &ldquo;rendered&rdquo;
}
```</p>

<p>Nevertheless, for improving caching and response size, when doing browser rendering, or using the <code>node.js</code> client or any language capable of executing server-side Javascript the request will look more like:
```sh
 curl <a href="http://my-components-registry.mydomain.com/hello-world/~1.0.0">http://my-components-registry.mydomain.com/hello-world/~1.0.0</a> -H Accept:application/vnd.oc.unrendered+json</p>

<p>{
  &ldquo;href&rdquo;: &ldquo;<a href="https://my-components-registry.mydomain.com/hello-world/~1.0.0">https://my-components-registry.mydomain.com/hello-world/~1.0.0</a>&rdquo;,
  &ldquo;name&rdquo;: &ldquo;hello-world&rdquo;,
  &ldquo;version&rdquo;: &ldquo;1.0.0&rdquo;,
  &ldquo;requestVersion&rdquo;: &ldquo;~1.0.0&rdquo;,
  &ldquo;data&rdquo;: {</p>

<pre><code>"name": "John doe"
</code></pre>

<p>  },
  &ldquo;template&rdquo;: {</p>

<pre><code>"src": "https://s3.amazonaws.com/your-s3-bucket/components/hello-world/1.0.0/template.js",
"type": "handlebars",
"key": "cad2a9671257d5033d2abfd739b1660993021d02"
</code></pre>

<p>  },
  &ldquo;type&rdquo;: &ldquo;oc-component&rdquo;,
  &ldquo;renderMode&rdquo;: &ldquo;unrendered&rdquo;
}
```</p>

<p>Making a similar request it is possible to get the compiled view&rsquo;s url + the view-model as data. This is useful for caching the compiled view (taking advantage of components' immutability).</p>

<h2>Setup a registry</h2>

<p>The registry is a Node.js Express app that serves the components. It just needs an S3 account to be used as library.</p>

<p>First, create a dir and install OC:
<code>sh
$ mkdir oc-registry &amp;&amp; cd oc-registry
$ npm init
$ npm install oc --save
$ touch index.js
</code></p>

<p>This is how <code>index.js</code> will look like:</p>

<p>```js
var oc = require(&lsquo;oc&rsquo;);</p>

<p>var configuration = {
  verbosity: 0,
  baseUrl: &lsquo;<a href="https://my-components-registry.mydomain.com/">https://my-components-registry.mydomain.com/</a>&rsquo;,
  port: 3000,
  tempDir: &lsquo;./temp/&rsquo;,
  refreshInterval: 600,
  pollingInterval: 5,
  s3: {</p>

<pre><code>key: 'your-s3-key',
secret: 'your-s3-secret',
bucket: 'your-s3-bucket',
region: 'your-s3-region',
path: '//s3.amazonaws.com/your-s3-bucket/',
componentsDir: 'components'
</code></pre>

<p>  },
  env: { name: &lsquo;production&rsquo; }
};</p>

<p>var registry = new oc.Registry(configuration);</p>

<p>registry.start(function(err, app){
  if(err){</p>

<pre><code>console.log('Registry not started: ', err);
process.exit(1);
</code></pre>

<p>  }
});
```</p>

<h2>Conclusions</h2>

<p>After more than a year in production, OC is still evolving. These are some of the most powerful features:</p>

<ul>
<li>It <strong>enables developers to create and publish components very easily</strong>. None of the operations need any infrastructural work as the framework takes care, when packaging, of making each component <em>production-ready</em>.</li>
<li>It is <strong>framework agnostic</strong>. Microsites written in <em>C#</em>, <em>Node</em> and <em>Ruby</em> consume components on the server-side via the API. In the front-end, it is great for delivering neutral pieces of HTML but works well for Angular components and React views too.</li>
<li>It enables <strong>granular ownership</strong>. Many teams can own components and they all are discoverable via the same service.</li>
<li>Isomorphism is good for <strong>performance</strong>. It enables consumers to render things on the server-side when needed (mobile apps, SEO) and defer to the client-side contents that are not required on the first load (third-party widgets, adverts, SPA fragments).</li>
<li>Isomorphism is good for <strong>robustness</strong>. When something is going bad on the server-side (the registry is erroring or slow) it is possible to use client-side rendering as a fail-over mechanism. The Node.js client does this by default.</li>
<li>It is a good approach for <strong>experimentation</strong>. People can work closely to the business to create widgets that are capable of both getting data from back-end services and deliver them via rich UIs. We very often had teams that were able to create and instrument tests created via OC in less then 24 hours.</li>
<li>Semver and auto-generated documentation <strong>enforce clear contracts</strong>. Consumers can pick the version they want and component owners can keep clear what the contract is.</li>
<li>A more componentised front-end leads to write <strong>more easily destroyable code</strong>. As opposite of writing highly maintainable code, this approach promotes small iterations on very small, easily readable and testable units of code. In this perspective, recreating something from scratch is perfectly acceptable and recommended, as there is almost zero cost for a developer to start a new project and the infrastructure in place makes maintainance and deprecation as easy as a couple of clicks.</li>
</ul>


<p>If you wish to try or know more about OpenComponents, visit <a href="https://github.com/opentable/oc">OC&rsquo;s github page</a> or have a look at <a href="https://github.com/matteofigus/oc-components-examples">some component examples</a>. If you would give us some feedback, asks us question, or contribute to the project get in touch via the <a href="https://gitter.im/opentable/oc">gitter chat</a> or via <a href="mailto:oc@opentable.com">e-mail</a>. We would love to hear your thoughts about this project.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Dismantling the monolith - Microsites at Opentable]]></title>
    <link href="http://tech.opentable.co.uk/blog/2015/02/09/dismantling-the-monolith-microsites-at-opentable/"/>
    <updated>2015-02-09T09:43:03+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2015/02/09/dismantling-the-monolith-microsites-at-opentable</id>
    <content type="html"><![CDATA[<p>A couple of years ago we started to break-up the code-base behind our consumer site <a href="http://www.opentable.com">opentable.com</a>, to smaller units of code, in order to improve our productivity. New teams were created with the goal of splitting up the logic that was powering the back-end and then bring to life new small services. Then, we started working on what we call <em>Microsites</em>.</p>

<h3>Microsites</h3>

<p>A microsite is a very small set of web-pages, or even a single one, that takes care of handling a very specific part of the system&rsquo;s domain logic. Examples are the <em>Search Results</em> page or the <em>Restaurant&rsquo;s Profile</em> page. Every microsite is an independently deployable unit of code, so it is easier to test, to deploy, and in consequence more resilient. Microsites are then all connected by a front-door service that handles the routing.</p>

<h3>Not a free ride</h3>

<p>When we deployed some microsites to production we immediately discovered a lot of pros:</p>

<ul>
<li>Bi-weekly deployments of the monolith became hundreds of deployments every week.</li>
<li>Not anymore a shared codebase for hundreds of engineers. Pull requests accepted, merged, and often deployed on the same day.</li>
<li>Teams experimenting and reiterating faster: product was happy.</li>
<li><em>Diversity</em> on tech stacks: teams were finally able to pick their own favourite web-stack, as soon as they were capable of deploying their code and taking care of it in terms of reliability and performance.</li>
<li>Robustness: when something was wrong with a microsite, everything else was fine.</li>
</ul>


<p>On the other hand, we soon realised that we introduced new problems on the system:</p>

<ul>
<li>Duplication: teams started duplicating a lot of code, specifically front-end components such as the header, the footer, etc.</li>
<li>Coordination: when we needed to change something on the header, for example, we were expecting to see the change live in different time frames, resulting in inconsistencies.</li>
<li>Performance: every microsite was hosting its own duplicated css, javascript libraries, and static resources; resulting as a big disadvantage for the end-user in terms of performance.</li>
</ul>


<h3>SRS &ndash; aka Site Resources Service</h3>

<p>To solve some of these problems we created a REST api to serve html snippets, that soon we started to call <em>components</em>. Main characteristics of the system are:</p>

<ul>
<li>We have components for shared parts of the website such as the header, the footer, and the adverts. When a change has to go live, we apply the change, we deploy, and we see the change live everywhere.</li>
<li>Output is in HTML format, so the integration is possible if the microsite is either a .NET MVC site or a node.js app.</li>
<li>We have components for the core CSS and the JS common libraries, so that all the microsites use the same resources and the browser can cache them making the navigation smooth.</li>
<li>The service takes care of hosting all the static resources in a separate CDN, so microsites don&rsquo;t have to host that resources.</li>
</ul>


<p>This is an example of a request to the <em>core</em> css component:
```sh
curl <a href="http://srs-sc.otenv.com/v1/com-2014/resource-includes/css">http://srs-sc.otenv.com/v1/com-2014/resource-includes/css</a></p>

<p>{
  &ldquo;href&rdquo;: &ldquo;<a href="http://srs-sc.otenv.com/v1/com-2014/resource-includes/css">http://srs-sc.otenv.com/v1/com-2014/resource-includes/css</a>&rdquo;,
  &ldquo;html&rdquo;: &ldquo;<link rel=\"stylesheet\" href=\"//na-srs.opentable.com/content/static-1.0.1388.0/css-new-min/app.css\" /><!--[if lte IE 8]><link rel=\"stylesheet\" href=\"//na-srs.opentable.com/content/static-1.0.1388.0/css-new-min/app_ie8.css\" /> &lt;![endif]&mdash;>&rdquo;,
  &ldquo;type&rdquo;:&ldquo;css&rdquo;
}
```</p>

<p>The downside of this approach is that there is a strict dependency with SRS for each microsite. On every request, a call to SRS has to be made, so <strong>we had to work hard to guarantee reliability and good performance</strong>.</p>

<h2>Conclusions</h2>

<p>When we tried the microsite approach we âtradedâ some of our code problems with some new cultural problems. We became more agile and we were working in a new different way, with the downside of having the <strong>need to more effectively coordinate more people</strong>. The consequence is that the <strong>way we were approaching the code</strong> evolved over time.</p>

<p>One year later, with the front-end (almost completely) living on micro-sites, and with the help of SRS, we are experimenting more effective ways to be resilient and robust, with the specific goal to allow teams to create their own components and share them with other teams in order to be independent, and use them to easily approach to A/B experiments.</p>

<p>In the next post I&rsquo;ll write about <a href="https://github.com/opentable/oc">OpenComponents</a>, an experimental framework we just open-sourced that is trying to address some of this needs.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking APIs - why itâs important, and how]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/02/28/api-benchmark/"/>
    <updated>2014-02-28T09:00:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/02/28/api-benchmark</id>
    <content type="html"><![CDATA[<p>Since I joined OpenTable Iâve been experimenting with performance monitoring, specifically on web services. One of the projects my team is responsible for is a REST API that provides UI elements for HTML5 applications, shaped as HTML snippets and static resources. Our consumers are websites deployed in multiple parts of the world, so our service needs to be fast and reliable.</p>

<h2>The why</h2>

<p>A couple of weeks after joining the company I decided, as part of my <a href="http://tech.opentable.co.uk/blog/2014/02/06/20-percent-time/">innovation time</a>, to rebuild the core of a .NET WebApi project in node.js in order to have a working prototype that could do exactly the same job as the original one, and could help me to observe how the two applications could react with similar volumes of traffic. After managing to make the two apis run on two clean VMs with the same configuration, I wrote a little node.js script to start performing some requests and test the response times. After seeing the results I thought that something was going wrong:</p>

<p>```sh
.NET/route1 x 9.16 ops/sec Â±12.71% (17 runs sampled)</p>

<h1>nodeJS/route1 x 106 ops/sec Â±1.19% (180 runs sampled)</h1>

<p>Fastest is nodeJS/route1</p>

<p>.NET/route2 x 10.70 ops/sec Â±8.54% (19 runs sampled)</p>

<h1>nodeJS/route2 x 118 ops/sec Â±1.22% (175 runs sampled)</h1>

<p>Fastest is nodeJS/route2</p>

<p>======================================
Fastest Service is nodeJS
```</p>

<p>After trying to microbenchmark different layers of the software, I found the problem. On the .NET side I was reading a file synchronously, for every request; the file systemâs library used with the node.js app, instead, was automatically caching the reads as default. After setting up a basic caching mechanism in the .NET app and running my script again the node.js API was only 1.4 times faster. After finding and solving that issue I thought how badly the application could have handled concurrency when deployed in production, even if it was heavily unit tested, the specs were well defined, and it was built using all the best techniques we all love.</p>

<p>As developers we rely on technologies that, with a minimum effort, can guarantee some pretty decent results in terms of performance. Modern web frameworks handle concurrency and thread management without requiring much plumbing code. Sophisticated and relatively cheap cloud services help us monitor our applications, providing dashboards, reports and alerting systems. Deploying on the cloud we can run our services and even auto-scale them depending on how much power we need. Even with these tools we must still own the responsibility of writing good quality code, testing it properly, and deploying as fast as possible in order to optimise the delivery process of our products.</p>

<p>But what about performance? I mean, what about the relationship between the code we write every day, and the way we impact overall performance? Are we sure that we are not deploying to production something that is degrading our servicesâ performance?</p>

<h2>The how</h2>

<p>Talking about the âwhyâ could be relatively easy, but the âhowâ is a controversial topic. In my experience there are three important steps to any dish. First, we need the right tools to manipulate the ingredients. I tried many different tools but I couldn&rsquo;t find a good fit for all my benchmarking requirements. It always makes sense to start with something to get you going but after a while it is important to find whatâs the best for you, your team, and your project.</p>

<p>Then, you start cooking: personally, the number of things Iâve learned by just starting to benchmark some services is incredible. Nevertheless, as with every time we talk about metrics, it is key to know what is important about the data we are analysing, recognise the false positives/negatives and be aware of vanity metrics that could emphasize something irrelevant, or, more importantly, hide something significant.</p>

<p>The last step is simple: react and persist. If you discover something relevant, you can do something to improve the quality of your software.  With the right tools you can write some benchmark tests to target different layers of your software and execute them each and every time you contribute to that repository.  Doing this helps you keep your system performance under control which is really valuable.</p>

<h2>The how, for me</h2>

<p>When I found that little bug in the application (and it wasnât actually a bug in the way we usually define them, as it wasnât breaking any test or any softwareâs feature), I decided to spend some time to make my benchmark script better, in order to support different HTTP verbs, HTTPS, and a few things I needed to test all the routes of the API in a easy way. The goal was to wrap my little script as a <a href="http://www.gruntjs.com">grunt.js</a> module, (we use Teamcity as a CI platform and we already use grunt to run various tasks during our release process). I wanted to run this benchmarks externally to avoid interfering with the performance of the application, and to have a configuration-based simple-to-use tool.</p>

<p>So after some refactoring I started working on <a href="https://github.com/matteofigus/api-benchmark">api-benchmark</a> and its grunt wrapper <a href="https://github.com/matteofigus/grunt-api-benchmark">grunt-api-benchmark</a>, in order to make performance testing part of our continuous delivery process. A couple of days later my team was using it to run benchmark tests on our pre-prod environments against our APIs, running some hundreds of requests on each route every time we made a single commit to Github. What I managed to do was to break the build if response times were not good enough (stopping the production deployment), and producing a tiny report with some graphs, in order to have something useful to observe and eventually collect. Now, a couple of months later, a lot of functionality has been added and other teams are using it with success.</p>

<p>In case of RESTful services, it is possible to make series of requests to test response times, find peaks and classify errors; it is possible to perform concurrent calls to see how many parallel requests your service can handle (when deployed in single boxes, or when load balanced and globalised); and every time grunt runs everything is saved and plotted to readable and shareable graphs, so the knowledge can be shared between people that belong to different backgrounds.</p>

<p><img class="center" src="/images/posts/api_benchmark.png"></p>

<p>A lot of other features are still under development, including support for SOAP services and historical analysis (compare results from previous benchmarks and create historical graphs to represent the evolution of your software).</p>

<h2>How it works &amp; how to use it</h2>

<p>The way it works is simple. A configuration file contains the list of all the routes of the API. For each route it is possible to set different parameters such as headers, methods, expected status code, and expected response times. Other options that can be set such as the minimum number of samples, the maximum time for collecting the results, the number of concurrent requests, etc. It should be something like:</p>

<p>```json
{
  &ldquo;service&rdquo;: {</p>

<pre><code>"My api": "http://localhost:3007/api/"
</code></pre>

<p>  },
  &ldquo;endpoints&rdquo;: {</p>

<pre><code>"simpleRoute": "v1/getJson",
"postRoute": {
  "route": "v1/postJson",
  "method": "post",
  "data": {
    "test": true,
    "someData": "someStrings"
  },
  "expectedStatusCode": 200
},
"deleteRoute": {
  "route": "v1/deleteMe?test=true",
  "method": "delete",
  "maxMean": 0.06,
  "maxSingleMean": 0.003
}
</code></pre>

<p>  },
  &ldquo;options&rdquo;: {</p>

<pre><code>"minSamples": 1000,
"runMode": "parallel",
"maxConcurrentRequests": 20,
"debug": true,
"stopOnError": false
</code></pre>

<p>  }
}
```</p>

<p>Then it is possible to include the script in a project that is written in any language and runs on any platform. The only requirement is to <a href="http://www.nodejs.org">install node.js</a> on that machine. If the project doesnât have a package.json file in the root of your project, itâs as easy as doing:</p>

<p><code>sh
$ npm init
</code></p>

<p>Once this is complete, include and install grunt-api-benchmark as a dependency (if you are not on Windows, you may want to sudo it depending on how youâve installed node.js):</p>

<p><code>sh
$ npm install grunt-api-benchmark --save-dev
</code></p>

<p>The last thing to do is to create a task inside your Gruntfile.js. Create one if you already donât have one, and then add the âapi_benchmarkâ task in order to have something like this:</p>

<p>```js
module.exports = function(grunt) {</p>

<p>  grunt.initConfig({</p>

<pre><code>pkg: grunt.file.readJSON('package.json'),
api_benchmark: {
  myApi: {
    options: {
      output: 'output_folder'
    },
    files: {
      'report.html': 'config.json',
      'export.json': 'config.json'
    }
  }
}
</code></pre>

<p>  });</p>

<p>  grunt.loadNpmTasks(&lsquo;grunt-api-benchmark&rsquo;);
  grunt.registerTask(&lsquo;benchmark&rsquo;, [&lsquo;api_benchmark&rsquo;]);
};
```</p>

<p>Where âgeneratedâ is the output folder, âconfig.jsonâ is your configuration file, and âreport.htmlâ (or âexport.jsonâ) is the outputâs filename. To run it just:</p>

<p><code>sh
$ grunt benchmark
</code></p>

<p>If you use TravisCI, TeamCity, or any other CI platform, all youâll have to do is to make it run after being sure the dependencies are resolved:</p>

<p><code>sh
$ npm install
</code></p>

<h2>Letâs benchmark &ndash; some lessons learned</h2>

<p>I think this is the most important part of the whole process, however I donât think there are any general rules that are applicable for every context. I believe that after testing and stressing your system you will find out what matters to you and to your business. Nevertheless, I want to share some of the lessons Iâve learned.</p>

<p>First, set-up everything correctly.</p>

<ul>
<li>Benchmarks need to run always on the same machine, same agent, and same configuration to be reliable and comparable.</li>
<li>The network should be tested to be sure there arenât any particular limits that would affect the benchmarks. It should be tested each time before running any benchmarks and could include things like bandwidth, host name correctness, and OS limitations.</li>
<li>Donât run the tests from the same machine that hosts the application. Run it from the outside, and if you deploy in different regions, keep that in mind when you look at the results.</li>
</ul>


<p>When you benchmark, remember that stress and performance are two different things.</p>

<ul>
<li>You should test both to learn about performance but also your limits, in order to have an idea on how to scale your application or how to fix it when necessary.</li>
<li>10 seconds is not enough. 1 minute is nice, 5 is better.</li>
<li>One route is not enough. Testing all the routes allow us to see the difference between different response lengths.</li>
<li>Sometimes your application needs a warm-up, especially if you test it after a deployment. Set up a script to do that or set a proper time-out to be sure you are retrieving some valuable numbers back.</li>
<li>Don&rsquo;t benchmark the live production environment. Your results are affected by too many variables. If possible, set-up a staging environment with exactly the same configuration to run benchmarks.</li>
</ul>


<p>If necessary, adapt your API to be more testable through some very basic design patterns.</p>

<ul>
<li>Performance could depend on synchronous calls to third-party APIs or databases. Ideally routes should have an optional parameter to mock external dependencies so we should test that as well.</li>
<li>Ensure that changes to data or the operating environment are not persisted after the benchmarks complete. This is important to ensure no side effects on subsequent runs and will allow you to  benchmark production boxes if needed (after the deployment and obviously before directing any traffic to them).</li>
</ul>


<p>Last but not least, letâs analyse the data</p>

<ul>
<li>Averages are not enough, peaks are important, investigate them.</li>
<li>When something unexpected happens, try to reproduce it in order to fix it.</li>
<li>If wildly different numbers come up every time you run the tests, your API is depending on too many unpredictable events. Try to fix it. Try to run benchmarks locally and microbenchmark your software until you find the element that is causing the unpredictability. Then, fix it or find a way to mock it if you have no other option.</li>
<li>Numbers should be readable and shareable by everyone. Find a tool that dashboards your results and easily allow you to share that data.</li>
</ul>


<h2>âbenchmarkingâ != âmonitoringâ;</h2>

<p>Benchmarking doesnât equal and doesnât replace monitoring. Once you start having an extensive knowledge about your systemâs performance, you can find useful and easy to establish correlations between your benchmarks and your monitoring metrics. Depending on the scale of your system, it could be something very important.</p>

<h2>Conclusions</h2>

<p>I believe that taking care of performance is our responsibility, as developers. We can and should do more, and I hope this subject will gain more interest. In the meanwhile, if <a href="https://github.com/matteofigus/api-benchmark">api-benchmark</a> sounds interesting for you and you are interested in trying it or contributing (it is totally open-source), donât hesitate to <a href="http://www.twitter.com/matteofigus">get in touch with me</a>.</p>
]]></content>
  </entry>
  
</feed>
