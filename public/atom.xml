<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[OpenTable Tech UK Blog]]></title>
  <link href="http://tech.opentable.co.uk/atom.xml" rel="self"/>
  <link href="http://tech.opentable.co.uk/"/>
  <updated>2015-01-22T19:49:25+00:00</updated>
  <id>http://tech.opentable.co.uk/</id>
  <author>
    <name><![CDATA[OpenTable]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On Strongly Typed Logging]]></title>
    <link href="http://tech.opentable.co.uk/blog/2015/01/23/on-strongly-typed-logging/"/>
    <updated>2015-01-23T13:13:13+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2015/01/23/on-strongly-typed-logging</id>
    <content type="html"><![CDATA[<p>Logging is a crucial element of monitoring highly available systems. It allows not only to find out about errors but also quickly identify their cause. Logs are often used to generate metrics that help business and engineering make informative decisions on future development directions.</p>

<p>At OpenTable we have a central logging infrastructure, that means all logs are stored in the same shared database (ElasticSearch for us). And everybody can access any logs they want without having very specialized knowledge (thanks Kibana!).</p>

<p>ElasticSearch, though living in a NoSQL world, is not actually a schema-free database. Sure, you do not need to provide schema to it but instead ES will infer schema for you from documents you send to it. This is very similar to type inference you can find in many programming languages. You do not need to specify type of field, but if you later on try to assign inappropriate value to it you will get an exception.</p>

<p>This trait of our database goes all the way to the root of our logging system design. Let me explain why I say that we have &lsquo;strongly typed logs&rsquo;.</p>

<h2>In The Beginning There Was String</h2>

<p>Before centralization we just logged a single message along with its importance. In code it looked something like:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>logger.ERROR(“Kaboom!”)</span></code></pre></td></tr></table></div></figure>


<p>which resulted in logline on disk having timestamp, severity and message.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>{2014-10-10T07:33:04Z [ERROR] Kaboom!}</span></code></pre></td></tr></table></div></figure>


<p>That worked pretty well. As time passed we often started making log messages more generic to hold relevant data:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>logger.INFO(string.Format(“Received {0} from {1}. Status: {2}. Took {3}”, httpMethod, sourceIp, statusCode, durationms));</span></code></pre></td></tr></table></div></figure>


<p>When we decided to centralize logs we moved the same logs from local disk to a central database. Suddenly things that used to live on single server in a file called &lsquo;application.log&rsquo; become part of one huge lump of data. Instead of easing access to logs they were really hard to filter, without even speaking about aggregation, or any simple form of operations to find the source of the problem. ElasticSearch is really good at free text searching, but frankly speaking FTS is never as precise as a good filter.</p>

<h2>Then There Was Dictionary Of Strings</h2>

<p>Wherever there is problem there is also a solution. So we changed the way our logging works. We created a custom logger and started sending logs more like documents than single string.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>customLogger.send(‘info’, new Dictionary&lt;string, string&gt; {
</span><span class='line'>{‘method’, httpMethod.ToString()},
</span><span class='line'>{‘sourceIp’, sourceIp.ToString()},
</span><span class='line'>{‘statusCode’, statusCode.ToString()},
</span><span class='line'>{‘duration’, durationms.ToString()},
</span><span class='line'>{‘requestId’, requestId.ToString()},
</span><span class='line'>{‘service’, ‘myservice’}
</span><span class='line'>{‘message’, string.Format(“Received {0} from {1}. Status: {2}. Took {3}”, httpMethod, sourceIp, statusCode, durationms)}
</span><span class='line'>}</span></code></pre></td></tr></table></div></figure>


<p><strong>That helped a lot.</strong></p>

<p>You might wonder why we serialized everything to string? The answer is ElasticSearch mapping as I described above. Mapping, once it is inferred, cannot be changed. So from time to time we used to have conflicts (e.g. one application logging requestId as number, other as guid). Those conflicts were costly &ndash; logs were lost &ndash; so we simply applied the simplest solution available and serialized everything.</p>

<p>Now filtering was working fine. We were even able to group requests based on a single field and count them. You cannot imagine how useful it is to simply count the different status codes returned by a service. Also you may have noticed we introduced some extra fields like &lsquo;service&rsquo; which helped us group logs coming from a single application. We did the same with hostname etc.</p>

<p>With this easy success our appetite has grown and we wanted to log more. And being lazy programmers we found a way to do it quickly so our logs often included just relevant objects.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>customLogger.log(‘info’, request)
</span><span class='line'>customLogger.log(‘error’, exception)</span></code></pre></td></tr></table></div></figure>


<p>Our custom logging library did all the serialization for us. This worked really well. Now we were actually logging whole things that mattered without having to worry about serialization at all. What&rsquo;s even better, whenever the object in question changed (e.g. a new field was added to request), it was automagically logged.</p>

<p>However one thing was still missing. We really wanted to see performance of our application in real time or do range queries (e.g. &ldquo;show me all requests that have 5xx status code&rdquo;). We also were aware that both ES and Kibana can deliver it but our logging is not yet good enough.</p>

<h2>Strongly Typed Logs</h2>

<p>So we looked at our logging and infrastructure and at what needs to be done to allow different types of fields to live in ElasticSearch. And you can imagine that it was a pretty simple fix; we just started using types. Each log format was assigned its own type. This type was then used by ElasticSearch to put different logs into separate buckets with separate mapping. The type is equivalent in meaning to classes in OO programming. If we take this comparison further then each log entry would be an object in OO programming. ElasticSearch supports searches across multiple types, which is very convenient when you don&rsquo;t know what you are looking for. On the other hand, when you know, you can limit your query to single type and take advantage of fields types.</p>

<p>It was a big application change as we needed to completely change our transport mechanism to LogStash. We started with Gelf and switched to Redis, which allowed us to better control format of our logs.</p>

<p>We also agreed on a first standard. The standard defined that type will consist of three parts:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;serviceName&gt;-&lt;logName&gt;-&lt;version&gt;</span></code></pre></td></tr></table></div></figure>


<p>This ensures that each team can use any logs they want to (thus serviceName). Each log will have its own format (thus logName). But they can also change in the future (thus version). One little word of caution, ES doesn&rsquo;t like dots in type name, so don&rsquo;t use them.</p>

<p>So our logs look now like this:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>customLogger.log(new RequestLog {
</span><span class='line'>Request = request,
</span><span class='line'>Headers = headers,
</span><span class='line'>Status = status})</span></code></pre></td></tr></table></div></figure>


<p>RequestLog is responsible for providing valid type to the logging library.</p>

<p>With sending serialized objects as logs and assigning each class unique type our logs have become strongly typed.</p>

<p>We are already couple steps further down the path of improving our logs. We standardized some common fields and logtypes. That, however, is a completely different tale. ​</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building a living styleguide at OpenTable]]></title>
    <link href="http://tech.opentable.co.uk/blog/2015/01/19/building-a-living-styleguide-at-opentable/"/>
    <updated>2015-01-19T17:00:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2015/01/19/building-a-living-styleguide-at-opentable</id>
    <content type="html"><![CDATA[<p>If you&rsquo;re reading this you&rsquo;ve probably built yourself a website.  A site &ndash; large or small &ndash; that&rsquo;s thrown together or crafted over many months.  And if you have, you’ve probably kept all your CSS class names in your head, or at least been able to go straight to the relevant stylesheets to retrieve them.</p>

<p>Well OpenTable is unsurprisingly built by many engineering teams across multiple continents, and was completely redesigned last year.  And as soon as you have more than a handful of people working on your front-end you will quickly find a well-intentioned developer causing one or both of these problems:</p>

<ul>
<li>Well-intentioned developer adds a new submission form but, like the design Philistine he is, his buttons are <span style="font-family:verdana;font-size:18px;color:#E40000;">18px Verdana #E40000</span>, not the correct <span style="font-family:arial;font-size:16px;color:#DA3743;">16px Arial #DA3743</span></li>
<li>Your good old developer knows which font size and colour it should be, but bungs a duplicate class into a random stylesheet (or worse still, inline)</li>
</ul>


<p>Despite these risks, a single front-end dev (or a team of them) cannot check every new piece of code or they will quickly become a bottleneck.</p>

<h3>You need some guidelines</h3>

<p>Offline designers regularly create ‘brand guidelines’ or ‘design standards&#8217; to document the precise way their brand or product should be recreated when outside of their control.  Online, such guidelines are similarly invaluable for maintaining brand and code consistency with multiple engineers and designers, but it is blindingly obvious that a printed or ‘static’ set of guidelines is completely unsuitable for a constantly changing website.</p>

<p>Step forward a ‘living’ styleguide.</p>

<p>A living styleguide gives a visual representation of a site’s UI elements using <strong>the exact same code</strong> as on the website, in most cases via the live CSS.  A living styleguide may also provide reusable CSS and HTML code examples and they are not just for engineers new to the code; I frequently use ours at OpenTable and I wrote the stylesheets in the first place (I can’t be expected to remember everything).</p>

<p>Providing reusable code improves collaboration, consistency and standards, and reduces design and development time &ndash; but like most documentation it is essential your guide is always up-to-date and trustworthy.  So if a living styleguide is (theoretically) always up-to-date, how did we build ours?</p>

<h2>How we built our styleguide</h2>

<p>Living styleguides are not new (although they were <a href="http://sideproject.io/an-exhaustive-look-at-the-year-in-web-design/#styleguides">one of the trends of 2014</a>) and as such many frameworks have been built over the years.  We chose to use <a href="http://kaleistyleguide.com/">Kalei</a> by <a href="https://github.com/thomasdavis">Thomas Davis</a> &ndash; I forget the exact reasons why but it was probably the easiest at the time to set up and customise.</p>

<p>Generating a Kalei styleguide is as simple as adding comments to your stylesheet; Kalei uses a variety of frameworks, including <a href="http://backbonejs.org/">Backbone.js</a>, <a href="http://www.glazman.org/JSCSSP/">JSCSSP</a> and <a href="https://github.com/chjj/marked">Marked</a> to convert these comments into HTML mark-up, generate a list of your individual stylesheets as navigation and present these as a single page web app.</p>

<p>For example in your <code>buttons.css</code> file it is as simple as adding the following comments:</p>

<pre><code>/*!
# Primary buttons
Primary buttons are only used when there is an exceedingly distinct and clear call-to-action.
```
&lt;a href="#" class="button"&gt;Button&lt;/a&gt;
&lt;a href="#" class="button secondary"&gt;Button secondary&lt;/a&gt;
&lt;a href="#" class="button success"&gt;Button success&lt;/a&gt;
&lt;a href="#" class="button alert"&gt;Button alert&lt;/a&gt;
```
*/
</code></pre>

<p>Which, by using the CSS in the file itself, Kalei would visually render like so:</p>

<p><img src="http://tech.opentable.co.uk/images/posts/styleguide-buttons-screenshot.png" alt="styleguide-buttons-screenshot" /></p>

<h3>Customising Kalei</h3>

<p>Kalei works well out-of-the-box but we had to make a few customisations.  These were mostly cosmetic changes, but a fundamental changes was to <strong>add support for Sass</strong>.  For this we wrote a Grunt task imaginatively called <code>grunt styleguide</code> in which we combined <em>Clean</em>, <em>Copy</em>, <em>Scss</em> and <em>Replace</em> tasks.  Unsatisfactorily it took a little while to set up and involved a number of steps, but below is simplification of the process.</p>

<ol>
<li>Clean all CSS files from the styleguide, excluding Kalei specific stylesheets</li>
<li>Copy our partial scss files into a temporary folder and rename them to remove the underscore (partial scss files begin with an underscore are are <a href="http://sass-lang.com/documentation/file.SASS_REFERENCE.html#partials">not compiled by default</a>)</li>
<li>Compile the scss files into CSS in the styleguide directory</li>
<li>Copy across dependent fonts and images, using <em>Replace</em> to update the relative paths</li>
<li>Delete the temporary directory</li>
</ol>


<p>This task is run as a deployment step and can be run locally when developing the guide.</p>

<p>Other that a few small UI tweaks we made one significant changes to the look and feel.  By default the navigation lists stylesheets using their full file name, e.g. <strong>breadcrumbs.css</strong> and <strong>buttons.css</strong>.  Using a regex function in the <code>menu.js</code> file and <code>text-transform: capitalize</code> in the Kalei stylesheet we modify the navigation to display the more attractive headings <strong>Breadcrumbs</strong> and <strong>Buttons</strong>.</p>

<p>View our styleguide at <a href="http://www.opentable.com/styleguide" target="_blank">opentable.com/styleguide</a>.</p>

<h2>What&rsquo;s next?</h2>

<p>Our living styleguide is intended to be an organic resource that we will grow and refine into an integral part of our software development.  We have many ideas for how we want to develop the guide &ndash; at the very least it is currently incomplete insomuch as we have not documented every one of our stylesheets.</p>

<p>There is also a fundamental weakness to this type of styleguide, which is duplication of code.  Whilst we use the exact same CSS as our live site, we are copying and pasting mark-up into these files and this content can go out of date without deliberate upkeep.  At OpenTable we have a <em>site resource service</em> which serves HTML snippets to different internal microsites so one option could be to use this service to integrate these snippets into the styleguide.  We may also investigate a solution using <a href="http://webcomponents.org/">web components</a> as cross-browser support is not a concern.</p>

<p>We are also interested to see whether it would be useful to run UI tests against the styleguide. We have used <a href="http://pdiff.sourceforge.net/">pDiff</a> in the past for visual regression on specific microsites, but the styleguide could be an opportunity to catch accidental, global UI changes.  We are going to look at running <a href="https://garris.github.io/BackstopJS/">BackstopJS</a> against each section of the guide to see if this increases its usefulness.</p>

<p>Finally, as one of the developers who created the styleguide I want it to be widely adopted across OpenTable. I want designers and engineers to contribute to the code and use it for their day-to-day designing and developing, and I want product owners and marketing folks to use it when creating promotional material and A/B tests.  My ultimate goal is for it to be an integral tool enabling everyone to work faster, avoid duplication and maintain a consistent brand identity.</p>

<h2>Read more</h2>

<ul>
<li>View an online directory <a href="http://styleguides.io/">of styleguide articles and examples</a></li>
<li>Join us &ndash; <a href="https://hire.jobvite.com/Jobvite/Job.aspx?b=nlsWXpwA&amp;j=oXeiYfwb">apply for our senior front-end engineer role at OpenTable, London.</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Explaining Flux architecture with macgyver.js]]></title>
    <link href="http://tech.opentable.co.uk/blog/2015/01/01/explaining-flux-architecture-with-macgyver-dot-js/"/>
    <updated>2015-01-01T15:33:46+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2015/01/01/explaining-flux-architecture-with-macgyver-dot-js</id>
    <content type="html"><![CDATA[<h2>What is Flux?</h2>

<p><a href="https://github.com/facebook/flux">Flux</a> is an application architectural pattern developed by Facebook. It was developed to solve some of the complexities of the MVC pattern when used at scale by favouring a uni-directional approach. It is a pattern and not a technology or framework.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/mvc-scale.png" alt="MVC scale issue" /></p>

<p>When applications that use the model-view-controller (MVC) pattern at any scale it becomes difficult to maintain consistent data across multiple views. In particular the case whereby flow between models and views is not uni-directional and may require increasing logic to maintain parity between views when model data is updated. Facebook hit this issue several times and in particular with their unseen count (an incremented value of unseen messages which is updated by several UI chat components). It wasn&rsquo;t until they realised that the MVC pattern accomodated the complexity that they stepped back from the problem and addressed the architecture.</p>

<p>Flux is intentionally unidirectional.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/flux.png" alt="flux" /></p>

<p>Key to this architecture is the dispatcher. The dispatcher forms the gatekeeper that all actions must go through. When a view, or views, wish to do something they fire an action which the dispatcher correctly routes via registered callbacks made by the stores.</p>

<p>Stores are responsible for the data and respond to callbacks from the dispatcher. When data is changed they emit change events that views listen to to notify them that data has changed. The view can then respond accordingly (for example to update/rebind).</p>

<p>This will become more obvious when we go through the macgyver.js example.</p>

<h2>What is macgyver.js?</h2>

<p><a href="https://github.com/stevejhiggs/macgyver">Macgyver</a> is a project fork of <a href="http://mullet.io/">mullet.io</a> by <a href="https://github.com/stevejhiggs">Steve Higgs</a>. Mullet is an aggregate stack to get started using Node.js with Facebook&rsquo;s <a href="http://facebook.github.io/react/">React</a> framework on the client and Walmart&rsquo;s <a href="http://walmartlabs.github.io/hapi/">hapi.js</a> on the server.</p>

<p>Steve initially swapped out Grunt for Gulp, updated hapi and React and fixed some issues with the React dev tools. I then added another example to incorporate the Flux architecture, which you can see <a href="https://github.com/stevejhiggs/macgyver/tree/master/reactPlusFlux">here</a>. As React was also developed by Facebook you can begin to see how flux compliments its design and component based model.</p>

<h2>The macgyver.js Flux example</h2>

<p>The demo is a very simple quiz. In true Macgyver style he is faced with abnormally unrealistic situations armed with impossibly useless &ldquo;every-day&rdquo; items to escape the situation. If you select the correct tool, you proceed to the next situation.</p>

<p><img class="left" src="http://tech.opentable.co.uk/images/posts/structure.png" width="200"></p>

<p>Let&rsquo;s start by going through the uni-directional flow above and at the same time look at the code and its structure.</p>

<p>When the game is first loaded the view fires an action to get the next situation. This is then fired off to the dispatcher, as are all actions.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">receiveSituations</span><span class="o">:</span> <span class="kd">function</span><span class="p">(</span><span class="nx">data</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="nx">AppDispatcher</span><span class="p">.</span><span class="nx">handleViewAction</span><span class="p">({</span>
</span><span class='line'>          <span class="nx">actionType</span><span class="o">:</span> <span class="nx">MacgyverConstants</span><span class="p">.</span><span class="nx">RECEIVE_SITUATIONS_DATA</span><span class="p">,</span>
</span><span class='line'>          <span class="nx">data</span><span class="o">:</span> <span class="nx">data</span>
</span><span class='line'>      <span class="p">});</span>
</span><span class='line'><span class="p">},</span>
</span></code></pre></td></tr></table></div></figure>


<p>The store registers to listen for events from the dispatcher with a registered callback. It has the job of loading the situation data and emitting an event when this data is changed. In this case the SituationStore.js has the job of setting the current situation for the view to render.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">AppDispatcher</span><span class="p">.</span><span class="nx">register</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">payload</span><span class="p">){</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">action</span> <span class="o">=</span> <span class="nx">payload</span><span class="p">.</span><span class="nx">action</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">switch</span><span class="p">(</span><span class="nx">action</span><span class="p">.</span><span class="nx">actionType</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>      <span class="k">case</span> <span class="nx">MacgyverConstants</span><span class="p">.</span><span class="nx">RECEIVE_SITUATIONS_DATA</span><span class="o">:</span>
</span><span class='line'>          <span class="nx">loadSituationsData</span><span class="p">(</span><span class="nx">action</span><span class="p">.</span><span class="nx">data</span><span class="p">);</span>
</span><span class='line'>          <span class="k">break</span><span class="p">;</span>
</span><span class='line'>      <span class="k">case</span> <span class="nx">MacgyverConstants</span><span class="p">.</span><span class="nx">CHECK_ANSWER</span><span class="o">:</span>
</span><span class='line'>          <span class="nx">checkAnswer</span><span class="p">(</span><span class="nx">action</span><span class="p">.</span><span class="nx">data</span><span class="p">);</span>
</span><span class='line'>          <span class="k">break</span><span class="p">;</span>
</span><span class='line'>      <span class="k">default</span><span class="o">:</span>
</span><span class='line'>          <span class="k">return</span> <span class="kc">true</span><span class="p">;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">SituationStore</span><span class="p">.</span><span class="nx">emitChange</span><span class="p">();</span>
</span><span class='line'>
</span><span class='line'>  <span class="k">return</span> <span class="kc">true</span><span class="p">;</span>
</span><span class='line'><span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p>The React view (in this case Game.jsx) registers an event listener for these changes in the SituationStore using the React &ldquo;componentDidMount&rdquo; function. When the situation is received by the component it rebinds to the data by loading the sitution and the possible answers.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">var</span> <span class="nx">Game</span> <span class="o">=</span> <span class="nx">React</span><span class="p">.</span><span class="nx">createClass</span><span class="p">({</span>
</span><span class='line'>
</span><span class='line'>  <span class="nx">componentDidMount</span><span class="o">:</span> <span class="kd">function</span> <span class="p">()</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">SituationStore</span><span class="p">.</span><span class="nx">addChangeListener</span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">_onChange</span><span class="p">);</span>
</span><span class='line'>      <span class="nx">ToolStore</span><span class="p">.</span><span class="nx">addChangeListener</span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">_onChange</span><span class="p">);</span>
</span><span class='line'>  <span class="p">},</span>
</span><span class='line'>  <span class="nx">componentWillUnmount</span><span class="o">:</span> <span class="kd">function</span><span class="p">()</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">SituationStore</span><span class="p">.</span><span class="nx">removeChangeListener</span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">_onChange</span><span class="p">);</span>
</span><span class='line'>      <span class="nx">ToolStore</span><span class="p">.</span><span class="nx">removeChangeListener</span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">_onChange</span><span class="p">);</span>
</span><span class='line'>  <span class="p">},</span>
</span><span class='line'>  <span class="nx">render</span><span class="o">:</span> <span class="p">...</span>
</span><span class='line'><span class="p">});</span>
</span></code></pre></td></tr></table></div></figure>


<p>When the user selects an answer this fires off another &ldquo;CHECK_ANSWER&rdquo; event to the dispatcher. The situation store recieves this event with the answer in the payload and checks whether the answer selected is the correct one. If it is it updates the situation and emits a changes event to which the view receives and rebinds the view to the new situation.</p>

<h2>Conclusion</h2>

<p>Flux can be quite difficult to fathom eventhough it is quite a simple architectural pattern. In this small example it does initially feel overly complex and indeed it probably is. The pattern was designed to solve issues that occur at large scale in MVC applications due to the increased amound of bi-directional dependencies between views and models. For smaller applications it could be seen as over-engineered, however I really like the simplicity in the uni-directional flow and the assurance that unit tests are almost always going to mimic the state changes possible in your application because of the guarantee of a simple flow of data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Supporting IE8 in the OpenTable redesign]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/12/08/supporting-ie8-in-the-opentable-redesign/"/>
    <updated>2014-12-08T21:14:57+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/12/08/supporting-ie8-in-the-opentable-redesign</id>
    <content type="html"><![CDATA[<p>We&rsquo;re really <a href="http://blog.opentable.com/2014/opentables-website-re-designed-re-architected-re-imagined/">proud to have released</a> last week our redesigned <a href="http://www.opentable.co.uk">OpenTable</a> site, the culmination of months of hard work from many talented people here in London and in San Francisco.</p>

<p>However despite killing off our old site and its 2004 design, 2.8% of our visitors could have been crying into their keyboard as a far worse opentable.co.uk filled their screen.</p>

<p>That version of OpenTable was our new responsive site viewed in Internet Explorer 8.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/redesign-ie8.png" alt="Our redesign before we optimised for IE8" /></p>

<p>The fundamental issue is that IE8 doesn&rsquo;t support media queries so the age-old browser would try to stretch our <em>mobile-first</em> responsive design as wide as it could go &ndash; not great across a 27&#8221; Thunderbolt.</p>

<p>To solve the problem we first tried the <a href="https://github.com/scottjehl/Respond">Respond.js</a> polyfill but this didn&rsquo;t work as we&rsquo;d hoped.  The main issue appeared to be that because we serve our CSS and JS on a separate sub-domain we fell foul of the browser&rsquo;s cross-domain security.  We followed the Respond.js instructions to solve this but having no luck we looked for alternatives.</p>

<h2>Legacssy</h2>

<p>Further Googling lead us to <a href="https://github.com/robinpokorny/grunt-legacssy">Legacssy</a>. With this Grunt task we could create a IE8-only stylesheet and not have to serve extra JS and cross-domain proxy files to all visitors.</p>

<p>Our existing process is to create our core CSS with an <code>app.scss</code> file and <a href="https://github.com/sindresorhus/grunt-sass">grunt-sass</a>.  Our additional step was to create an <code>app_ie8.scss</code> file, parse it with <code>grunt-sass</code> like before, but then also run it through Legacssy.</p>

<h3>Our app.scss file</h3>

<pre><code>@import 'normalize';

@import 'components/global';

@import
  'components/icons',
  'components/buttons',
  'components/calendar',
  'components/forms',
  'components/pagination',
  'components/star-rating';

@import
  'partials/footer',
  'partials/header',
  'partials/location-picker';
</code></pre>

<h3>And our app_ie8.scss file</h3>

<pre><code>@import ‘app';
@import 'browsers/_ie8.scss';
</code></pre>

<p>This process left us with a duplicate of the main site CSS that would be IE8 friendly &ndash; and as you can see we are also able to merge in an IE8 specific stylesheet with further overrides.  All we now needed to was serve this inside conditional comments after the main stylesheet, and CSS specificity would ensure our IE rules overwrite the media queries.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;!--[if lte IE 8]&gt;
</span><span class='line'>&lt;link rel="stylesheet" href="//eu-srs.opentable.com/content/static/css/app_ie8.css" /&gt;
</span><span class='line'>&lt;![endif]—&gt;</span></code></pre></td></tr></table></div></figure>


<h2>Drawbacks</h2>

<p>We are very happy with this solution as it generates the IE8 fixes as part of our automated build with no extra effort.  The only obvious negative is for the IE8 visitors who will effectively be downloading the same stylesheet twice, but if they are routinely browsing the web with IE8 this could be the least of their worries.</p>

<h2>Conclusion</h2>

<p>We&rsquo;re really proud of our new site; we&rsquo;re still ironing out some kinks but we hope that it&rsquo;s good enough for those of you who visit us with Internet Explorer 8 (probably through no fault of your own).</p>

<p>For the record, here a couple of other issues we found which may help other intrepid developers with their IE debugging in 2015 and beyond.</p>

<h3>Other IE8 issues</h3>

<ul>
<li>The other unsupported CSS values were rem units and RGBA colours.  After running Legacssy we used <a href="https://github.com/robwierzbowski/grunt-pixrem">pixrem.js</a> to replace rems with pixels and a custom task to replace RGBA values with their HEX equivalents.</li>
<li>IE11&rsquo;s F12 developer tools don&rsquo;t render identically to native IE8. We used Microsoft&rsquo;s excellent <a href="http://www.modern.ie">modern.ie</a> site to download virtual machines with Windows 7 and IE8.</li>
<li>We only tested on Windows 7 as this makes up 66% of our IE8 visitors. The remainder are on different operating systems which collectively only make up 0.95% of our total visitors; this is below our threshold for support.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Proxying Services With Hapi.js]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/11/28/proxying-with-hapi/"/>
    <updated>2014-11-28T10:32:42+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/11/28/proxying-with-hapi</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve raved in the past about how awesome <a href="http://hapijs.com">hapi.js</a> is, but I&rsquo;m going to talk about just a specific case today.</p>

<p>We started off with just a couple of hapi.js apis. This was at a time when standing up new infrastructure was still a bit painful, so inevitably those apis ended up having more functionality in them than they should have. Now it&rsquo;s easy for us to get infrastructure, so we want to do more of it.</p>

<p>Our goal is to have lots of small(er) apis that just look after one specific piece (skillfully avoiding using the buzzword &lsquo;microservices&rsquo;).</p>

<p>When you want to split out functionality from one api to another, it can be a pain, especially if you have a lot of consumers who aren&rsquo;t particularly fast-moving or communicative. Or maybe you don&rsquo;t know all your consumers up front.</p>

<p>You&rsquo;ve got a couple of options here:</p>

<ul>
<li><p>Maintain the functionality in two places and slowly migrate consumers across</p></li>
<li><p>Use a proxy or routing layer in-front of the api to rewrite or redirect requests</p></li>
<li><p>Write code in your api to proxy requests to a different server</p></li>
</ul>


<p>The first two options are pretty icky, and frankly the third isn&rsquo;t all that great either. It all depends on you having the right framework. Do you see where I&rsquo;m going here?</p>

<h3>Enter Hapi.js</h3>

<p>Hapi.js has the concept of a &lsquo;proxy&rsquo; handler, which can transparently proxy requests to a different server.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="nx">server</span><span class="p">.</span><span class="nx">route</span><span class="p">([</span>
</span><span class='line'>  <span class="p">{</span>
</span><span class='line'>    <span class="nx">method</span><span class="o">:</span> <span class="s1">&#39;GET&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="nx">path</span><span class="o">:</span> <span class="s1">&#39;/foo&#39;</span><span class="p">,</span>
</span><span class='line'>    <span class="nx">handler</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>      <span class="nx">proxy</span><span class="o">:</span> <span class="p">{</span>
</span><span class='line'>        <span class="nx">host</span><span class="o">:</span> <span class="s1">&#39;my-other-service.mydomain.com&#39;</span><span class="p">,</span>
</span><span class='line'>        <span class="nx">port</span><span class="o">:</span> <span class="mi">80</span><span class="p">,</span>
</span><span class='line'>        <span class="nx">protocol</span><span class="o">:</span> <span class="s1">&#39;http&#39;</span><span class="p">,</span>
</span><span class='line'>        <span class="nx">passThrough</span><span class="o">:</span> <span class="kc">true</span><span class="p">,</span>
</span><span class='line'>        <span class="nx">xforward</span><span class="o">:</span> <span class="kc">true</span>
</span><span class='line'>      <span class="p">}</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">]);</span>
</span></code></pre></td></tr></table></div></figure>


<p>And boom, you&rsquo;re done. You can now safely delete <em>all</em> of that code from your api and move it. The <em>only</em> thing you need to have kicking about is that proxy handler code.</p>

<p>The <code>passthrough</code> setting specifies whether or not to preserve headers on the original request, and <code>xforward</code> tells hapi to add (or append) an &lsquo;x-forwarded-for&rsquo; header to the request.</p>

<p>The proxy handler is really powerful. It can rewrite the request (using <code>mapUri</code>), pass local-state (from the hapi instance) along, reject unauthorised requests, you can even hook into the response and monkey about with it if you want (using <code>onResponse</code>).</p>

<p>For full details, see the <a href="http://hapijs.com/api/v7.5.2#route-options">proxy section</a> of the route options.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hobknob v2.0: A new dimension]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/11/26/hobknob-v2-dot-0-a-new-dimension/"/>
    <updated>2014-11-26T10:11:37+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/11/26/hobknob-v2-dot-0-a-new-dimension</id>
    <content type="html"><![CDATA[<p>Sometimes there is the requirement for more granularity when toggling a feature switch.
Version 2.0 of <a href="https://github.com/opentable/hobknob">Hobknob</a> hopes to address this with feature categories.</p>

<h3>TL;DR.</h3>

<p>Hobknob now allows you to define categories of features that have multiple toggles per feature.</p>

<p>For example, you can define the &lsquo;Domain Features&rsquo; category which allows you to toggle a feature OFF in <code>your-website.com</code>, but ON in <code>your-website.co.uk</code>.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/hobknob-domain-features.png" alt="Domain Features" /></p>

<h2>Categories</h2>

<p>Feature categories are configured with a few pieces of information. For example:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;categories&quot;</span><span class="p">:</span> <span class="p">[</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Simple Features&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Use when you want your feature to be either on or off&quot;</span>
</span><span class='line'>    <span class="p">},</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Domain Features&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Use when you want your features to be toggled separately for different domains&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;values&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;com&quot;</span><span class="p">,</span> <span class="s2">&quot;couk&quot;</span><span class="p">,</span> <span class="s2">&quot;de&quot;</span><span class="p">,</span> <span class="s2">&quot;commx&quot;</span><span class="p">,</span> <span class="s2">&quot;jp&quot;</span><span class="p">,</span> <span class="s2">&quot;ca&quot;</span><span class="p">]</span>
</span><span class='line'>    <span class="p">},</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>      <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;Locale Features&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Use when you want your features to be toggled separately for different locales&quot;</span><span class="p">,</span>
</span><span class='line'>      <span class="nt">&quot;values&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;en-GB&quot;</span><span class="p">,</span> <span class="s2">&quot;en-US&quot;</span><span class="p">,</span> <span class="s2">&quot;fr-CA&quot;</span><span class="p">,</span> <span class="s2">&quot;de-DE&quot;</span><span class="p">,</span> <span class="s2">&quot;ja-JP&quot;</span><span class="p">,</span> <span class="s2">&quot;es-MX&quot;</span><span class="p">]</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>  <span class="p">]</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>Notice that each category (except the simple feature category) provides an array of accepted toggle values.</p>

<p>All non-simple feature toggles will have the key <code>application-name/feature-name/toggle-name</code>.
For example, <code>main-website/show-user-section/com</code>.</p>

<p>Simple features will continue to have the key <code>application-name/feature-name</code>.</p>

<h2>Setting Toggles</h2>

<p>Both simple and non-simple features are added via the application view (which is accessed via the left-hand navigation menu). Simple features are automatically set to false, this value can be changed in the feature view (by clicking the feature name).</p>

<p>A newly added non-simple feature will be initialised with no toggles values. You can add a toggle by clicking the Add Toggle button in the feature view, and choosing which toggle to add.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/hobknob-adding-toggle.png" alt="Adding a toggle" /></p>

<p>New toggles are initially set to OFF. The toggle can be switched in the usual way by clicking the toggle button. You should also see an audit of the change in the panel below.</p>

<h2>Getting Toggles</h2>

<p>All of the Hobknob <a href="https://github.com/opentable/hobknob#hobknob-clients">clients</a> now support getting non-simple features toggles. The only requirement is to pass the name of the toggle, so for example in node:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">var</span> <span class="nx">nonSimpleFeatureInCom</span> <span class="o">=</span> <span class="nx">hobknobClient</span><span class="p">.</span><span class="nx">getOrDefault</span><span class="p">(</span><span class="s1">&#39;feature-name&#39;</span><span class="p">,</span> <span class="s1">&#39;com&#39;</span><span class="p">,</span> <span class="kc">false</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>


<p>It is highly reccommended that you use the get or default methods when accessing non-simple features. Getting a toggle that does not exist for a non-simple feature could be a likely scenario, since you might want to only set a feature for a few toggles explicitly (e.g. com and couk) and then use the default value for the rest (e.g. jp and de).</p>

<p>The behaviour for simple features has not changed, and is backwards compatible in all client libraries.</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">var</span> <span class="nx">simpleFeature</span> <span class="o">=</span> <span class="nx">hobknobClient</span><span class="p">.</span><span class="nx">getOrDefault</span><span class="p">(</span><span class="s1">&#39;simple-feature&#39;</span><span class="p">,</span> <span class="kc">true</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Interacting with ElasticSearch using Hubot]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/11/08/interacting-with-elasticsearch-using-hubot/"/>
    <updated>2014-11-08T10:32:42+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/11/08/interacting-with-elasticsearch-using-hubot</id>
    <content type="html"><![CDATA[<p>At OpenTable, we use a few <a href="">ElasticSearch</a> clusters. Our aim was to be able to interact with our ElasticSearch clusters via <a href="http://www.hipchat.com">HipChat</a> so that we could troubleshoot easily and without having to log into our VPN. We already use <a href="http://hubot.github.com">Hubot</a> as part of our systems workflow, so it made sense to be able to interact with ElasticSearch with it.</p>

<h3>Setting a cluster alias</h3>

<p>When a pager wakes me at 3am, I really do not want to have to try and type the cluster URL into my mobile hipchat client. So the first thing that was added to the script was the ability to give a cluster an alias.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>elasticsearch add alias my-test-alias http://my-cluster.com:9200</span></code></pre></td></tr></table></div></figure>


<p><img src="http://tech.opentable.co.uk/images/posts/elasticsearch-add-alias.png" alt="add-alias" /></p>

<p>This allows us to use that alias for all commands going forward. Please note that you can remove and query aliases as well:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>elasticsearch show aliases</span></code></pre></td></tr></table></div></figure>


<p><img src="http://tech.opentable.co.uk/images/posts/elasticsearch-show-aliases.png" alt="show-alias" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>elasticsearch clear alias my-test-alias</span></code></pre></td></tr></table></div></figure>


<p><img src="http://tech.opentable.co.uk/images/posts/elasticsearch-clear-alias.png" alt="clear-alias" /></p>

<h3>Using the ElasticSearch Cat API</h3>

<p>A lot of what we do with ElasticSearch can be done via the <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat.html">cat</a> API. This has proved extremely useful to get node status, cluster health and index status.</p>

<h4>Cat Health</h4>

<p>As documented <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-health.html#cat-health">here</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>elasticsearch cluster health my-test-alias</span></code></pre></td></tr></table></div></figure>


<h4>Cat Nodes</h4>

<p>As documented <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-nodes.html">here</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>elasticsearch cat nodes my-test-alias</span></code></pre></td></tr></table></div></figure>


<p></p>

<h4>Cat Indices</h4>

<p>As documented <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-indices.html">here</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>elasticsearch cat indexes my-test-alias</span></code></pre></td></tr></table></div></figure>


<h4>Cat Allocation</h4>

<p>As documented <a href="">here</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>elasticsearch cat allocation my-test-alias</span></code></pre></td></tr></table></div></figure>


<h3>Getting the Cluster Settings</h3>

<p>Sometimes when we are rebalancing shards or recycling nodes, we want to be able to control the cluster settings. By using the cluster settings API, can have some insight into the settings currently set on the cluster:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>elasticsearch cluster settings my-test-alias</span></code></pre></td></tr></table></div></figure>


<p>More information about the cluster settings API can be found <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-update-settings.html#cluster-settings">here</a></p>

<h3>Getting the Settings for an Index</h3>

<p>Should we want to start to understand the actual settings that are attributed to an index, we can use the Cat Indices settings API. More information can be found <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-get-settings.html">here</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>elasticsearch index settings my-test-alias my-index-name-2014-11-07</span></code></pre></td></tr></table></div></figure>


<h3>Clearing the cluster Cache</h3>

<p>The last piece of the puzzle we are able to do, is to clear the cache of the ElasticSearch cluster. This can be done as follows:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hubot elasticsearch clear cache my-test-alias</span></code></pre></td></tr></table></div></figure>


<h3>Where can I find the code?</h3>

<p>The code is available on <a href="https://github.com/stack72/hubot-elasticsearch">github</a> or also as an <a href="https://www.npmjs.org/package/hubot-elasticsearch">NPM package</a>. Please feel free to send PRs or create issues on our repository. All feedback is useful.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coach don't rescue]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/10/31/coach-dont-rescue/"/>
    <updated>2014-10-31T10:14:04+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/10/31/coach-dont-rescue</id>
    <content type="html"><![CDATA[<p>I recently attended a fascinating and emotionally-charged talk by <a href="https://twitter.com/sisoma">Samantha Soma</a> at <a href="https://2014.dareconf.com">DareConf 2014</a>, <a href="http://vimeo.com/108047198">&lsquo;How to stop rescuing people&rsquo;</a>. It strongly mirrored my experience of moving into a leadership role and I&rsquo;d recommend anyone with a spare 30mins to watch it.</p>

<p>Samantha&rsquo;s talk made me reflect on how I struggle to coach talented individuals; how I can identify when it&rsquo;s going wrong and what steps I can take to remedy the situation.</p>

<h2>Gold star syndrome</h2>

<p>A new concept for me and a recurring theme throughout the sessions at DareConf, &lsquo;Gold Star Syndrome&rsquo; is a fixation on finding validation for your work. I feel this is a result of early childhood values spinning the perception of working life away from the actual reality. As a child, especially during our school years, we discovered that when we do good things, good things happen to us. Remember how it felt to get that gold star in your spelling test or that A+ on an English essay?</p>

<p>That was great in school and even through to University but working life is a much more terse environment and getting positive reenforcement is much less common. As professionals, the majority of our work goes unnoticed &ndash; until there is a problem or issue to solve. Then we feel open to the stinging criticism but resentful that months of good work went unnoticed.</p>

<p>When we continue to search for a gold star or continually strive for perfectionism we, as individuals, become much more insular and isolated. We tend to avoid showing our work until it is 100% ready and put up a shield to protect us from any feedback, in fear of being made to look stupid or being called a fraud.</p>

<h2>Empowerment</h2>

<p>There is a considerable amount of <a href="http://whatworksforhealth.wisc.edu/program.php?t1=20&amp;t2=6&amp;t3=84&amp;id=311">research</a> to suggest children who have bad experiences and manage to overcome them tend to grow up to become more rounded adults. By encouraging grit and allowing kids to solve their own problems, children learn they are empowered. They become more creative, more respectful, less dependant on others and display less problem behaviour.</p>

<p>This is also relatively easy to implement and can be as simple as involving children in a decision-making process. &lsquo;What do you want to eat with dinner; carrots or broccoli&rsquo;? This might progress to &lsquo;What colour socks do you want to wear?&rsquo; or &lsquo;Which swing do you want to play on?&rsquo;</p>

<p>The concept of preventing yourself from controlling a situation is really key to successful coaching. Rescuing people by dictating an outcome  requires one weak person and one strong person. This propagates itself so people drift towards being a victim or a rescuer. A much better outcome would be a group of confident, empowered individuals who are able to work together.</p>

<h2>Provide tools not solutions</h2>

<p>Even if they are unaware themselves, individuals we coach don&rsquo;t want a solution to their problem. What they want is for you to help them find their own solution. To paraphrase a great line in Samantha&rsquo;s presentation &ndash; &lsquo;Our role is to give people a view of the life they want instead of giving them the life we think they want.&rsquo;</p>

<p>And how can we start doing that? I have started to adopt Samantha&rsquo;s principles and have been staggered by how effective they are in practice:</p>

<ol>
<li><p><a href="http://outsmartyourbrain.com/find-your-emotional-triggers-on-this-list/">Know your triggers</a> &ndash; what words and situations make you race to help a colleague in distress? Are there patterns you can spot and say &lsquo;Hang on I&rsquo;ve been here before&rsquo;?</p></li>
<li><p><a href="http://www.successrockets.com/Blog-Professional-Personal-Development/bid/51210/Leadership-Development-Skill-Detached-Involvement">Maintain engaged detachment</a> &ndash; this is tough and requires us to build and nurture a certain skill set; if all else fails remember <a href="http://www.huffingtonpost.com/karen-ann-kennedy/not-my-circus-not-my-monk_b_5390455.html">&lsquo;Not my circus, not my monkeys&rsquo;</a>.</p></li>
<li><p><a href="http://www.mindtools.com/pages/article/newTMC_85.htm">Appreciative Inquiry</a> &ndash; there are lots of models and frameworks out there but in essence Appreciative Inquiry  boils down to asking someone lots of questions and listening intently to their response, however difficult!</p></li>
<li><p><a href="http://centerx.gseis.ucla.edu/xchange/teacher-leadership/teacher-workroom/reflecting-conversation">Challenge them to reflect on what happened</a> &ndash; before an individual can see past the problem, they need to acknowledge the problem and the events that lead them there.</p></li>
<li><p><a href="http://www.reallifecoaching.net/tips-on-committing-to-your-goals/">Ask them to articulate what they want to happen next</a> &ndash; coach them through how this problem can be solved and what steps they might take to resolve it. This commitment helps to clarify any misunderstanding and sets a clear path forward for coach and coachee.</p></li>
<li><p><a href="http://www.myinternalgps.com/?p=1149">Celebrate their success</a> &ndash; a pat on the back (mentally or physically) can go a long way. Why should individuals receive feedback when there is a problem? Loop back with people and celebrate the small wins as much as possible.</p></li>
</ol>


<h2>Leaders are not lifeguards!</h2>

<p>While not exactly child&rsquo;s play, I hope you have read enough here to spot the danger signs and modify your approach away from the rescuer role that ultimately helps no one.</p>

<p>Effective leaders stop jumping to the rescue and start coaching people through a bad experience, empowering individuals by providing them with the tools to come to their own conclusions and solve their own problems.</p>

<h2>Further reading</h2>

<p><a href="http://outsmartyourbrain.com/stop-fixing-people-what-to-do-when-your-brilliant-ideas-arent-helping/">http://outsmartyourbrain.com/stop-fixing-people-what-to-do-when-your-brilliant-ideas-arent-helping/</a></p>

<p><a href="http://www.huffingtonpost.com/nora-t-akins/resist-the-rescue-management_b_5537309.html">http://www.huffingtonpost.com/nora-t-akins/resist-the-rescue-management_b_5537309.html</a></p>

<p><a href="http://teambuildersplus.com/articles/heroic-leaders-dont-always-save-the-day">http://teambuildersplus.com/articles/heroic-leaders-dont-always-save-the-day</a></p>

<p><a href="http://www.dialogueworks.com/pages/blogs.php?blog_id=52#.VDery9TF87d">http://www.dialogueworks.com/pages/blogs.php?blog_id=52#.VDery9TF87d</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hobknob v1.0: Now with authorization]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/10/22/hobknob-v1-dot-0-now-with-authorization/"/>
    <updated>2014-10-22T14:00:31+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/10/22/hobknob-v1-dot-0-now-with-authorization</id>
    <content type="html"><![CDATA[<p>We are pleased to announce the version 1.0 release of <a href="https://github.com/opentable/hobknob">Hobknob</a>, our open-source feature toggle management system. With it comes a few additions and several improvements.</p>

<p>This post will expand on some of the changes, in particular, authorisation via access control lists.
For an introduction to Hobknob, see our previous post: <a href="http://tech.opentable.co.uk/blog/2014/09/04/introducing-hobknob-feature-toggling-with-etcd/">Introducing Hobknob: Feature toggling with etcd</a>.</p>

<h2>Authorisation with ACLs</h2>

<p>A much requested feature was the ability to control who can add/update/delete toggles on an application by application basis. We achieve this via the use if an Access Control List for each application. Users that are part of the ACL for an application are known as application owners.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/hobknob-owners.png" alt="Hobknob Owner List" /></p>

<p>Application owners can (for an owned application):</p>

<ul>
<li>Add toggles</li>
<li>Set the value of a toggle</li>
<li>Delete toggles</li>
<li>Add additional owners</li>
<li>Remove owners</li>
</ul>


<p>Everyone can:</p>

<ul>
<li>Add an application</li>
<li>See toggles</li>
<li>See application owners</li>
<li>See the audit trail for a toggle</li>
</ul>


<p>When a user creates an new application, they are automatically added as an owner for that application.
The user can then add other application owners by clicking the &lsquo;Add user&rsquo; button in the Owners panel and entering the users email address.</p>

<p><strong>Note:</strong> this feature is only available when authentication is enabled. If Hobknob is not configured to require authentication, everyone has owner permissions to all applications. See the <a href="https://github.com/opentable/hobknob#configuring-authentication">readme</a> for more information on how to configure authentication.</p>

<h2>Deleting Toggles</h2>

<p>Feature toggles can now be deleted. This ability is available on the toggle view (get there by clicking a toggle name in the application view).</p>

<p><img src="http://tech.opentable.co.uk/images/posts/hobknob-delete.png" alt="Hobknob Toggle Delete" /></p>

<p>You&rsquo;ll notice the delete toggle button in the Danger Zone panel (we didn&rsquo;t steal that idea from Github, honest). You&rsquo;ll need to confirm the delete by clicking the delete button a second time.</p>

<p><strong>Warning:</strong> Deleting a toggle will perform a &lsquo;hard&rsquo; delete, that is, the key is deleted in etcd. The audit will persist however, and can be accessed via this route: <code>/#!/applications/app-name/toggle-name</code>. You are also allowed to re-add a toggle, and the audit will be appended to an existing audit for that toggle name.</p>

<p><strong>Note:</strong> If authentication is enabled, you must be an application owner to delete a toggle.</p>

<h2>Makeover</h2>

<p>Gone is the &lsquo;Add Toggle&rsquo; modal dialog from the previous version. This is replaced by two separate inline forms.</p>

<p>Applications are now added by clicking &lsquo;Add&rsquo; in the sidebar.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/hobknob-newapplication.png" alt="Hobknob New Application" /></p>

<p>Toggles are added by clicking &lsquo;New Toggle&rsquo; in the Toggles panel for an application.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/hobknob-newtoggle-v2.png" alt="Hobknob New Toggle" /></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PuppetConf 2014 - Part 3]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-3/"/>
    <updated>2014-10-06T13:43:36+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-3</id>
    <content type="html"><![CDATA[<h2>Day 2</h2>

<p>This is our summary of PuppetConf 2014. In our <a href="http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-2/">previous post</a> we gave an overview of the first day of the conference. This post will provide an
overview of the final day.</p>

<p>There were even more inspiring keynotes and lots more talks which have given us plenty of ideas to go home and think about.</p>

<h3>Key Notes</h3>

<h4>Animating the Puppet: Creating a Culture of Puppet Adoption &ndash; Dan Spurling (<a href="https://twitter.com/spurling">@spurling</a>), Getty Images &ndash; <a href="http://www.slideshare.net/PuppetLabs/keynote-animating-the-puppet-creating-a-culture-of-puppet-adoption-puppetconf-2014">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="http://tech.opentable.co.uk/images/posts/puppetconf-dan.jpg">
</div>


<p>Dan Spuring, VP of Tech Services at Getty came out of the gate with a strong message. His <a href="http://www.urbandictionary.com/define.php?term=GSD">GSD</a> t-shirt
giving you a clear understanding of who he is. His talk about creating a culture of Puppet adoption at his company was a great story of how challenging it
can be to move various business units with projects of various ages to a configuration-management (with Puppet) ethos.</p>

<p>I think it is good to hear that they are rolling cm out into that huge backlog of legacy infrastructure that we all try to pretend isn’t there.
How do you make it integrate into existing processes? How do you sell the DevOps message at the same time as introducing a tool like Puppet into the mix as
part of that message? Dan gave some thoughts on this and it was good to hear some of that from someone who appears to be on the other side of that challenge.</p>

<p>One of the analogies that he used I that found quite useful was that undertaking a project like this is like moving a boulder. It requires an executive sponsor to
get the thing moving at all and then it requires everyone pulling in the same direction if it’s ever doing to get anywhere.</p>

<p>The big take-away was that you need to puppetize right away &ndash; that you can’t wait for the right environment or conditions to start doing it, you just need
start now and demonstrate it. This echo’s the Continuous Delivery ideal of &ldquo;if it hurts, then do it more often&rdquo;.</p>

<h4>Decentralize Your Infrastructure &ndash; Alan Green, Sony Computer Entertainment America &ndash; <a href="http://www.slideshare.net/PuppetLabs/keynote-decentralize-your-infrastructure-alan-green-sony-computer-entertainment-america">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="http://tech.opentable.co.uk/images/posts/puppetconf-alan.jpg">
</div>


<p>Alan’s talk posed an interesting argument: decentralise and let your developers choose the tools and services that they want &ndash; just make it easy for them to
do so. This obviously flies in the face of conventional sysadmin wisdom of trying to centralise, standardise and control everything but for an organisation the
size of scale of SCEA this is just never going to work. Sony has many different studios, each has their own special requirements and tooling that they need to
try and support.</p>

<p>The story of the interaction with these studios is a great classic sysadmin story that is worth repeating. It starts with something we have all heard before &ldquo;I
need to X right now because it’s preventing me from releasing this game on time”. The reaction here is to either say Yes and risk burning out your people getting
it done or No risk your career if the release date gets pushed. As a sysadmin you&rsquo;re on the back-foot at this point &ndash; you pretty much have to do whatever it takes.
If you decentralize your infrastructure you get to turn the tables &#8220;No I don’t have tool X but we do have tool Y and Z that will meet your needs&rdquo;. This gives
the engineers/managers the choice to make rather than you &ndash; they can go out on their own and implement their first choice tool and it will take a bit longer or
they can have something supported by the team right now. Alan also made a interesting call-back to Kate Matsudaira’s keynote of the previous day when he said that
it’s all about honesty and trust. Be truthful with your engineers about what you are capable of achieving or not.</p>

<p>This is the sort of thing we do here at OpenTable and it’s been working very well. You need to design puppet to be as flexible as possible and to support those
teams that need support in their puppet implementations. Having a diverse set of tools is not a bad thing &ndash; especially when you are dealing with creative people &ndash;
it keeps them creative and you can push that creativity back into the product. You&rsquo;re also decentralising control, giving teams the ability to move their
infrastructure as fast as they need to move the product &ndash; meaning that your business is going to move faster get meet it’s ROI (because managers care about that
sort of thing)</p>

<h4>Q&amp;A with Luke Kanies</h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="http://tech.opentable.co.uk/images/posts/puppetconf-luke-2.jpg">
</div>


<p>The last &ldquo;keynote&rdquo; of the conference brought Luke back to the stage for a Q&amp;A with the audience. Allowing people to text in questions live led to some amusement
and once the silly questions were out of the way ( what is your favourite book?, what is your favourite animal? ) we got down to some of the big questions that
people really wanted answers to.</p>

<p><strong>Q</strong>: What is the roadmap for Puppet Apps?<br/>
<strong>A</strong>: I would be surprised if we release more than one per quarter, I’d rather put out four than 20, with five releases for each app. We are a small company,
and we have to try not to get overextended to the point where we can’t evolve the apps. They have to be evolved to be successful.</p>

<p>This seems fair, they is a lot of work involved in putting together something that is polished and tested and ready for market.</p>

<p><strong>Q</strong>: What is the future of Open Source Puppet?<br/>
<strong>A</strong>: My goal is to keep the two products complementary, and to understand each is used for different reasons .. We’re trying to change how the market works
and thinks and this is done better with software that’s absolutely everywhere.</p>

<p>He probably gets asked this all the time. The more features that are poured into Enterprise it would be easy to think that the OSS efforts are diminishing and
that there is even motivation for them to close-source. My conversations with various parties suggest that this is far from the case and I think that open source
puppet community will continue to be vibrant for a long time yet.</p>

<p><strong>Q</strong>: Where does Puppet fit into environments that don’t require convergence, where instead of adjusting the container you just re-provision?<br/>
<strong>A</strong>: Containers are a result of 10 to 15 years of investment in virtualization, so it’s easy to switch from the virtualization world to the containers world —
but a container can’t do everything.</p>

<p>This is a very pragmatic argument and he’s right. Containers are a very exciting space right now and there is no doubt that it will be a big part of the future
but the community and tooling needs to mature and there is also going to be a very long tail of “traditional” virtualisation technologies around for a very
long time yet.</p>

<p><strong>Q</strong>: Are there any plan to integration remote orchestration into Puppet?<br/>
<strong>A</strong>: It’s an area we are investing heavily in, and I’m personally investing heavily in. … I’m a big fan of small independent tools that do one job and do it
correctly, rather than big huge tools that do a lot. I want to make our orchestration better, not by adding to Puppet, but by adding tools. I don’t want to add
more functionality to Puppet, but add functionality to the Puppet ecosystem.</p>

<p>MCollective has been in the puppet eco-system for a while now. It’s going to be getting a lot more attention over the next year so I am very excited to see how
this evolves.</p>

<h3>Tech Talks</h3>

<h4>Continuous Integration for Infrastructure as Code &ndash; Gareth Rushgrove, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/continuouslytestinginfrastructure">Slides</a></h4>

<p>Arguable one of the most interesting talks of the conference. This talk took the idea of infrastructure TDD to the next level. What would it be like to be able
to test common expectations of your infrastructure (monitoring, backups, machines in each region, budget limitations). There are lots of built-in assumptions
that we make about of infrastructure and a lot of business decisions that have been difficult to codify. This talk raising the challenge of providing a complete
API for your infrastructure and then testing against it.</p>

<ul>
<li>usual tools (serverspec, wrecker)

<ul>
<li>for containers</li>
<li>TDD</li>
</ul>
</li>
<li>Policy Driven development</li>
<li>Infrastructure as an API</li>
<li>common expectations (budget etc)</li>
<li>clojure</li>
<li>can you generate serverspec tests from PuppetDB data??? &ndash; yes!</li>
<li>rake test::role::web_server</li>
<li><a href="https://github.com/garethr/serverspec-puppetdb">serverspec-puppetdb</a></li>
<li>rspec outputter &ndash; monitoring &ndash; using it as a bridge</li>
</ul>


<h4>Experiences from Running Masterless Puppet &ndash; Erik Dalén, Spotify &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf-2014-1">Slides</a></h4>

<p>Erik (this years MVP) always has a lot of interesting insights about Puppet from scaling out the infrastructure at Spotify and this talk is no exception.
This talk explains their decision to go masterless and the challenges in doing so. It seems that they have put in a lot of work in writing services to manage
things like hiera data and managing secrets. It is great to see how this approach scales, one can only hope that future work by PuppetLabs with the Apps project
improves this as option for most people.</p>

<ul>
<li>scaling workflow rather than puppet masters</li>
<li>complex modules dependencies make it easy to break things</li>
<li>r10k is still a fixed environment (upgrade apache and progress at the same time)</li>
<li>they use their own tool for secret management</li>
</ul>


<h4>Getting Started with Puppet on Windows &ndash; Josh Cooper, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf2014-gettingstartedwindowsfinal140925174855phpapp01">Slides</a></h4>

<p>This was a basic introduction to Puppet on windows. It covers what is possible and the many edge cases that you might run into. It was also the time to
re-announce the recent support for 64-bit puppet on windows. Thanks to Josh we also got a shout-out for the work we have done with our
<a href="forge.puppetlabs.com/opentable">forge modules</a></p>

<ul>
<li>Basic intro</li>
<li>powershell, registry_key</li>
<li>installing &ndash; mention of 64-bit</li>
<li>puppet resource</li>
<li>supported modules</li>
<li>community modules (inc OT)</li>
<li>geppetto vs VS</li>
<li>problems

<ul>
<li>quotes</li>
<li>case sensitivity</li>
<li>UAC</li>
</ul>
</li>
</ul>


<h4>Test Driven Development with Puppet &ndash; Gareth Rushgrove, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/tddforpuppet-39598529">Slides</a></h4>

<p>This is Gareth’s basic introduction to TDD with Puppet. It covers the latest tooling and how to build yourself a recent CI pipeline for your modules so that
they are forge-ready. Useful for anyone who is new to the space or who hasn’t released any modules yet.</p>

<ul>
<li>TDD</li>
<li><a href="http://rspec-puppet.com/">rspec-puppet</a></li>
<li><a href="http://puppet-lint.com/">puppet-lint</a></li>
<li><a href="https://github.com/guard/guard-rspec">guard</a></li>
<li><a href="https://github.com/gds-operations/puppet-syntax">puppet-syntax</a></li>
<li><a href="https://github.com/puppetlabs/beaker">beaker</a> (vagrant + serverspec)</li>
<li><a href="https://travis-ci.org/">travis</a></li>
<li><a href="https://github.com/garethr/puppet-module-skeleton">puppet module skeleton</a></li>
</ul>


<h4>Using Docker with Puppet &ndash; James Turnbull, Kickstarter &ndash; <a href="http://www.slideshare.net/PuppetLabs/using-docker-with-puppet-puppetconf-2014">Slides</a></h4>

<p>James gave a good introduction to Docker. Showing off the things that Docker is good at and also detailing some of the things that it isn’t.
He also showed how and when to use Puppet in this environment. For anyone moving from a  traditional set-up to a Docker based one then this talk is a must.</p>

<ul>
<li>what is docker</li>
<li>dockerfile</li>
<li>dockerhub</li>
<li>what it does</li>
<li>what it doesn’t

<ul>
<li>low-level</li>
<li>resource dependencies</li>
<li>what runs, when</li>
</ul>
</li>
<li>don’t install puppet inside your containers</li>
<li>puppet apply</li>
</ul>


<h4>Tools and Virtualization to Manage our Operations at Puppet Labs &ndash; Cody Herriges, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/tools-and-virtualization-to-manage-our-operations-at-puppet-labs-puppetconf-2014">Slides</a></h4>

<p>Cody, is a member of the PuppetLabs operations team and wow they seriously have their work cut out for them. They have to manage pretty much every network,
vm technology and cloud platform available. This gives some of the challenges in doing that and some of the tools they have built to help them in
achieving that.</p>

<ul>
<li>all the VM technologies</li>
<li>all the cloud platforms</li>
<li>all the network providers</li>
<li>automation</li>
<li>monitoring (ELK)</li>
<li>vmpooler (<a href="https://github.com/puppetlabs/vmpooler">https://github.com/puppetlabs/vmpooler</a>)</li>
</ul>


<h3>Other Talks</h3>

<ul>
<li>The Switch as a Server &ndash; Leslie Carr, Cumulus Networks &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-switch-as-a-server-puppetconf-2014">Slides</a></li>
<li>Intro to Using MCollective &ndash; Devon Peters, Jive Software &ndash; <a href="http://www.slideshare.net/PuppetLabs/intro-to-using-mcollective-puppetconf-2014">Slides</a></li>
<li>How Puppet Enables the Use of Lightweight Virtualized Containers &ndash; Jeff McCune, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/how-puppet-enables-the-use-of-lightweight-virtualized-containers-jeff-mc-cune-puppet-labs">Slides</a></li>
<li>Server Locality Using Razor and LLDP &ndash; Jonas Rosland, EMC &ndash; <a href="http://www.slideshare.net/PuppetLabs/server-locality-withrazorandlldp">Slides</a></li>
<li>Node Classifier Fundamentals &ndash; Dan Lidral-Porter, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/node-classifier-fundamentals-dan-lidralporter-puppet-lab">Slides</a></li>
<li>What&rsquo;s Next for Puppet Enterprise &ndash; Lindsey Smith, Puppet Labs &amp; Susannah Axelrod, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/whats-next-for-puppet-enterprise-and-beyond">Slides</a></li>
<li>The DevOps Field Guide to Cognitive Biases (2nd Edition) &ndash; Lindsay Holmwood, Bulletproof Networks</li>
<li>Delegated Configuration with Multiple Hiera Databases &ndash; Robert Terhaar, Atlantic Dynamic &ndash; <a href="http://www.slideshare.net/PuppetLabs/rob-terhaar-puppetconf2014">Slides</a></li>
<li>Understanding OpenStack Deployments &ndash; Chris Hoge, OpenStack Foundation &ndash; <a href="http://www.slideshare.net/PuppetLabs/understanding-openstack-deployments-puppetconf-2014">Slides</a></li>
<li>Implementing Puppet at a South American Government Agency, Challenges and Solutions &ndash; Pablo Wright, Edrans &ndash; <a href="http://www.slideshare.net/PuppetLabs/implementing-puppet-at-a-south-american-government-agency-challenges-and-solutions-pablo-wright-edrans">Slides</a></li>
<li>Infrastructure as Software &ndash; Dustin J. Mitchell, Mozilla, Inc. &ndash; <a href="http://www.slideshare.net/PuppetLabs/infrastructure-as-software-dustin-j-mitchell-mozilla-inc?">Slides</a></li>
<li>Dev to Delivery with Puppet &ndash; Sam Bashton, Bashton Ltd. &ndash; <a href="http://www.slideshare.net/PuppetLabs/dev-to-delivery-with-puppet-sam-bashton-bashton-ltd">Slides</a></li>
<li>Get Puppet Enterprise into Your Company &ndash; Iko Saadhoff, KPN</li>
<li>The Puppet Master on the JVM &ndash; Chris Price, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-puppet-master-on-the-jvm-puppetconf-2014">Slides</a></li>
<li>The Grand Puppet Sub-Systems Tour &ndash; Nicholas Fagerlund, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-grand-puppet-subsystems-tour-nicholas-fagerlund-puppet-labs">Slides</a></li>
<li>Building Community: One Puppet Module at a Time &ndash; Diane Mueller, Red Hat &amp; Diego Castro, Getup Cloud</li>
<li>Puppet for Everybody! &ndash; Federated and Hierarchical Puppet Enterprise &ndash; Chris Bowles, University of Texas at Austin &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppet-for-everybody-federated-and-hierarchical-puppet-enterprise-puppetconf-2014">Slides</a></li>
<li>Puppetizing Multitier Architecture &ndash; Reid Vandewiele, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetizing-multitier-architecture-puppetconf-2014">Slides</a></li>
<li>The Evolving Design Patterns of Puppet Enterprise &ndash; Jonathan Spinks, Sourced Group &amp; John Painter, Sourced Group &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-evolving-design-patterns-of-puppet-enterprise-jonathan-spinks-sourced-group-john-painter-sourced-group">Slides</a></li>
<li>From Development to Testing to Deployment with Puppet Enterprise and Microsoft Azure &ndash; Ross Gardler, Microsoft Open Technologies, Inc. &ndash; <a href="http://www.slideshare.net/PuppetLabs/from-development-to-testing-to-deployment-with-puppet-enterprise-and-microsoft-azure-ross-gardler-microsoft-open-technologies-inc">Slides</a></li>
<li>Exploring the Final Frontier of Data Center Orchestration: Network Elements &ndash; Jason Pfeifer, Cisco &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf-cisco">Slides</a></li>
<li>An In-Depth Introduction to the Puppet Enterprise Console &ndash; Ruth Linehan, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/an-indepth-introduction-to-the-puppet-enterprise-console-ruth-linehan-puppet-labs">Slides</a></li>
<li>Packaging Software, Puppet Labs Style &ndash; Melissa Stone, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/packaging-software-puppet-labs-style-puppetconf-2014">Slides</a></li>
<li>Orchestrated Functional Testing with Puppet-spec and Mspectator &ndash; Raphaël Pinson, Camptocamp &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf-mspectator-talk">Slides</a></li>
<li>Fully Automate Application Delivery with Puppet and F5 &ndash; Colin Walker, F5 &ndash; <a href="http://www.slideshare.net/PuppetLabs/i-control-rest-presentation-for-puppet">Slides</a></li>
<li>Managing the File and Exposing the API &ndash; Christopher Webber, Chef Software</li>
<li>Case Study: Developing a Vblock Systems Based Private Cloud Platform with Puppet and VMware vCloud Suite &ndash; Peng Liu &amp; Paul Harb, VCE &ndash; <a href="http://www.slideshare.net/VCE_Computing/puppet-confvce-preso20140925">Slides</a></li>
<li>Got Logs? Get Answers with Elasticsearch ELK &ndash; Jordan Sissel, Elasticsearch &ndash; <a href="http://www.slideshare.net/PuppetLabs/got-logs-get-answers-with-elasticsearch-elk-puppetconf-2014">Slides</a></li>
<li>Managing Network Security Monitoring at Large Scale with Puppet &ndash; Michael Pananen &amp; Chris Nyhuis, Vigilant Technology Services &ndash; <a href="http://www.slideshare.net/PuppetLabs/managing-network-security-monitoring-at-large-scale-with-puppet-puppetconf-2014">Slides</a></li>
<li>Building and Testing from Scratch a Puppet Environment with Docker &ndash; Carla Souza, Reliant &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppet-conf2014">Slides</a></li>
</ul>


<h3>Other Interesting Links</h3>

<ul>
<li><a href="http://blog.superk.org/2014/09/puppet-conf-2014-review.html">http://blog.superk.org/2014/09/puppet-conf-2014-review.html</a></li>
<li><a href="http://www.olindata.com/blog/2014/09/first-impressions-new-cfacter">http://www.olindata.com/blog/2014/09/first-impressions-new-cfacter</a></li>
<li><a href="http://cwebber.net/blog/2014/09/26/i-am-not-a-coder/">http://cwebber.net/blog/2014/09/26/i-am-not-a-coder/</a></li>
<li><a href="http://www.slideshare.net/PuppetLabs/tag/puppetconf-2014">http://www.slideshare.net/PuppetLabs/tag/puppetconf-2014</a></li>
<li><a href="http://puppetlabs.com/blog/puppetconf-2014-day-1-tips-treats-and-tweets">http://puppetlabs.com/blog/puppetconf-2014-day-1-tips-treats-and-tweets</a></li>
<li><a href="http://puppetlabs.com/blog/puppetconf-2014-day-2-luke-q-and-a-devops-containers-and-more">http://puppetlabs.com/blog/puppetconf-2014-day-2-luke-q-and-a-devops-containers-and-more</a></li>
<li><a href="http://puppetlabs.com/blog/puppetconf-2014-day-1-tips-treats-and-tweets">http://puppetlabs.com/blog/puppetconf-2014-day-1-tips-treats-and-tweets</a></li>
<li><a href="http://puppetlabs.com/blog/puppet-conf-2014-wrap-up">http://puppetlabs.com/blog/puppet-conf-2014-wrap-up</a></li>
<li><a href="https://forge.puppetlabs.com/approved/criteria">https://forge.puppetlabs.com/approved/criteria</a></li>
<li><a href="http://puppetlabs.com/blog/puppet-server-bringing-soa-to-a-puppet-master-near-you">http://puppetlabs.com/blog/puppet-server-bringing-soa-to-a-puppet-master-near-you</a></li>
<li><a href="https://github.com/puppetlabs/puppetlabs-strings/">https://github.com/puppetlabs/puppetlabs-strings/</a></li>
<li><a href="http://bitergia.dev.puppetlabs.com/browser/">http://bitergia.dev.puppetlabs.com/browser/</a></li>
<li><a href="https://www.flickr.com/photos/pleia2/sets/72157648049231891">https://www.flickr.com/photos/pleia2/sets/72157648049231891/</a></li>
<li><a href="http://theshipshow.com/2014/10/the-pulse-of-puppetconf-2014/">http://theshipshow.com/2014/10/the-pulse-of-puppetconf-2014/</a></li>
<li><a href="http://www.theregister.co.uk/2014/09/23/puppetconf_2014_keynote/">http://www.theregister.co.uk/2014/09/23/puppetconf_2014_keynote/</a></li>
<li><a href="http://www.infoq.com/news/2014/09/puppet-approved-modules">http://www.infoq.com/news/2014/09/puppet-approved-modules</a></li>
<li><a href="https://github.com/ferventcoder/puppet-chocolatey-presentation">https://github.com/ferventcoder/puppet-chocolatey-presentation</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PuppetConf 2014 - Part 2]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-2/"/>
    <updated>2014-10-06T12:30:58+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-2</id>
    <content type="html"><![CDATA[<h2>Day 1</h2>

<p>This is our summary of PuppetConf 2014. In our <a href="http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-1/">previous post</a> we gave an overview of the contributor summit. This post will provide an overview
of the first day of PuppetConf.</p>

<p>As you might expect there were great keynotes with plenty of announcements and too many talks for us to attend. We have provided an outline for all the talks
we did attend and links to those we didn&rsquo;t.</p>

<h3>KeyNotes</h3>

<h4>Nearly a Decade of Puppet: What We’ve Learned and Where We’re Going Next &ndash; Luke Kanies, PuppetLabs &ndash; <a href="http://www.slideshare.net/PuppetLabs/luke-kanies-keynote-nearly-a-decade-of-puppet-what-weve-learned-and-where-were-going-next-puppetconf-2014">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="http://tech.opentable.co.uk/images/posts/puppetconf-luke-1.jpg">
</div>


<p>The big keynote of the event to kick off the first day from the author of Puppet himself. This was obviously going to be a tweet worthy affair full of photos
and big announcements and it did not disappoint.</p>

<p>Native Clients (CFactor + C++ rewrite of agents) are coming in the very near future. This is not only a matter of improving the performance for existing users part
of philosophy of PuppetLabs to become ubiquitous across as many devices and platforms as possible. This is one of those improvements that is really setting up
PuppetLabs for the future.</p>

<p>Puppet Server (a.k.a the Clojure rewrite). This is PuppetLabs big move away from Ruby on onto the JVM. Being on the JVM means they can slowly rewrite the
codebase while also maintaining compatibility thanks to JRuby. They have gained a lot of experience with Clojure thanks to the PuppetDB &amp; TrapKeeper projects and given how
successful that project has been it has helped ease many of the fears people have in moving the JVM. Puppet Server is also a self contained application so there
is no longer any need to worry about the whole apache/passanger yak shave. There was even a demo on the metrics that are now exposed by Puppet Server &ndash; yes
you can now plug Puppet into graphite.</p>

<p>There have been plenty of follow-ups on this that you might be interested in reading:</p>

<ul>
<li><a href="http://www.infoworld.com/article/2687553/devops/puppet-server-drops-ruby-for-clojure.html">http://www.infoworld.com/article/2687553/devops/puppet-server-drops-ruby-for-clojure.html</a></li>
<li><a href="http://puppetlabs.com/blog/puppet-server-bringing-soa-to-a-puppet-master-near-you">http://puppetlabs.com/blog/puppet-server-bringing-soa-to-a-puppet-master-near-you</a></li>
<li><a href="http://puppetlabs.com/blog/new-era-application-services-puppet-labs">http://puppetlabs.com/blog/new-era-application-services-puppet-labs</a></li>
<li><a href="https://github.com/puppetlabs/puppet-server">https://github.com/puppetlabs/puppet-server</a></li>
<li><a href="http://www.informationweek.com/cloud/software-as-a-service/puppet-servers-big-revamp/d/d-id/1315934">http://www.informationweek.com/cloud/software-as-a-service/puppet-servers-big-revamp/d/d-id/1315934</a></li>
</ul>


<p>Puppet Apps was the next big announcement. Puppet Apps is actually a fantastic piece of marketing around the idea that they are refactoring to a more micro-services
style approach &ndash; splitting up the monolith that is currently the Puppet master into smaller applications that have their own release cadence and can be scaled
separately.</p>

<p>The first announcement from the &ldquo;Apps&rdquo; initiative is Puppet Node Manager the new node classifier which will roll out in the Q1 of 2015 as an add-on
to Puppet Enterprise. Given that Puppet has allowed external node classifiers to be written for a long time now (and there are many open source ones out there)
it is good to see PuppetLabs stepping up and trying to own this more and improve the experience.</p>

<p><a href="http://puppetlabs.com/about/press-releases/puppet-labs-kicks-puppetconf-announcements-major-updates-industrys-most-popular">http://puppetlabs.com/about/press-releases/puppet-labs-kicks-puppetconf-announcements-major-updates-industrys-most-popular</a></p>

<p>Another huge announcement (of which we got a preview at the contributors summit) was Puppet Approved Modules. Luke and the rest of PuppetLabs have the huge
idea that 80% of what you&rsquo;re going to want to configure on your systems should be possible with what is available on the forge. Some of the bigger pieces have
been covered by the module engineers at PuppetLabs under the existing Puppet Support Modules program. This has been fantastic in driving for consensus around
configuration making installation of certain products (like apache) easier for people.</p>

<p>The reality is that if PuppetLabs want to achieve its 80% goal they are are not going to be able to do that with the engineers and resources they have
available to them. Nor do they have the expertise to know about all the software out there. This is where the Puppet Approved program comes in. Its aim is to
provide the same standard of quality that you see in the Supported modules but for modules written by the community. It is easy for users of the forge to
be able to pick out high quality, actively maintained modules and know what they are getting. As a user this is very exciting and as a module author, while
there will be plenty of work for me to do, I am glad that the community is moving in this direction.</p>

<p>Speaking of the community, Luke used this opportunity to announce the finalists and the winner of the Most Valued Puppetier (MVP) competition.</p>

<p>Finalists:</p>

<ul>
<li>Daniele Sluijters (<a href="https://twitter.com/daenney">@daenney</a>)</li>
<li>Felix Frank</li>
<li>Tim Sharp (<a href="https://twitter.com/rodjek">@rodjek</a>)</li>
</ul>


<p>Winner</p>

<ul>
<li>Erik Dalén (<a href="(https://twitter.com/erik_dalen">@erik_dalen</a>)</li>
</ul>


<p>The last part of the keynote was talking about some of the wider thoughts as we look to the next ten years of Puppet and what comes next. There is going to be
more focus on the ubiquity of Puppet, on devices more network device partners and solving problems like orchestration. The next ten years is going to be about
taking Puppet beyond the single node. We are already thinking of machines as cattle and not pets &ndash; Puppet should also better reflect that change.</p>

<p>I for one am very excited by all this and look forward to seeing what comes out over the next few years.</p>

<h4>The Phoenix Project: Lessons Learned &ndash; Gene Kim, IT Revolution Press &ndash; <a href="http://www.slideshare.net/PuppetLabs/keynote-the-phoenix-project-lessons-learned-puppetconf-2014">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="http://tech.opentable.co.uk/images/posts/puppetconf-gene.jpg">
</div>


<p>This was a great overview of Gene’s research of DevOps and how that intersects with high performing organisations. There were many interesting results that came
out the the survey that he did in joint co-operation with PuppetLabs many of which he shared during this talk.</p>

<p>I think the one that stands out and often tweeted is the following:</p>

<p><em>&ldquo;High performers have 30x more deployments and 8000x faster lead time, 2x the change success rate and 12x faster recovery&rdquo;</em></p>

<p>Read that again &ndash; wow.</p>

<p>This talk as one might expect was all about DevOps, its history, why and how it works. Even if you&rsquo;re fully familiar with the whole culture of DevOps there are
plenty of things to be learnt from this keynote and I look forward to re-watching it when the video lands on YouTube.</p>

<h4>Trust Me &ndash; Kate Matsudaira, Popforms &ndash; <a href="http://www.slideshare.net/PuppetLabs/keynote-trust-me-puppetconf-2014">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="http://tech.opentable.co.uk/images/posts/puppetconf-kate.jpg">
</div>


<p>Following the theme of culture, Kate’s talk was a refreshing look at the culture of trust within an organisation. Far from being the usual &ldquo;this is what my
company culture looks like&rdquo; sort-of talk, this talk had a lot of practical advice. Discussion of how to build relationships, how to raise your profile within
the organisation and how to improve yourself as a manger. &ldquo;If you use your 1-on-1 to talk about status, you&rsquo;re wasting time. Get to know your boss, solicit
feedback on your performance.&rdquo; &ndash; Great advice like this is littered throughout the talk.</p>

<p>She says that trust is like money and that you need to be wise in how you spend that trust. Most organisations are not a meritocracy and we need to stop thinking that they are. Your relationships within the organisation are just as important as the quality of the work that you do.  There needs to be balance between these two things &ndash; are your relationships as good as the work that you do?</p>

<p>If you want to improve yourself and advance your career, either as an engineer or as a manager then you should absolutely take the time to listen to this talk.</p>

<p><strong>Bonus</strong>: the slides rock! (I won’t spoilt it &ndash; take a look)</p>

<h3>Track Talks</h3>

<h4>The Puppet Debugging Kit: Building Blocks for Exploration and Problem Solving &ndash; Charlie Sharpsteen, Puppet Labs (<a href="https://twitter.com/csharpsteen">@csharpsteen</a>) &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-puppet-debugging-kit-building-blocks-for-exploration-and-problem-solving-charlie-sharpsteen-puppet-labs">Slides</a></h4>

<p>Interesting tool, has some cross-over with the Beaker testing tool. PDK is more for focused manual testing rather than automated acceptance tests.</p>

<ul>
<li><a href="https://github.com/Sharpie/puppet-debugging-kit">https://github.com/Sharpie/puppet-debugging-kit</a></li>
<li>vagrant + oscar (<a href="https://github.com/adrienthebo/oscar">https://github.com/adrienthebo/oscar</a>)</li>
<li>oscar is a collection of vagrant plugins</li>
<li>vagrant-config_builder &ndash;> adds role to share vagrant config  (similar to the beaker nodeset file)</li>
<li>PDK is a set of oscar roles</li>
<li>facter / hiera and Puppet running off GitHub</li>
<li>beaker vs oscar &ndash; oscar is optimised for manual testing. There is room to share stuff here.</li>
</ul>


<h4>Cloudy with a Chance of Fireballs: Provisioning and Certificate Management in Puppet &ndash; Eric Sorenson (<a href="https://twitter.com/ahpook">@ahpook</a>), Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/sorenson-fireballspuppet-conf2014">Slides</a></h4>

<ul>
<li>Apple iCloud uses Puppet + autosign</li>
<li>auto sign doesn&rsquo;t work very well for the cloud</li>
<li>Amazon IAM can be applied by machines &ndash; IAM so instance can read it’s own tags (if it has ec2-client-utils installed)</li>
<li>puts instance_id, ami_id and role into /etc/puppet/csr_attriubutes.yaml</li>
<li>can validate the metadata in the cert using x509</li>
<li>true_node_data = true &amp; immutable_node_data = true</li>
<li>closes security hole of setting certname to fact on agent</li>
</ul>


<h4>Beaker: Automated, Cloud-Based Acceptance Testing &ndash; Alice Nodelman (<a href="https://twitter.com/alicenode">@alicenode</a>), Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/beaker-automated-cloudbased-acceptance-testing-puppetconf-2014">Slides</a></h4>

<p>Having contributed to this tool, I was a little bias in attending this talk. Still plenty of interesting new things that came up though.
If you haven’t heard of beaker yet you will also be interested in our <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">previous blog post</a>.</p>

<ul>
<li>basic introduction to what beaker is and how to use it.</li>
<li>rspec vs test dsl &ndash; both are still supported methods of writing tests.</li>
<li>junit export &ndash; useful when integrating with Jenkins</li>
<li><code>on host as</code> &ndash; is a feature that is coming soon so that you can run a command on a host with a given user account</li>
</ul>


<h4>Puppet Language 4.0 &ndash; Henrik Lindberg (<a href="https://twitter.com/hel">@hel</a>), Puppet Labs  &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppet-language-40-puppetconf-2014">Slides</a></h4>

<p>Lots and lots of interesting information here about the new Puppet 4 syntax and jokes about some of the terrible edge cases of the past. It is good to
know now that with Puppet 4 there is a formal specification for the language so we should no longer see these sorts of weird edge cases of the past.
There are also lots of new features in the language: some to deal with long standing pain points (interation), some to help in the move away from ruby
(Puppet templates) and some to prevent authors themselves writing buggy manifests (the type system). Puppet 4 is going to be an exciting this to use.</p>

<ul>
<li>pain-points / cleanup (specification)

<ul>
<li>numbers are numbers (and not strings)</li>
<li>Type references</li>
</ul>
</li>
<li>heredoc</li>
<li>Puppet templates</li>
<li>iteration (each, map, filter, reduce, slice, with)</li>
<li>local defaults</li>
<li>Type system</li>
</ul>


<h4>7 Puppet Horror Stories in 7 Years &ndash; Kris Buytaert (<a href="https://twitter.com/KrisBuytaert">@KrisBuytaert</a>), Inuits &ndash; <a href="http://www.slideshare.net/KrisBuytaert/7-years-of-puppet-horror-stories">Slides</a></h4>

<p>This was more of an interactive talk, trying to get members of the audience to try and predict what the actual problem was. For more senior Puppetiers
this was a fun talk, reminding us of the challenges many of us have faced. For newer Puppet developers this was likely acting as a good warning and
foreshadowing of things that may arise if your not careful (or are very unlucky).</p>

<ul>
<li>SSL</li>
<li>Full Disk</li>
<li>Puppet Bugs</li>
<li>DNS (everything is a DNS problem)</li>
</ul>


<h4>Killer R10K Workflow &ndash; Phil Zimmerman (<a href="https://twitter.com/phil_zimmerman">@phil_zimmerman</a>), Time Warner Cable &ndash; <a href="http://www.slideshare.net/PuppetLabs/killer-r10k-39571913">Slides</a></h4>

<p>This was a good introduction to r10k and the reasons you would want to use it. The workflow is pretty straightforward and I think that for anyone managing Puppet at scale this is going to be something to look at.</p>

<ul>
<li>some good use cases for r10k

<ul>
<li>upgrading modules</li>
<li>not having to wait for all role tests to run</li>
<li>deploying everything to all masters (even hiera)</li>
</ul>
</li>
<li>workflow

<ul>
<li>ci per module</li>
<li>release job per module (tags)</li>
<li>deploy job per module (cap task to wrap r10k for masters/nodes)</li>
</ul>
</li>
</ul>


<h3>Other Talks from the Day</h3>

<ul>
<li>Infrastructure-as-Code with Puppet Enterprise in the Cloud &ndash; Evan Scheessele, HP &ndash; <a href="http://www.slideshare.net/PuppetLabs/infrastructure-ascode-with-puppet-enterprise-in-the-cloud-evan-scheessele-hp">Slides</a></li>
<li>Getting Started with Puppet &ndash; Michael Stahnke, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/getting-started-with-puppet-puppetconf-2014">Slides</a></li>
<li>Plan, Deploy &amp; Manage Modern Applications Leveraging vCloud Automation Center and Puppet &ndash; Pradnesh Patil, VMware &ndash; <a href="http://www.slideshare.net/PuppetLabs/plan-deploy-manage-modern-applications-leveraging-vcloud-automation-center-and-puppet-puppetconf-2014">Slides</a></li>
<li>Writing and Publishing Puppet Modules &ndash; Colleen Murphy, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/writing-and-publishing-puppet-modules-colleen-murphy-puppet-labs">Slides</a></li>
<li>To the Future! &ndash; Goals for Puppet 4 &ndash; Andrew Parker, Puppet Labs &amp; Kylo Ginsberg, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/to-the-future-goals-for-puppet-and-facter-1">Slides</a></li>
<li>Managing and Scaling Puppet &ndash; Miguel Zuniga, Symantec &ndash; <a href="http://www.slideshare.net/PuppetLabs/managing-and-scaling-puppet-puppetconf-2014-39542923">Slides</a></li>
<li>What Developers and Operations Can Learn from Design: 6 Ways to Work Better Together &ndash; Ashley Hathaway, IBM Watson &ndash; <a href="http://www.slideshare.net/PuppetLabs/what-developers-and-operations-can-learn-from-design-6-ways-to-work-better-together-puppetconf-2014">Slides</a></li>
<li>Performance Tuning Your Puppet Infrastructure &ndash; Nic Benders, New Relic &ndash; <a href="http://www.slideshare.net/PuppetLabs/performance-tuning-your-puppet-infrastructure-nic-benders-new-relic">Slides</a></li>
<li>&ldquo;Sensu and Sensibility&rdquo; &ndash; The Story of a Journey From #monitoringsucks to #monitoringlove &ndash; Tomas Doran, Yelp &ndash; <a href="http://www.slideshare.net/PuppetLabs/130pm-210pm-tomas-doran-track-1-puppetconf2014-sensu">Slides</a></li>
<li>DevOps Means Business &ndash; Gene Kim, IT Revolution Press &amp; Nicole Forsgren Velasquez, Utah State University &ndash; <a href="http://www.slideshare.net/PuppetLabs/devops-means-business-gene-kim-it-revolution-press-nicole-forsgren-velasquez-utah-state-university">Slides</a></li>
<li>Auditing/Security with Puppet &ndash; Robert Maury, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/auditingsecurity-with-puppet-puppetconf-2014">Slides</a></li>
<li>Absolute Beginners Guide to Puppet Through Types &ndash; Igor Galić, Brainsware OG &ndash; <a href="http://www.slideshare.net/PuppetLabs/absolute-beginners-guide-to-puppet-through-types-igor-galic-brainsware-og">Slides</a></li>
<li>Plugging Chocolatey into Your Puppet Infrastructure &ndash; Rob Reynolds, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/plugging-chocolatey-into-your-puppet-infrastructure-rob-reynolds-puppet-labs">Slides</a></li>
<li>PuppetDB: One Year Faster &ndash; Deepak Giridharagopal, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf-2014">Slides</a></li>
<li>The Puppet Community: Current State and Future Plans &ndash; Dawn Foster, Puppet Labs &amp; Kara Sowles, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-puppet-community-current-state-and-future-plans-dawn-foster-puppet-labs-kara-sowles-puppet-labs">Slides</a></li>
<li>Continuous Delivery of Puppet-Based Infrastructure &ndash; Sam Kottler, Digital Ocean &ndash; <a href="http://www.slideshare.net/PuppetLabs/continuous-delivery-of-puppetbased-infrastructure-puppetconf-2014">Slides</a></li>
<li>The Seven Habits of Highly Effective Puppet Users &ndash; David Danzilio, Constant Contact &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-seven-habits-of-highly-effective-puppet-users-puppetconf-2014">Slides</a></li>
<li>Fact-Based Monitoring &ndash; Alexis Le-Quoc, Datadog &ndash; <a href="http://www.slideshare.net/PuppetLabs/fact-based-monitoring-puppetconf-2014">Slides</a></li>
<li>Test-Driven Puppet Development &ndash; Nan Liu, Bodeco &ndash; <a href="http://www.slideshare.net/PuppetLabs/testdriven-puppet-development-puppetconf-2014">Slides</a></li>
<li>A Practical Guide to Modules &ndash; Lauren Rother, Puppet Labs &amp; Morgan Haskel, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/a-practical-guide-to-modules-lauren-rother-puppet-labs-morgan-haskel-puppet-labs">Slides</a></li>
<li>Leveraging the PuppetDB API: Puppetboard &ndash; Daniele Sluijters, Nedap</li>
<li>Puppet Availability and Performance at 100K Nodes &ndash; John Jawed, eBay/PayPal &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppet-availability-and-performance-at-100k-nodes-puppetconf-2014">Slides</a></li>
<li>DevOps and Software Defined Networking &ndash; John Willis, Pacific Crest</li>
<li>Razor, the Provisioning Toolbox &ndash; David Lutterkort, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/razor-the-provisioning-toolbox-puppetconf-2014">Slides</a></li>
<li>How to Puppetize Google Cloud Platform &ndash; Katharina Probst, Google, Matt Bookman, Google &amp; Ryan Coleman, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/how-to-puppetize-google-cloud-platform-katharina-e">Slides</a></li>
<li>Continuous Infrastructure: Modern Puppet for the Jenkins Project &ndash; R.Tyler Croy, Jenkins &ndash; <a href="http://www.slideshare.net/PuppetLabs/continuous-infrastructure-modern-puppet-for-the-jenkins-project-rtyler-croy-jenkins">Slides</a></li>
<li>How to Measure Everything: A Million Metrics Per Second with Minimal Developer Overhead &ndash; Jos Boumans, Krux &ndash; <a href="http://www.slideshare.net/PuppetLabs/how-to-measure-everything-a-million-metrics-per-second-with-minimal-developer-overhead-puppetco">Slides</a></li>
<li>How to Open Source Your Puppet Configuration &ndash; Elizabeth Krumbach Joseph, HP &ndash; <a href="http://www.slideshare.net/PuppetLabs/how-to-open-source-your-puppet-configuration-elizabeth-krumbach-joseph-hp">Slides</a></li>
<li>Manageable Puppet Infrastructure &ndash; Ger Apeldoorn, Freelance Puppet Consultant &ndash; <a href="http://www.slideshare.net/PuppetLabs/manageable-puppet-infrastructure-ger-apeldoorn-freelance-puppet-consultant">Slides</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PuppetConf 2014 - Part 1]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-1/"/>
    <updated>2014-10-06T11:47:57+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-1</id>
    <content type="html"><![CDATA[<p><img src="http://tech.opentable.co.uk/images/posts/puppetconf2014.jpg" alt="The start of PuppetConf 2014" /></p>

<p>It has been one week since our attendance at this years PuppetConf and we have just now caught up on all the great talks that were
given and the projects demonstrated over the 3 day period. Here&rsquo;s our summary of the event (split into 3 parts), hopefully you will
find as much inspiration in the content as we have.</p>

<h2>Day 0 &ndash; Contributor Summit</h2>

<p>For the first time, this years Puppet Contributor Summit was held the day prior to the conference itself and I think this was a great idea.
Most of the Puppetlabs staff and many of the high profile community members were in town for PuppetConf anyway so it made sense. There was
roughly 60-70 people in attendance both senior contributors and people new to the community so it was a great mix that led to some
fantastic discussions.</p>

<p>The day itself had two tracks: a module track for forge modules and a core track for people contributing to puppet and factor.</p>

<p>Those of you who have seen our <a href="http://forge.puppetlabs.com/opentable">forge module page</a> will understand why we chose to stay in the module track.
Although I heard there were many great discussions to be had with regards to Puppet 4 in the core track.</p>

<p>Each track was split into three sections: a brief introduction from the track lead Ryan Coleman (<a href="https://twitter.com/ryanycoleman">@ryanycoleman</a>),
followed by some lighting talks and then several hours of hacking and open discussions.</p>

<h3>Lightning Talks:</h3>

<p>Here is a quick overview of the lightening talks from the module track:</p>

<h4>Puppet Analytics (Spencer Krum <a href="https://twitter.com/nibalizer">@nibalizer</a>)</h4>

<p>Spencer gave a quick demonstration of his latest project <a href="http://puppet-analytics.org/">puppet-analytics</a>. This problem that this tool was aiming to
solve was that at the present time the are no good analytics for the forge modules. The number of downloads listed for each module is very inaccurate
and can be easily inflated by (for example) an automated CI process. The point of this web app and it’s corresponding client
<a href="https://github.com/nibalizer/puppet-analytics-client">puppet-analytics-client</a> was to be built into an existing tool chain and for end users to report
which modules and versions they were using. It also has the added benefit that we could also get stats for teams using private forges.</p>

<p>Ryan also commented that PuppetLabs has some metrics it uses for it’s own modules that can be found here:
<a href="http://forge-module-metrics.herokuapp.com/">http://forge-module-metrics.herokuapp.com/</a></p>

<h4>Puppet Community (Daniele Sluijters <a href="https://twitter.com/daenney">@daenney</a>)</h4>

<p>Discussion of the shared namespace for community modules: <a href="http://puppet-community.github.io/">puppet-community</a>. This talk was about a community project
to keep modules in a shared namespace so that everyone can work on them independent of company ownership. There are limitations right now with with
regards to the forge e.g. no shared accounts and no easy migration path to move modules between namespaces but working with Ryan on that.</p>

<p>This is how the boxen project works and it seems to work pretty well.</p>

<h4>Beaker Testing Windows Environments (Liam Bennett <a href="https://twitter.com/liamjbennett">@liamjbennett</a>) &ndash; me!!</h4>

<p>My talk on hacking beaker to work better for testing windows environments.</p>

<p>Demos and PRs. Discussed more in some of our earlier posts: <a href="http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-2-the-windows-story/">Testing Puppet with Beaker pt.2 &ndash; The Windows story</a>
and <a href="http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-3-testing-roles/">Testing Puppet with Beaker pt.3 &ndash; Testing Roles</a></p>

<h4>Module Anti-Patterns (Peter Souter <a href="https://twitter.com/petems">@petems</a>) &ndash; <a href="http://www.slideshare.net/petems/puppet-module-anti-patterns">Slides</a></h4>

<p>Some interesting patterns here that are still quite preverlant in the modules found on the forge. Hopefully improved tooling and the new Puppet Approved
program will help here.</p>

<h4>Puppetlabs ModuleSync tool (Colleen Murphy <a href="https://twitter.com/pdx_krinkle">@pdx_krinkle</a>)</h4>

<p>A demonstration of the the tool <a href="https://github.com/puppetlabs/modulesync">puppetlabs-modulesync</a> which aims to take out some of the pain of managing common
static build files across a number of modules (e.g. a common Rakefile or .travis.yml which the same across almost all modules)</p>

<p>Having used this on a number of our modules now I can say that this in extremely useful and I don’t know how we managed without it. A key use case for us was
adding support for puppet 3.7 into our test matrix of our travis.yml file. 1 line change &ndash; 1 command &ndash; 18 modules updated.</p>

<h4>Strict Variables (Tomas Doran <a href="https://twitter.com/bobtfish">@bobtfish</a>)</h4>

<p>Tomas has one very good point to make here: enable <a href="https://docs.puppetlabs.com/references/latest/configuration.html#strictvariables">strict_variables</a>. Many
languages have a strict option and Puppet’s makes sure to check for those unknown variable references. The latest version of the
<a href="https://github.com/puppetlabs/puppetlabs_spec_helper">puppetlabs_spec_helper</a> supports adding this setting with an environment variable so that you can now
add this into your testing matrix.</p>

<p>We have enabled this on our open source modules and it did indeed surfice a few bugs so go and do it now.</p>

<h4>Puppet Documentation Linting (Peter Souter <a href="https://twitter.com/petems">@petems</a>)</h4>

<p>While we have very good linking for our puppet manifests themselves thanks to the <a href="http://puppet-lint.com/">puppet-lint</a> project. We still do not have any
coverage for our documentation of those manifests. That is where Peter’s <a href="https://github.com/petems/puppet-doc-lint">puppet-doc-lint</a> project comes in and aims
to lint each of you manifests for correct rdoc documentation.</p>

<p>This only supports puppet 3.4.3 right now but it is a useful tool and demonstrations something functional in an area that is missing from the current crop of
community tooling. This is going to become more useful as we want to have good documentation for Puppet Approved status.</p>

<p>It is also worth noting that PuppetLabs themselves have been doing some work in this area with
<a href="https://github.com/puppetlabs/puppetlabs-strings/">puppet-strings</a>. This projects works on puppet 3.6 + and support yard doc but is roughly the same idea.</p>

<h4>Quick Survey (Michael Stahnke <a href="https://twitter.com/stahnma">@stahnma</a>)</h4>

<p>Michael here decided to use the opportunity of having everyone in the room to ask a few questions regarding the state of puppet use and the platforms it’s
deployed on. Not too many surprises here: Debian (mostly ubuntu 12) and RedHat (mostly centos 6) dominate with a small grouping of other platforms like AIX and
Solaris in toe. Some poor individuals still have ubuntu 8 and 10 in production but I won’t name name’s because we have all been in that position before. No
mention of windows but then I did bring that up in my own talk so I think that was covered already.</p>

<h3>Hacking and Discussions</h3>

<p>The second part of the day was the hacking and discussions part. This was more un-conference style with variables tables put together to discuss various topics,
try and resolves issues or hack on projects. There were four main areas that I noticed: module testing, windows, docker, forge improvements (apologies if I
missed your topic/table).</p>

<h4>Module Testing</h4>

<p>This was probably the most common topic and several tables were set up around this idea but a huge range of things were discussed. Some people wanted to know
about how to get up and running with beaker tests using vagrant+vagrant cloud, some wanted to discuss specific platform issues (windows, docker, solaris), other
 wanted to discuss how best to scale out the tests once you have them.</p>

<p>There was some discussion based around the tools like puppet-doc-lint that were demonstrated during the lightning talks and it’s good to see these missing
aspects of the testing tool chain getting some light.</p>

<p>It’s nice to think that we have moved to this stage now where we have all the tools to support a full development tool chain for puppet and that most of the
discussion was around improving and maturing what we have.</p>

<h4>Docker</h4>

<p>Docker is one of those tools that can be considered the latest hotness so it’s no surprise that it gained some interest here also. Many people wanted to see it
in use and demonstrated and to discuss it’s use either from the point of view of being able to test with it or test against it.</p>

<p>I see this topic getting a lot more coverage in the future as more and more teams move into this space.</p>

<h4>Windows</h4>

<p>Led by myself, Drewi Wilson and Travis Fields (<a href="https://twitter.com/tefields">@tefields</a>) the two aims for this discussion were to gather input/feedback from people using
the existing windows modules and to try and discover areas in the windows ecosystem that were not currently managed (either well or at all) by Puppet.</p>

<p>We got some fantastic feedback we got regarding our OpenTable modules &ndash; thank you to everyone who was there any everyone else who reached out about that.</p>

<p>We also managed to start to populate a list of things that need some work. You can contribute to that list
<a href="https://docs.google.com/document/d/1bwgTo4D7lL8REA1s-IIKlfMrvY434Xn0cyZ7b1X-TwQ">here</a></p>

<p>There was also some discussion of using MCollective on Windows. This has been a little painful in the past (I should blog about this in the future) but it will
be getting a little more love going forward. Generally PuppetLabs is very aware of the orchestration space and will be looking into solving this problem with
it’s tools going forward.</p>

<h4>Forge Improvements</h4>

<p>Given that this was the Module track it was obvious that at some point we would all want to discuss improvement that we would want to see in the puppet forge. Ryan
led the table here and there was lots of be said by all.</p>

<p>A couple of interesting documents emerged that you might be interested in:</p>

<ul>
<li><a href="https://docs.google.com/document/d/1N8U_8UnIGFHC1Q6aTyLgx1d6wvvjuyTT1EO-OYSIu3k">Suggestions for the Puppet Approved module criteria</a></li>
<li><a href="https://docs.google.com/document/d/1gwoM8xHnWaRQ3Jqce0oursI_ts5BWnHEUVXRQuIh6Yk">Forge Improvements</a></li>
</ul>


<p>There was also some discussion of how best to pull stats out from the forge. Many people either scrape the API, use the API to take a dump of the whole of the forge
but none of these approaches are best for either the user or for the forge site itself. PuppetLabs uses various approaches to this internally depending on the use
case. Such use cases include: &ldquo;who is using my module?&rdquo; or &ldquo;who is using the bit of code?&rdquo;. There should be improvements to the forge to make answering these sorts
of questions a little easier in the future.</p>

<h2>Summary</h2>

<p>The contributor summit was personally one of the most useful days of the conference. Being able to see the lastest tooling and discuss the latest problems is always
very useful to module authors like ourselves. Hopefully you&rsquo;ll find this summary as useful as we do.</p>

<p>Next up Day 1 &ndash; PuppetConf proper..</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing Hobknob: Feature toggling with etcd]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/09/04/introducing-hobknob-feature-toggling-with-etcd/"/>
    <updated>2014-09-04T20:09:52+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/09/04/introducing-hobknob-feature-toggling-with-etcd</id>
    <content type="html"><![CDATA[<p>The ability to dynamically turn features on/off in software without the need to redeploy code is extremely beneficial. Whether you are trialing a new feature or using branch by abstraction to avoid creating feature branches, the use of feature toggles can aid continuous delivery and provide a mechanism to reduce mean time to resolution when an issue occurs.</p>

<p>With a relatively large engineering department with multiple teams spread across the US and UK the need to manage feature toggles has evolved to the point whereby individual teams have developed their own implementations. Most of these are simple config files.</p>

<p>We decided to unify this effort by providing a central place to store feature toggles, provide a dashboard to be able to turn these toggles on/off and provide language specific clients to integrate into our software components.</p>

<p>The results of this was <a href="https://github.com/opentable/hobknob">Hobknob</a>.</p>

<h2>Why etcd?</h2>

<p>We made the decision to use <a href="https://github.com/coreos/etcd">etcd</a>. Etcd is &ldquo;a highly-available key value store for shared configuration&rdquo; (<a href="https://github.com/coreos/etcd#etcd">https://github.com/coreos/etcd#etcd</a>). It provides a HTTP API to store and retrieve data. This is what makes it perfect for a feature toggling solution used by multiple components. It means that we didn&rsquo;t have to write an intermediate API on top of a data store for consumers.</p>

<p>So, for example, to store a feature toggle in etcd:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -L http://127.0.0.1:4001/v2/keys/v1/toggles/restaurant-api/testtoggle -XPUT -d value="true"</span></code></pre></td></tr></table></div></figure>


<p>To retrieve a feature toggle:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -L http://127.0.0.1:4001/v2/keys/v1/toggles/restaurant-api/testtoggle</span></code></pre></td></tr></table></div></figure>


<h2>The Hobknob Clients</h2>

<p>To aid adoption we created, and open sourced, several hobknob clients in multiple languages:</p>

<ul>
<li>NodeJs (NPM) &ndash; <a href="https://github.com/opentable/hobknob-client-nodejs">https://github.com/opentable/hobknob-client-nodejs</a></li>
<li>.NET (Nuget) &ndash; <a href="https://github.com/opentable/hobknob-client-net">https://github.com/opentable/hobknob-client-net</a></li>
<li>Go &ndash; <a href="https://github.com/opentable/hobknob-client-go">https://github.com/opentable/hobknob-client-go</a></li>
<li>Java (Maven) &ndash; <a href="https://github.com/opentable/hobknob-client-java">https://github.com/opentable/hobknob-client-java</a></li>
</ul>


<p>The clients all store a configurable in-memory cache that is periodically updated on a polling interval. They are all read-only and updates only occur on the dashboard where they can be audited.</p>

<p>We decided to create a simple <a href="https://github.com/opentable/hobknob-demo">demo application</a> to show off how easy it is to use Hobknob in your applications. In order to try the demo you will need to start up Hobknob (see instructions below). The demo app uses the NodeJS client which is as simple as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var client = new Client("hobknob-demo", {
</span><span class='line'>  etcdHost: etcdHost,
</span><span class='line'>  etcdPort: etcdPort,
</span><span class='line'>  cacheIntervalMs: 5000
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p>In the route definition it uses the client to request the toggle named <em>show-first-and-last-name-input</em> and passes the toggle value through to the view:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var result = hobknobClient.getOrDefault('show-first-and-last-name-input', true);
</span><span class='line'>res.render('server', {
</span><span class='line'>                  page: 'server',
</span><span class='line'>              useTwoFieldNameInput: value
</span><span class='line'>              });</span></code></pre></td></tr></table></div></figure>


<p>The view then uses the value to decide whether to display one or two textboxes on the page:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>if useTwoFieldNameInput
</span><span class='line'>  input.form-control.demo-input-small(type='text', placeholder='First name', name='firstname')
</span><span class='line'>  input.form-control.demo-input-small(type='text', placeholder='Last name', name='lastname')
</span><span class='line'>else
</span><span class='line'>  input.form-control.demo-input-large(type='text', placeholder='Full name', name='fullname')</span></code></pre></td></tr></table></div></figure>


<h2>The Hobknob Dashboard</h2>

<p>Hobknob is a NodeJS/AngularJS app. If you want to play with Hobknob the simplest way to get started is to use Vagrant. If you don&rsquo;t have it installed then get it from <a href="http://www.vagrantup.com/">http://www.vagrantup.com/</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/opentable/hobknob
</span><span class='line'>cd hobknob
</span><span class='line'>vagrant up</span></code></pre></td></tr></table></div></figure>


<p>You should now be able to open the dashboard on <a href="http://127.0.0.1:3006">http://127.0.0.1:3006</a></p>

<p><img src="http://tech.opentable.co.uk/images/posts/hobknob-dashboard.png" alt="Hobknob dashboard" /></p>

<p>All actions in the dashboard are audited. So when you create or update a toggle by turning it on/off an audit is written for that toggle. Clicking on a toggle in the dashboard takes you to the audit view:</p>

<p><img src="http://tech.opentable.co.uk/images/posts/hobknob-audit.png" alt="Hobknob audit" /></p>

<h3>Authentication</h3>

<p>By default Hobknob ships with authentication disabled. As a result all auditing will be recorded as &ldquo;Anonymous&rdquo;. Currently, we only support Google OAuth. To enable this follow the instructions <a href="https://github.com/opentable/hobknob/blob/master/README.md#configuring-authentication">here</a></p>

<h3>Session Storage</h3>

<p>By default Hobknob ships using in-memory session storage. You don&rsquo;t want to use this when you have a load balanced infrastructure. Hobknob supports both redis and etcd itself as a session store. To use either of these simply npm install the relevent connect middleware (<a href="https://github.com/visionmedia/connect-redis">connect-redis</a> or <a href="https://github.com/opentable/connect-etcd">connect-etcd</a>). To learn more follow the instructions <a href="https://github.com/opentable/hobknob/blob/master/README.md#configuring-session">here</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Puppet with Beaker pt.3 - Testing Roles]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-3-testing-roles/"/>
    <updated>2014-09-01T13:09:05+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-3-testing-roles</id>
    <content type="html"><![CDATA[<p>In the first two parts of this blog series we have focusing on testing puppet <em>modules</em> with beaker. As an open source contributor there is always
a large test matrix so this makes absolute sense. But what about the other large use-case for beaker &ndash; what about our day-to-day internal code base?
Not all of this is modules, in fact a large portion of it is other puppet code &ndash; roles, profiles, facts, hiera data etc. All of this needs testing
as well.</p>

<p>In this blog post I will be showing how we have started using beaker to test our puppet roles and profiles for both Linux and Windows.</p>

<h2>Master-vs-Masterless</h2>

<p>Prior to this post all our beaker testing has been master-less i.e. using using puppet agent apply. This is perfectly adequate for most use cases when
testing modules in isolation but doesn&rsquo;t always work when testing an internal code base (unless you are masterless there as well then please skip to the next section).</p>

<p>At OpenTable we do use a central puppet master to compile our catalogs. So when testing our puppet roles we wanted to make sure that we were also testing
with a master-agent configuration. It is worth mentioning here that if (like us) you are testing windows agents then you are going to need to test with master-agent
approach due to the lack of a windows master.</p>

<p>Testing the master-agent configuration means configuring multi-node sets in beaker. There are not many examples of this but the principle is very much
the same as the single-node nodeset. Here is an example:</p>

<pre><code>HOSTS:
  ubuntu-server-12042-x64-master:
    roles:
      - master
    platform: ubuntu-12.04-amd64
    box: ubuntu-server-12042-x64-vbox4210-nocm
    box_url: http://puppet-vagrant-boxes.puppetlabs.com/ubuntu-server-12042-x64-vbox4210-nocm.box
    hypervisor: vagrant
    ip: '10.255.33.135'
  win-2008R2-std:
    roles:
      - default
      - agent
    platform: windows-server-amd64
    box: opentable/win-2008r2-standard-amd64-nocm
    box_version: = 1.0.0
    box_check_update: false
    hypervisor: vagrant
    user: vagrant
    ip: '10.255.33.129'
    communicator: bitvise
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>In this example you will see that we are specifying different &lsquo;roles&rsquo; for each host in the nodeset. What a role is in in this context is a tag for that node that allows
us to reference it directly later when running commands on the host. To avoid any further confusion, from this point onwards if I am referring to the role defined in the
nodeset file I will call it the &lsquo;nodeset role&rsquo; otherwise I am referring the the puppet role provided in the manifest. There are a couple of build-in nodeset roles that
Beaker already knows about: master, agent and default. The first two are pretty self explanatory but the last nodeset role &ndash; default &ndash; is the location where the tests
themselves run. In you don&rsquo;t specify the &lsquo;default&rsquo; nodeset role on any of your host definitions then the tests will run on the first host that you specified in in the
nodeset file (which in the case of the example above would be wrong).</p>

<p>You may have a more complicated configuration that you wish to test and this allows you to specify arbitrary tags which can be very useful.</p>

<p>We can now use these nodeset roles to configure our master and agent.</p>

<p>In parts <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">[1]</a> and <a href="http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-2-the-windows-story">[2]</a> of this series we saw what a basic spec_acceptence file looks like. So let&rsquo;s start with that:</p>

<pre><code>require 'beaker-rspec/spec_helper'
require 'beaker-rspec/helpers/serverspec'
require 'winrm'

hosts.each do |host|

  if host['platform'] =~ /windows/
    include Serverspec::Helper::Windows
    include Serverspec::Helper::WinRM
  end

  version = ENV['PUPPET_VERSION'] || '3.5.1'
  install_puppet(:version =&gt; version)

  if host['roles'].include?('master')

    ... # Install a master

  else

    ... # Install an agent

  end
end

RSpec.configure do |c|

  c.before :suite do

    hosts.each do |host|
      c.host = host

      if host['platform'] =~ /windows/
        endpoint = "http://127.0.0.1:5985/wsman"
        c.winrm = ::WinRM::WinRMWebService.new(endpoint, :ssl, :user =&gt; 'vagrant', :pass =&gt; 'vagrant', :basic_auth_only =&gt; true)
        c.winrm.set_timeout 300
      end
    end
  end
end
</code></pre>

<p>We can see here how we use the host[&lsquo;roles&rsquo;] in order to select the appropriate code-path for configurting each nodeset role. Now let&rsquo;s
move onto how we configure each of those nodeset roles.</p>

<h2>Configuring the master</h2>

<p>There are a lot of things that go into building a puppetmaster:
 &ndash; puppetmaster packages
 &ndash; hiera backends
 &ndash; gems for addditional dependencies (eyaml + puppetdbquery)
 &ndash; downloading external modules</p>

<p>Now let&rsquo;s step through our new spec_acceptence file that supports this multi-node environment:</p>

<h3>Deploying the codebase</h3>

<p>Stage one is getting our puppet codebase onto the master, which includes all the files, internal modules and anything else we need to get the master up and
running. We do this like follows:</p>

<pre><code>files = [ 'environments','facts','hiera','roles', 'profiles', 'keys', 'app_modules', 'auth.conf','autosign.conf',
          'fileserver.conf', 'Gemfile','hiera.yaml','Puppetfile'
        ]

files.each do |file|
  scp_to master, File.expand_path(File.join(File.dirname(__FILE__), '..', file)), "/etc/puppet/#{file}"
end

# scp dist modules folder (this excludes stuff like spec and test folders)
dist_modules = Dir["#{dist_modules_root}/*/"].map { |a| File.basename(a) }
dist_modules.each do |module_name|
  dist_module_dir = "#{dist_modules_root}/#{module_name}"
  copy_module_to(master, :source =&gt; dist_module_dir, :module_name =&gt; module_name)
end
</code></pre>

<p>Here we are selecting all the files that we want and calling the scp_to method which will scp any file or directory to the host of choice, in this case our
master.</p>

<h3>The puppetmaster:</h3>

<pre><code>...

on master, "apt-get install -y rubygems git"
on master, "apt-get install -y puppet-common=#{version}-1puppetlabs1 puppetmaster-common=#{version}-1puppetlabs1 puppetmaster=#{version}-1puppetlabs1 "
on master, "echo '*' &gt; /etc/puppet/autosign.conf"

...
</code></pre>

<p>So we have already installed puppet at a previous stage in our script. At this point we are performing all the steps required to install the
puppetmaster: git, rubygems (if on an older distro) and the puppetmaster packages. We also making sure that we auto-signing if configured to
save us some pain later on. This step should really be configured as another beaker method that we can just call but for now it is still manual.
It is at this point that we have first introduced the &ldquo;on master&rdquo; this does what you think it might, it executes the command you pass it onto
the host with the nodeset role on &lsquo;master&rsquo;.</p>

<h3>Set the puppet.conf file:</h3>

<pre><code>...

config = {
  'main' =&gt; {
    'server'   =&gt; master_name,
    'certname' =&gt; master_name,
    'logdir'   =&gt; '/var/log/puppet',
    'vardir'   =&gt; '/var/lib/puppet',
    'ssldir'   =&gt; '/var/lib/puppet/ssl',
    'rundir'   =&gt; '/var/run/puppet'
  },
  'agent' =&gt; {
    'environment' =&gt; 'vagrant'
  }
}

configure_puppet(master, config)

...
</code></pre>

<p>Here we are configuring out puppet.conf file, making sure that it includes any customization we might need. This uses a configure_puppet method
that we have added to beaker to allow us to do this customization and in this case it is taking the hash to modify the puppet.conf file on the master
host.</p>

<h3>Install the required ruby gems:</h3>

<pre><code>...

on master, "gem install bundler"
on master, "gem install hiera-eyaml"
on master, "cd /etc/puppet &amp;&amp; bundle install --without development"

...
</code></pre>

<p>The average production-ready puppetmaster also requires a number of gems to function such as hiera-eyaml, deep_merge any many others
depending upon what backends and other custom puppet extensions you have implemented. Here we are installing all our dependencies from
the Gemfile we have already put onto the host.</p>

<h3>Installing modules:</h3>

<pre><code>...

on master, "cd /etc/puppet &amp;&amp; bundle exec librarian-puppet install"

...
</code></pre>

<p>The last major step is installing any external modules you have. You may be using librarian-puppet or r10k to do this. In our case it
is the former so we go ahead and make sure that our modules directory is full of all the modules we require.</p>

<h3>Networking:</h3>

<pre><code>...

master_name = "#{master}.test.local"
on master, "echo '10.255.33.135   #{master_name}' &gt;&gt; /etc/hosts"
on master, "hostname #{master_name}"
on master, "/etc/init.d/puppetmaster restart"

...
</code></pre>

<p>This last step is a small hack that you will probably require if you are running on vagrant. It just configures the host file to make
sure that it&rsquo;s hostname if configured correctly from certificate signing to work as expected. This might not be required in your
environment and I would try it without first but it&rsquo;s worth noting anyway.</p>

<h2>Configuring the agent</h2>

<p>So if you&rsquo;ve got to this point well done &ndash; most of the hard work is done. Configuring the agent(s) is pretty straightforward in comparison
to a puppetmaster and some of the steps are similiar:</p>

<h3>Set the puppet.conf file:</h3>

<pre><code>if host['roles'].include?('master')
  ...
else
  agent = host
  master = only_host_with_role(hosts, 'master')
  agent_name = agent.to_s.downcase
  master_fqdn = "#{master}.test.local"
  agent_fqdn = "#{agent_name}.test.local"

  if agent['platform'] =~ /windows/
    config = {
      'main' =&gt; {
        'server'   =&gt; master_fqdn,
        'certname' =&gt; agent_name,
        'logdir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\log',
        'vardir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\lib',
        'ssldir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\lib\\ssl',
        'rundir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\run'
      },
      'agent' =&gt; {
        'environment' =&gt; 'vagrant'
      }
    }
  else
    config = {
      'main' =&gt; {
        'server'   =&gt; master_fqdn,
        'certname' =&gt; agent_fqdn,
        'logdir'   =&gt; '/var/log/puppet',
        'vardir'   =&gt; '/var/lib/puppet',
        'ssldir'   =&gt; '/var/lib/puppet/ssl',
        'rundir'   =&gt; '/var/run/puppet'
      },
      'agent' =&gt; {
        'environment' =&gt; 'vagrant'
      }
    }
  end
  ...

  configure_puppet(agent, config)
end

...
</code></pre>

<p>Again here we are again using the configure_puppet method, this time change the puppe.conf file on the agent.</p>

<p>As you can see we are catering for both windows and linux hosts here. We are also making sure that the certname
and server are defined properly and match what we set-up for the master so that auto-signing works correctly.</p>

<h2>Testing the Role</h2>

<p>At this point we have now does all our prerequisites and we can spin up two machines to test against &ndash; 1 master and
1 agent. But when we are testing a role what is it that we actually want to test and why is this not covered in
earlier less-expensive puppet-rspec unit tests?</p>

<p>Well there are 3 key things that we wanted to test:
1. Idempotence <br/> This is pretty straight forward to test. Beaker provides a method run_agent_on that will run the puppet agent on a
  given host. This means we can test idempotency like this:</p>

<pre><code>   run_agent_on(agent, :catch_failures =&gt; true)
   expect(run_agent_on(agent, :catch_failures =&gt; true).exit_code).to be_zero
</code></pre>

<ol>
<li><p>Interaction of multiple modules and profiles <br/> This is the big motivator &ndash; we want to test that and make sure that the combinations
of profiles that we are applying work together and do not either break the catalog or operate in a non-idempotent way. We are also gaining
the ability to test that updates in modules (many of which are external from the puppet forge) do not break our roles in any way.</p></li>
<li><p>Postivie/Negative testing &ndash; do we clean up after ourselves if we remove something. <br/> This is not something that is often considered
very often, particularly in a world where machines are torn down and re-build so often but there still exists a use-case where this is not
always possible and we want to make sure that our roles and manifests are not littering our machines unnecessarily.</p></li>
</ol>


<p>Below is a full example of one of our linux profiles:</p>

<pre><code>require 'spec_helper_acceptance'

describe 'linux_base_profile', :if =&gt; fact('osfamily').eql?('Debian') do
  context 'linux base profile' do
    it 'should should run successfully' do

      agent = only_host_with_role(hosts, 'agent')
      master = only_host_with_role(hosts, 'master')

      pp = "node \"#{agent}\" { include profiles::linux::base }"
      on master, "echo '#{pp}' &gt;&gt; /etc/puppet/manifests/site.pp"

      run_agent_on(agent, :catch_failures =&gt; true)
      expect(run_agent_on(agent, :catch_failures =&gt; true).exit_code).to be_zero
    end

    context 'installation of ops tools' do

      describe package('sysstat') do
        it { should be_installed }
      end

      describe package('iotop') do
        it { should be_installed }
      end

      describe package('ngrep') do
        it { should be_installed }
      end

      describe package('lsof') do
        it { should be_installed }
      end

      describe package('unzip') do
        it { should be_installed }
      end
    end

    context 'managing puppet version' do

      describe file('/etc/apt/sources.list.d/puppetlabs.list') do
        it { should be_file }
        it { should be_owned_by 'root' }
        it { should be_mode 644 }
      end

      describe package('puppet') do
        it { should be_installed.by('apt').with_version('3.6.1-1puppetlabs1') }
      end
    end

    context 'manage sshd configuration' do

      describe process("sshd") do
        it { should be_running }
      end

      describe port(22) do
        it { should be_listening }
      end

      describe file('/etc/ssh/sshd_config') do
        its(:content) { should match /PermitRootLogin no/ }
        its(:content) { should match /PasswordAuthentication yes/ }
        its(:content) { should match /UseDNS no/ }
      end
    end
  end
end
</code></pre>

<p>We have a lot of roles and profiles that we would like to test. As you might imagine this could get quite verbose and repetitive pretty quickly. We are currently building
up <a href="https://www.relishapp.com/rspec/rspec-core/docs/example-groups/shared-context">shared_contexts</a> for each of our profiles which we can
then wrap up into roles to easily reflect our roles/profiles structure in the main codebase.</p>

<h2>Summary</h2>

<p>We are just at the very beginning of this journey with Beaker. As well as testing all our modules we are looking to scale our to test the roles and profiles in our whole
code base. These examples here are how we are doing it at the moment for our mixed-platform environment. We will continue to expand upon it and build it into our pipeline.
At this moment we are looking to expand beyond vagrant and run these against AWS instances but perhaps that is for the next post &hellip;</p>

<p>As usual for any questions or comments then please reach out to me on twitter <a href="https://twitter.com/liamjbennett">@liamjbennett</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Testing Puppet with Beaker pt.2 - The Windows story]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-2-the-windows-story/"/>
    <updated>2014-09-01T09:43:10+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-2-the-windows-story</id>
    <content type="html"><![CDATA[<p>In <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">part one</a> we discussed our first steps into the world of acceptance testing our Puppet manifests.
By using Beaker we able to test managing local users on our Linux boxes. This was a positive experience for us. It allowed us to get to grips with the basics of configuring
Beaker to run tests and configuring our node sets to run those tests against. In this post, we will be discussing how we went about getting Beaker working with Windows.</p>

<p>As many of your reading this will be aware, OpenTable currently has quite a large Windows infrastructure and we are using Puppet extensively to maintain that environment.
We are also moving forward with releasing as many of our modules open source onto the <a href="https://forge.puppetlabs.com/opentable">Puppet Forge</a> as possible (11 out of 18 of which are Windows
exclusive). What this means is that there was no way that we could ignore trying to use Beaker to test our manifests against Windows. We knew that we would have to support
many different versions and editions of Windows out there in the community as well that the ones we have to support internally.</p>

<p>This was going to be a challenge (configuration management with Windows usually is) but we were up for it.</p>

<h2>The Preliminaries</h2>

<h3>Serverspec</h3>

<p>The first step was looking at Serverspec. Serverspec is a Ruby gem that provides extensions to RSpec that allow you to test the actual state of your servers, either locally
or from the outside in via SSH. What we needed to know was did it support Windows? The answer was thankfully a resounding &ldquo;Yes!&rdquo;. All the <a href="http://serverspec.org/resource_types.html">resource types</a> that you might want to test including file, service and user are available and supported on Windows. There are also a couple of Windows specific ones such as iis_website, Windows_feature and Windows_registry_key. We even added our own to support <a href="https://github.com/serverspec/serverspec/pull/403/files">Windows_scheduled_task</a>. Interestingly Serverspec also supports WinRM as an alternative to SSH when you are testing from the outside-in but we will go back into that later. As long as your using Serverspec > 1.6 you will have all the Windows support you might need.</p>

<h3>Packer</h3>

<p>Step two was to build some Windows Vagrant boxes to test against. The documentation on the <a href="http://github.com/puppetlabs/beaker/wiki">wiki</a> was (at the time) a bit slim when it came
to building test boxes but we knew we needed Cygwin so we went ahead and created the boxes that we needed. All our boxes are created with Packer <a href="http://github.com/opentable/packer-images">and are open sourced on GitHub</a>. They have also been <a href="http://vagrantcloud.com/opentable">published to Vagrant Cloud</a>
so you can download pre-built images and get up and running quickly (version 1.x images contain the Cygwin installation).</p>

<h3>Beaker</h3>

<p>So far, so good. We hit a couple of issues in our initial test runs with Beaker: missing module_path, installation using the msi and 32-bit Windows support &ndash; but these were very
small issues and we were happy to be able to contribute back some changes (<a href="https://github.com/puppetlabs/beaker/pull/234">234</a>,
<a href="https://github.com/puppetlabs/beaker/pull/235">235</a>, <a href="https://github.com/puppetlabs/beaker/pull/236">236</a>). We were very happy and managed to get out first module tested,
the <a href="http://github.com/opentable/puppet-puppetversion/">cross-platform module puppet-puppetversion</a> for doing Puppet upgrades.</p>

<h2>The First Example</h2>

<p>Let&rsquo;s take a more detailed look at those puppetversion tests, how we configured Beaker to run and how it changed for the Windows support. I am going to assume at this point that
you already have some familiarity with Beaker; if not and this is your first steps into the testing tool then I would suggest going back and read <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">part one</a> of this series which contains a little bit of background to this and some useful resources for getting started.</p>

<p>The first thing that we needed to change for Windows was our <a href="https://raw.githubusercontent.com/opentable/puppet-puppetversion/master/spec/spec_helper_acceptance.rb">spec_accepentance.rb file</a>.</p>

<p>Step one was to include the appropriate Serverspec helpers. What this does is let Serverspec know that we are executing on Windows so that underlying resources work correctly. We are also telling
server spec here to communicate using WinRM.</p>

<pre><code>require 'beaker-rspec/spec_helper'
require 'beaker-rspec/helpers/serverspec'
require 'winrm'

hosts.each do |host|
  case host['platform']
    when /windows/
      include Serverspec::Helper::Windows
      include Serverspec::Helper::WinRM

      version = ENV['PUPPET_VERSION'] || '3.4.3'

      install_puppet(:version =&gt; version)

  else
    install_puppet
  end
end

...
</code></pre>

<p>The next step is to configure WinRM so that it can connect properly. In our case this meant connecting to Vagrant boxes.</p>

<pre><code>...

RSpec.configure do |c|
  ...
  hosts.each do |host|
    c.host = host

    if host['platform'] =~ /windows/
      endpoint = "http://127.0.0.1:5985/wsman"
      c.winrm = ::WinRM::WinRMWebService.new(endpoint, :ssl, :user =&gt; 'vagrant', :pass =&gt; 'vagrant', :basic_auth_only =&gt; true)
      c.winrm.set_timeout 300
    end

    ...
  end
end

...
</code></pre>

<p>Now let&rsquo;s look at one of the tests themselves.</p>

<p>The first part should look pretty familiar. We use the face(&lsquo;osfamily&rsquo;) helper in Beaker to make sure that the test itself is only ever executed for our Windows hosts. We are then running an apply_manifest two times in order to validate that the manifest is idempotent. The only different here is that we are specifying a custom Windows-specific module_path.</p>

<pre><code>...
require 'spec_helper_acceptance'

describe 'puppetversion', :unless =&gt; UNSUPPORTED_PLATFORMS.include?(fact('osfamily')) do
  ...

  context 'upgrade on windows', :if =&gt; fact('osfamily').eql?('windows') do

  it 'should install the new version' do
    pp = &lt;&lt;-PP
      class { 'puppetversion':
        version =&gt; '3.5.1',
        time_delay =&gt; 1
      }
    PP

    apply_manifest(pp, :modulepath =&gt; 'C:/ProgramData/PuppetLabs/puppet/etc/modules', :catch_failures =&gt; true)
    expect(apply_manifest(pp, :modulepath =&gt; 'C:/ProgramData/PuppetLabs/puppet/etc/modules', :catch_failures =&gt; true).exit_code).to be_zero
  end
</code></pre>

<p>The next part is where we actually perform the bulk of the tests. In the case of this module, we are testing
that the scheduled task has run and that Puppet has been upgraded to the appropriate version. We are making use
of the Windows_scheduled_task resource that we created earlier:</p>

<pre><code>  describe Windows_scheduled_task('puppet upgrade task') do
    it { should exist }
  end

  #This will fail if your laptop (and therefor the Vagrant vm) is not running on AC power
  describe package('puppet') do
    it {
      sleep(240) #Wait for the task to execute
      should be_installed.with_version('3.5.1')
    }
  end

  describe Windows_scheduled_task('puppet upgrade task') do
    it {
      pp = &lt;&lt;-PP
        class { 'puppetversion':
          version =&gt; '3.5.1'
        }
      PP

      apply_manifest(pp, :modulepath =&gt; 'C:/ProgramData/PuppetLabs/puppet/etc/modules', :catch_failures =&gt; true)

      should_not exist
    }
  end
end
</code></pre>

<p>The final part was to configure the nodeset file for the Windows box that we wanted to run our test against:</p>

<pre><code>HOSTS:
  windows-2008R2-serverstandard-x64:
  roles:
    - agent
  platform: windows-server-amd64
  box : opentable/win-2008r2-standard-amd64-nocm
  box_url : opentable/win-2008r2-standard-amd64-nocm
  hypervisor : vagrant
  user: vagrant
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>This was working well for us so we continued on to our next module.</p>

<p>The next module we chose to look at was <a href="http://github.com/opentable/puppet-Windowsfeature">puppet-Windowsfeature</a>.</p>

<p>The test we have implemented in this moudle looks like this:</p>

<pre><code>require 'spec_helper_acceptance'

describe 'Windowsfeature' do
  context 'windows feature should be installed' do
    it 'should install .net 3.5 feature' do

      pp = &lt;&lt;-PP
        Windowsfeature { 'as-net-framework': }
      PP

      apply_manifest(pp, :catch_failures =&gt; true)
      expect(apply_manifest(pp, :catch_failures =&gt; true).exit_code).to be_zero
    end

    describe Windows_feature('as-net-framework') do
      it { should be_installed.by('powershell') }
    end
  end
end
</code></pre>

<p>This module was a little more tricky. Why? Installing Windows features requires elevated permissions. What this meant is that when Beaker attempted to SSH into our Windows box and our Puppet module ran its underlying PowerShell we were faced with a harsh and non-descriptive <strong>&ldquo;Access is denied error&rdquo;</strong>.</p>

<h2>SSH</h2>

<p>Not all SSH daemons are created equal. To understand the &ldquo;Access is denied error&rdquo; we were seeing and why it happens you need to understand a little bit about how sshd on Cygwin works. You can read all about the details from the Cygwin forum archives (<a href="http://cygwin.com/ml/cygwin/2004-09/msg00087.html">[1]</a>, <a href="http://cygwin.com/ml/cygwin/2006-06/msg00862.html">[2]</a>, <a href="http://thread.gmane.org/gmane.os.cygwin/128472">[3]</a>, <a href="https://cygwin.com/cygwin-ug-net/ntsec.html#ntsec-nopasswd1">[4]</a>) but TLDR; is that you need to use a username and password rather than private key authentication in order to get reasonable admin permissions. Having said all that, and having read all the documentation about the issue above we were still facing the same problem so we had to look at alternative options.</p>

<p>There are several paths you can go down and I want to tell you about them here to save you a similar yak shave:</p>

<h3>OpenSSH for Windows</h3>

<p>A thinner alternative than having to install the full Cygwin stack using OpenSSH for Windows is the same OpenSSH implementation. The issue here however is that it doesn’t contain some of the Unix binaries required for Beaker to function. You can work around this if you have Git for Windows installed on your machine by putting its bin directory on your path but overall this doesn’t really solve any of the issues we were facing. We get a lighter footprint on the machine but still the same error as before</p>

<h3>Bitvise SSH Server</h3>

<p><a href="http://www.bitvise.com/ssh-server-download">Bitvise SSH Server</a> is an alternative SSH implementation (of which there are many more listed on <a href="http://en.wikipedia.org/wiki/Comparison_of_SSH_servers">Wikipedia</a>). It resolves the permissions issue (it deals with the elevation internally) and has the benefit that it provides a proper command shell rather than a emulated bash shell. It also means we don’t have to have any binaries on there that we don’t need &ndash; a big plus. It would mean that we needed to make a few small changes to Beaker in order to replace some of the internal bash command with Windows alternatives but this was not a big task to do and is something we could contribute back.</p>

<h3>WinRM</h3>

<p>Could we do away with SSH altogether? It eliminates the problem we were facing and also means we don’t have to install anything on our Windows boxes &ndash; it’s all built in already. This would be an ideal solution but does all our tooling support it? I’ll discuss this in a little bit more detail later.</p>

<h3>Not use Beaker at all</h3>

<p>The nuclear option. If we couldn’t get anything to work we could not use Beaker at all, we could try and use Test Kitchen (with <a href="https://github.com/neillturner/kitchen-puppet">test-kitchen-puppet</a>) or some hand-rolled solution. Not the best idea, for us or the community but we though we might have to go down this path at one point. We even added <a href="https://github.com/liamjbennett/kitchen-puppet/tree/Windows_support">Windows support to test-kitchen-puppet</a> as part our diversion in this direction.</p>

<h2>What worked in the end:</h2>

<p>So we went down all these avenues and decided that the best option for us was to use Bitvise. It fixed the problem we were facing but it meant that we had some work ahead of us:
1. We had to rebuild all our Windows images with Bitvise rather than Cygwin (<a href="https://vagrantcloud.com/opentable">vagrantcloud.com/opentable</a> &ndash; version 2.x images now have this)
2. We had to make some changes to Beaker to support using a standard Windows command shell rather than a Unix shell:</p>

<ul>
<li> <a href="https://github.com/puppetlabs/beaker/pull/419">https://github.com/puppetlabs/beaker/pull/419</a> – configure_puppet method</li>
<li> <a href="https://github.com/puppetlabs/beaker/pull/420">https://github.com/puppetlabs/beaker/pull/419</a> – host_entry method</li>
<li> <a href="https://github.com/puppetlabs/beaker/pull/418">https://github.com/puppetlabs/beaker/pull/419</a> – Vagrant box_version</li>
<li> <a href="https://github.com/puppetlabs/beaker/pull/424">https://github.com/puppetlabs/beaker/pull/419</a> – Bitvise SSH</li>
</ul>


<p>With it finally working we had something that looked like this:</p>

<h2>Second Example</h2>

<p>Well with all the changes we implemented there were actually very few changes that we needed to make to our actual tests code.</p>

<p>No adjustments are needed for our spec_acceptance.rb file.</p>

<p>No adjustments are required for our spec file (show above).</p>

<p>The main change we made was in the nodeset file:</p>

<pre><code>HOSTS:
  win-2008R2-std:
  roles:
    - default
    - agent
  platform: Windows-server-amd64
  box: opentable/win-2008r2-standard-amd64-nocm
  hypervisor: vagrant
  user: vagrant
  ip: '10.255.33.129'
  communicator: bitvise
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>The biggest change you will see here is the addition of the &lsquo;communicator&rsquo; option. What this does is to allows us to actively select to use either Cygwin or Bitvise. This means that in our case we want to use Bitvise SSH as this fixes the error we were seeing and it&rsquo;s the version of SSH now installed on your newer Vagrant boxes. Bitvise is the only supported option at the moment (in our Beaker fork) but it is likely that this will soon support WinRM as well.</p>

<p>Things to note here:
* The name of the Windows host defined in the node set must be the same as the name of the Windows hostname &ndash; if it is not then when Vagrant boots up it will change the hostname
which will put Windows into a &ldquo;restart pending&rdquo; state.</p>

<h2>The Future and WinRM</h2>

<p>Most modern versions of Windows server have WinRM enabled by default but if you are using an older version or you are attempting to test a client then you will need to make sure
that this is enabled on your boxes. This is still the direction that we would like to go long-term as it is the most Windows-friendly approach but there are few road blocks in
the way of doing so right now:</p>

<ol>
<li><p>Packer doesn’t set support WinRM as a communication method. This is being actively worked on and you can follow the work here:</p>

<ul>
<li><a href="https://github.com/mitchellh/packer/issues/451">https://github.com/mitchellh/packer/issues/451</a></li>
<li><a href="https://github.com/dylanmei/packer-communicator-winrm">https://github.com/dylanmei/packer-communicator-winrm</a></li>
</ul>
</li>
<li><p>Beaker doesn’t yet support WinRM as a communication protocol. This is currently being discussed internally after we raised the idea. The work that we have completed for
Bitvise support will go some way it allowing other providers, such as wirm going forward and therefore WinRM support should be coming in the near future.</p></li>
</ol>


<h2>Summary</h2>

<p>Using Beaker to test modules for Windows has been a long and complicated journey. I have attempted to cover here all the problems that you might run into when trying to do this for yourselves and provide some good examples to get you going. You will soon see this being rolled out to all of the OpenTable open source modules shortly so you will have some complete working examples to reference. We will continue to work with PuppetLabs in improving Beaker (and its Windows support) in order to make this a easy process for everyone.</p>

<p>For any questions or comments then please reach out to me on twitter <a href="https://twitter.com/liamjbennett">@liamjbennett</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[What can I do?]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/08/08/what-can-i-do/"/>
    <updated>2014-08-08T13:52:30+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/08/08/what-can-i-do</id>
    <content type="html"><![CDATA[<p>I&rsquo;ve noticed recently a lot of content in my social media network is based on the current escalating problems in the Middle East.</p>

<p>Whenever I see such content from a friend or follower, I&rsquo;m reminded of Stephen Covey&rsquo;s <a href="http://www.amazon.co.uk/gp/product/B00GOZV3TM/ref=as_li_tl?ie=UTF8&amp;camp=1634&amp;creative=19450&amp;creativeASIN=B00GOZV3TM&amp;linkCode=as2&amp;tag=helenpieli-21&amp;linkId=RTIUTRBYIE6APBJA">The 7 Habits of Highly Effective People</a> &ndash; Circle of Influence, Circle of Concern.</p>

<p>Covey says that in order to remain truly effective, we should focus our time and energy on situations we can influence. Don&rsquo;t worry about things we cannot control, such as the weather, the economy or indeed foreign conflicts.</p>

<p>Covey&rsquo;s work implies that if we identify the areas that are inside our Circle of Influence we will be more productive and make more of an impact;</p>

<p><img src="http://www.chowamigo.co.uk/images/what_can_i_do.png" alt="image" /></p>

<p>If I&rsquo;m honest I find this work quite harsh in its approach. Indeed it might not sit well with everyone. However, I personally found myself much less burdened once I could identify which problems I had control over.</p>

<h2>Expand your influence</h2>

<p>I still find myself watching the news, reading press reports and shaking my head at the global problems we&rsquo;ve yet to solve. Covey&rsquo;s stance is a tough one to take &ndash; don&rsquo;t waste energy on X, your time and resources are better utilised by focusing on what you can change.</p>

<p>Some examples:</p>

<ul>
<li><p>I&rsquo;m concerned about global warming => recycle waste as best you can, be energy efficient with your devices, insulate your home more effectively.</p></li>
<li><p>I&rsquo;m concerned about my finances => learn new skills, look for a better role, try to control your spending, reduce unnecessary outgoings.</p></li>
<li><p>I&rsquo;m concerned about my health => get a check up, start to eat healthily, take regular exercise, reduce tobacco and alcohol intake.</p></li>
</ul>


<p>What I like about Covey&rsquo;s advice is that it&rsquo;s sending a clear message &ndash; are we able to make the sun shine at weekends? Can we solve the UK economic crisis by next month? Is there really anything we can do to bring about peace in the Middle East? The likely answer for most of us is no. We  simply have no control on areas outside of our power.</p>

<p>But if you step back into your circle of influence and be pro-active, you can make a real difference.</p>

<h2>Further reading</h2>

<p><a href="http://www.ndoherty.com/circle-of-influence-circle-of-concern">http://www.ndoherty.com/circle-of-influence-circle-of-concern</a></p>

<p><a href="http://www.upcyclededucation.com/2012/09/circle-of-concern-circle-of-influence.html">http://www.upcyclededucation.com/2012/09/circle-of-concern-circle-of-influence.html</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Acceptance Now]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/05/20/acceptance-now/"/>
    <updated>2014-05-20T15:00:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/05/20/acceptance-now</id>
    <content type="html"><![CDATA[<p><em>When is acceptance-only testing a good idea, and how can its problems be overcome?</em></p>

<p>In <a href="http://tech.opentable.co.uk/blog/2014/04/16/look-ma-no-unit-tests">a recent post</a>, I espoused some of the benefits my team enjoyed by reducing our test-base to a single layer of acceptance tests, with no separate unit or integration tests. It caused <a href="http://www.reddit.com/r/programming/comments/237fr1/look_ma_no_unit_tests/">some minor controversy</a>, which was not to be unexpected. At the time, I knew I had left out some details for brevity&rsquo;s sake. In this post—spurred on by some interesting <a href="https://twitter.com/NathanGloyn/status/456756552092098561">questions</a> and <a href="http://www.reddit.com/r/programming/comments/237fr1/look_ma_no_unit_tests/cgujuv7">commentary</a>—I&rsquo;d like to offer a more constructive view on the subject, and dig a little deeper into the nitty gritty of how we made it work.</p>

<p>I&rsquo;ll also point out some other hidden benefits of moving to acceptance-only testing, and suggest synergistic practices that can help decide if this is the right approach for your project.</p>

<h2>Seams</h2>

<p>After-the-fact unit testing requires us to <a href="https://www.youtube.com/watch?v=wEhu57pih5w">find the seams</a> along which code can be isolated and tested. Where those seams don&rsquo;t exist,  the temptation is to refactor code until they do, using patterns like <a href="http://martinfowler.com/articles/injection.html">dependency injection</a>, and following <a href="http://en.wikipedia.org/wiki/Single_responsibility_principle">SRP</a> and other <a href="http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod">SOLID</a> patterns.</p>

<p><a href="http://en.wikipedia.org/wiki/Test-driven_development">Test-first unit testing</a> aka TDD naturally tends to maximise seams, and results in highly decoupled code with small, specific tests. TDD must result in 100% unit test coverage, if practiced according to <a href="http://en.wikipedia.org/wiki/Test-Driven_Development_by_Example">the gospel</a>. In other words, TDD results in the best kind of code for later <a href="http://sourcemaking.com/refactoring">modification without risk</a>, and the best tests for detailed, granular feedback. Unit tests also tend to run very quickly, sometimes fast enough to <a href="http://misko.hevery.com/2009/05/07/configure-your-ide-to-run-your-tests-automatically/">run every time you hit &ldquo;Save&rdquo;</a>.</p>

<p><strong>Acceptance testing has far fewer seams available.</strong> Typically only one: the application boundary. Yes: <em>Your tests must invoke the entire application each time they are run.</em> Thus running acceptance tests is potentially very slow for even moderately sized projects with few dependencies—let alone large ones with many. Another problem is that, when an acceptance test fails, the failure could be at <em>any</em> layer in the stack. You immediately lose the pinpoint specificity afforded by unit tests.</p>

<p>Why, then, is it sometimes a good idea to forgo unit- in favour of acceptance-tests?</p>

<p>As we will see, the first issue, performance, can be mitigated. The second, granularity, is more difficult to overcome. But sometimes <em>that might be okay.</em></p>

<h2>Performance</h2>

<p>In most cases I have seen, acceptance tests are <em>really</em> slow. In some cases, there might be nothing you can do, but usually there is. Front-end automation suites using <a href="http://docs.seleniumhq.org/">Selenium</a> are temporally incorrigible, but they can be parallelised. Complex transactional pieces might be irreducible, but they can be helped with mock data. No matter which box your code is in, there is probably an escape route. Following are some of the techniques we used to overcome performance bottlenecks in our acceptance test suite&hellip;</p>

<h3>Sandbox Data</h3>

<p><em>This is probably a topic that deserves its own post, but I&rsquo;ll try to give a high-level treatment here.</em>
In our project, we took on the overhead of providing mock &ldquo;sandbox&rdquo; data. For our consumers this was a required feature anyway, so implementing it began early in the project, well before we <a href="http://tech.opentable.co.uk/blog/2014/04/16/look-ma-no-unit-tests">deleted all the unit tests</a>. It turned out this was an important enabler in moving to acceptance-only, since it allowed us to run tests much faster by <em>sometimes</em> circumventing data access.</p>

<p>Since this project was written in C# using strict TDD, our data layer already had <a href="http://en.wikipedia.org/wiki/Interface_(computer_science)#Software_interfaces_in_object-oriented_languages">an interface</a> for each data source. In the <em>unit</em> tests this allowed us to easily stub out the data. We reused the same interfaces to build up our sandbox, with dependency injection at runtime to choose between real and mock data. (I like to call this a &lsquo;pseudoseam&rsquo; in that it allows us to isolate data access at runtime, just like an ordinary seam allows you to isolate classes and methods in test.)</p>

<p><em>Sandbox data is hard to implement. A lot of the effort that would have gone into unit tests and their maintenance was instead pumped into writing good, wholesome, fake data for the sandbox.</em> However, sandbox data, unlike unit tests, solves three problems at once:</p>

<ul>
<li>It lets your <em>consumers</em> test in a predictable way without making real transactions;</li>
<li>It enables you to record specific data conditions from The Real World™, increasing your understanding of that data;</li>
<li>It lets you test internal business logic independently from real data, fast.</li>
</ul>


<p><em>Snip!</em> I went into too much detail on sandbox data here, saving that for a future post.</p>

<h3>Loosen Isolation</h3>

<p>Isolation between test runs is really important. If the order you run tests in can ever alter the results, then you have shared state, and you can no longer trust that your tests are testing the same thing each time they are run.</p>

<p>Usually, in unit testing, we rely on the test runner to respect directives in code that enforce isolation in this way. In NUnit with C# we use attributes for <a href="http://www.nunit.org/index.php?p=setup&amp;r=2.2.10">set-up</a> and <a href="http://www.nunit.org/index.php?p=teardown&amp;r=2.2.10">tear-down</a>, for example. We usually throw away the entire object graph before each test. <em>Sometimes before each assertion.</em> For acceptance testing, where spinning up your test subject tends to take longer, it can be helpful to bend the rules somewhat.</p>

<p>In an ideal world, each test run would begin on a new, freshly installed OS, thus eliminating any possibility of differing test results due to environmental issues—the system clock and network state notwithstanding. For unit testing we rarely if ever take this extremist approach. More usual is to rely on the test runner to enforce &ldquo;similar enough&rdquo; initial conditions each time a test is run–commonly relying on the developer to write this set-up and tear-down code correctly.</p>

<p>In acceptance testing it can be very beneficial for performance to loosen this one step further and re-use the same application instance (process) between test runs. Sandboxed data, especially if it is immutable, can help enormously to avoid the pitfalls of shared state. When running your acceptance tests against real data, where shared state is a real concern, you may discover interesting bugs that would otherwise have gone unnoticed if you were using only unit tests. <strong>Upon the discovery of issues with real data, you must implement the same failing condition in your sandbox data so that you don&rsquo;t accidentally introduce regressions later.</strong></p>

<p>The way we initially achieved this in our acceptance tests was by using the <code>TestFixtureSetup</code> attribute from NUnit to invoke the application, and run it to a point where it had generated an interesting result. Then, each &lsquo;test&rsquo; is in fact a single assertion on the state of the world at that point. Like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='csharp'><span class='line'><span class="na">[TestFixture]</span>
</span><span class='line'><span class="k">public</span> <span class="k">class</span> <span class="nc">my_acceptance_tests</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'><span class="na">    [TestFixtureSetUp]</span>
</span><span class='line'>    <span class="k">public</span> <span class="k">void</span> <span class="nf">do_stuff</span><span class="p">()</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="n">_result</span> <span class="p">=</span> <span class="n">InvokeTheApplicationsWithSomeGivenParams</span><span class="p">();</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'><span class="na">    [Test]</span>
</span><span class='line'>    <span class="k">public</span> <span class="k">void</span> <span class="nf">it_is_cool</span><span class="p">()</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="n">Assert</span><span class="p">.</span><span class="n">That</span><span class="p">(</span><span class="n">_result</span><span class="p">.</span><span class="n">Coolness</span><span class="p">,</span> <span class="n">Is</span><span class="p">.</span><span class="n">GreaterThan</span><span class="p">(</span><span class="m">1337</span><span class="p">));</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'><span class="na">    [Test]</span>
</span><span class='line'>    <span class="k">public</span> <span class="k">void</span> <span class="nf">it_has_2_bananas</span><span class="p">()</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="n">Assert</span><span class="p">.</span><span class="n">That</span><span class="p">(</span><span class="n">_result</span><span class="p">.</span><span class="n">Bananas</span><span class="p">,</span> <span class="n">Is</span><span class="p">.</span><span class="n">EqualTo</span><span class="p">(</span><span class="m">2</span><span class="p">));</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="p">.</span>
</span><span class='line'>    <span class="p">.</span>
</span><span class='line'>    <span class="p">.</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>We eventually refined this to use constructors to set up the initial state, and did some other not-necessarily-normal things to coax our acceptance tests into a reasonably elegant suite. More on this in an upcoming post on sandbox data.</p>

<h2>Granularity</h2>

<p>Granularity, in this case, refers to the level of detail revealed by a failing test. When a <em>unit</em> test fails, if it&rsquo;s written correctly, you should immediately know which method in which class went wrong. Often, the call stack in the exception message will tell you exactly which line of code was at fault. You can immediately jump to the offending code.</p>

<p>With acceptance tests, when something goes wrong, it could be anywhere in your program. Any of the tens, hundreds, thousands, or even millions of lines of code in your program could be at fault. This is clearly less than ideal, however there are ways to mitigate the pain:</p>

<h3>Minimise Code, Maximise Seams</h3>

<p>Clearly, the fewer lines of code in your app, the easier it will be to hunt down obscure test failures. However some problems are too big to be solved with few lines of code. What then to do? One answer, which we are beginning to explore in a new project, may be to write many small programs, each of which solves only a small part of the problem domain. This approach is known as <a href="http://martinfowler.com/articles/microservices.html">microservices</a>, and certainly has its own complexities in threading together many small pieces at OS or network level. However, a microservices architecture has additional potential benefits tangential to its affinity with acceptance-only testing. I&rsquo;m planning a blog post on this subject soon, once we have more data.</p>

<h3>Debugfu</h3>

<p>This isn&rsquo;t really a way to make your tests more granular, but it can help mitigate the problems of low granularity in acceptance tests. If the test won&rsquo;t tell you which line of code went wrong, attach a debugger to find out! If you&rsquo;re using Visual Studio, you are blessed with a best-of-breed debugger. Use it, trace through the execution and try to spot what went wrong. Use bookmarks and breakpoints to index your code. Does one area of code cause problems time and time again? There is probably something wrong with it, see if it can be rewritten more clearly. Users of  IntelliJ IDEA, Eclipse, or a myriad other IDEs, will also have access to usable debuggers.</p>

<h3>Sandbox Data (Again)</h3>

<p>Sandbox data allows you to run your tests against very specific data conditions. Often your code will have a different execution path depending on data, and this is really valuable knowledge when trying to nail down the cause of a test failure. Sandbox data, that can be selected by your tests, will improve the percieved granularity of such, by limiting the potential execution paths.</p>

<h2>Benefits</h2>

<p>I mentioned a few of the benefits of moving to acceptance-only testing in my last post. However, a few more have come to light since then which are worth mentioning.</p>

<h3>Acceptance Tests Are Language-Agnostic</h3>

<p>After writing v1 of our API, and acceptance-testing the living daylights out of it, we realised that we were still probably maintaining too much code, this time in the application itself. We decided to port the whole thing to JavaScript using Node to see what it would be like.</p>

<p>It worked, and <em>we didn&rsquo;t have to touch a single test,</em> even though those tests were written in C#, and the application was in JavaScript.</p>

<p>Just imagine the overhead of porting hundreds of unit tests over to a different language, along with the application. If that had been a requirement of our experiment, it would not have happened, and we would not have learned what we did. <em>(In the end we did keep using the C# implementation in production, but the speedy rewrite was still a valuable learning aid.)</em></p>

<h3>Acceptance Tests Behave Exactly Like Your Users</h3>

<p>When an acceptance-level test passes, you can be confident that a whole user journey using your application is working correctly. That&rsquo;s a huge win. When one fails, you can be pretty confident that something important to your users is not working properly and needs attention. Also, very valuable knowledge. This contrasts somewhat with unit-level tests that might tell you something internal is awry with your application, but its real impact to consumers will still often be unknown. Should you fix it? If there are multiple failures, which are the most important? Unit tests will rarely answer these questions for you.</p>

<p>Of course, you will fix it, or else be unable to confidently release your software–but surely, at times, you will be fixing something that does not matter, or is no longer relevant to your consumers. Unit tests in this way can encourage code rot, making it very difficult to unpick dependencies that are no longer needed. With acceptance tests, you only need to unpick the dependencies in your application, not also in the tests.</p>

<h2>Synergy</h2>

<p>Much of this is new to me, and certainly isn&rsquo;t without contention. However, the problems with acceptance-only testing, and specifically the solutions to those problems, indicate certain synergistic practices that may improve its viability:</p>

<h3>Thin Layers</h3>

<p>The project we first tried moving to acceptance-only testing on was a very thin layer–a facade over a collection of internal services. It had minimal business logic, and thus few potential execution paths. This certainly allowed us to keep the number of acceptance tests lower than might be expected for a large, complex application, that might branch off into numerous modes of operation. Of course one should probably try to minimise the <a href="http://en.wikipedia.org/wiki/Cyclomatic_complexity">cyclomatic complexity</a> of a code-base anyway, for one&rsquo;s own sanity.</p>

<h3>Statelessness</h3>

<p>If your system maintains a lot of state, acceptance testing can be much harder. This is because you will likely have to set-up much of that state for each test, increasing both developer effort and execution time. Both of which are bad. However, my new favourite thing, immutable sandbox data, may well be your friend in this case.</p>

<h3>Microservices</h3>

<p>We have only just begun experimenting with microservices ourselves, however I think it stands to reason that if each application is small overall, then the number of test cases for each will be small as well. This means the whole suite will run faster, and give you more granular feedback. I differentiate microservices from &lsquo;thin layers&rsquo; in that a microservice may well do data access, input parsing, validation, HTTP handling, and a bit of business logic–but over a very narrow domain–i.e. a thin vertical. A thin layer, on the other hand will perform only one kind of function–e.g. HTTP handling–but across multiple facets of the system. If thin layers are the lines of latitude, then microservices can be sections of the lines of longitude.</p>

<h2>Too Short; Read Also</h2>

<p>I hope this article has been a little more useful than the previous one. I have tried to explain more specifically what we actually did, from end-to-end, and how we overcame problems along the way. However, I&rsquo;ve really only scratched the surface. I will hopefully get the chance flesh out some of the ideas here in the coming months. In the mean time, there are plenty of <a href="http://www.shino.de/2012/07/02/atdd-by-example/">books</a> and <a href="http://jonkruger.com/blog/2012/02/20/when-acceptance-tests-are-better-than-unit-tests/">blog posts</a> on the subject of acceptance testing. In addition, Martin Fowler has writen <a href="http://martinfowler.com/articles/microservices.html">a great primer on microservices</a> that&rsquo;s really got me thinking about their utility alongside acceptance-only testing and sandbox data.</p>

<p>Thanks for reading :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Continuous Delivery: Automating Deployment Visibility]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/05/19/continuous-delivery-automating-deployment-visibility/"/>
    <updated>2014-05-19T17:17:40+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/05/19/continuous-delivery-automating-deployment-visibility</id>
    <content type="html"><![CDATA[<p>In our continued effort to drive towards a service oriented architecture each of our teams are continuously improving their deployment processes. Recently our team has focussed on automating as much as possible, putting as much into chat as we can and improving our logging/metrics.</p>

<p>The image below shows at a high level what our teams current deployment pipeline looks like and this post will attempt to summarise some recent changes that have allowed us to automate visibility.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/release-pipeline.png" width="900" height="350" title="image" alt="images"></p>

<h2>Kicking off a deployment</h2>

<p>I wrote previously that we started <a href="http://tech.opentable.co.uk/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot/">using chatops</a> to increase visibility operationally. Hubot is central to this and we wrote a small script to kick off deployments within <a href="https://www.hipchat.com/">Hipchat</a></p>

<p><img src="http://tech.opentable.co.uk/images/posts/hubot-deploy-restaurant.png" width="350" height="350" title="image" alt="images"></p>

<p>We have two TeamCity instances. The first is used as a build and deployment system to our pre-production servers. The second is used as a deployment system to our production servers. Artifacts from our non-production instance are stored in <a href="http://www.jfrog.com/home/v_artifactory_opensource_overview">Artifactory</a> and our production deployment makes an API call to non-production TeamCity to ask for the last successfully pinned build. Pinning a build only occurs when we&rsquo;re happy that the build is ready to be shipped (passing unit and acceptance tests). The above Hubot command will pin the non-production build, given that the build succeeded, and add a build to the queue in production.</p>

<p>To configure Hubot to do this we wrote a command to setup aliases providing the build id of the build to pin (non-production) and the build id of the build to kick off (production).</p>

<p><img src="http://tech.opentable.co.uk/images/posts/hubot-deploy-alias.png" width="350" height="350" title="image" alt="images"></p>

<h2>Deployment visibility</h2>

<p>Our production deployments must be auditable and it&rsquo;s important that we know what went out with each release and keep a log of this for our Risk Management team. We do this by creating a ticket in <a href="https://www.atlassian.com/software/jira">JIRA</a>, internally known as a CCB, and this gives us a central store of all deployments by all teams.</p>

<p>In the past these tickets were manually created for each release. We soon realised that this was something we could automate. To do so we created a new &ldquo;deployment-info&rdquo; endpoint for our service. This simply contains the SHA of the last commit released along with a time stamp. The first step of our production deploy is to query this endpoint and then using the Github API to get all the commits since that last SHA. These commits are then logged to JIRA to create a CCB ticket using the JIRA API. Each of these steps are automated from TeamCity using grunt tasks. You can find information of the grunt tasks on github as follows:</p>

<ul>
<li><a href="https://github.com/opentable/grunt-ccb">https://github.com/opentable/grunt-ccb</a></li>
<li><a href="https://github.com/opentable/grunt-github-manifest">https://github.com/opentable/grunt-github-manifest</a></li>
<li><a href="https://github.com/opentable/grunt-package-github">https://github.com/opentable/grunt-package-github</a></li>
</ul>


<h2>Build Notifications to Kibana</h2>

<p>Once we have a CCB we fire a start and end event from TeamCity containing the build number to Redis which is then piped into <a href="http://logstash.net/">Logstash</a>. An event is sent before and after deploying the code to all nodes. This is hugely beneficial because it allows us to plot releases against our graphs in <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a>. Kibana recently added a new feature called Markers. Essentially these are tags that display at the bottom of a graph.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/kibana-tags.png" width="350" height="350" title="image" alt="images"></p>

<p>You can find information on this Markers module on github &ndash; <a href="https://github.com/opentable/grunt-deployment-logger">https://github.com/opentable/grunt-deployment-logger</a></p>

<p>This has already proved incredibly useful for the team and has allowed us to visually correlate issues or changes in key metrics (response times/requests per second) to releases. The following image shows how these look over several graphs.</p>

<p><img src="http://tech.opentable.co.uk/images/posts/kibana-dashboard.png" width="900" height="900" title="image" alt="images"></p>

<h2>Hipchat build complete notification</h2>

<p><img src="http://tech.opentable.co.uk/images/posts/hubot-notification.png" width="350" height="200" title="image" alt="images"></p>

<p>Once our deployment pipeline has completed we send a notification to our teams room in Hipchat (as a final step in TeamCity) to inform the team that the release has completed. It&rsquo;s great to see a deployment start and end in chat. Having a central log of key operations in our team means that we don&rsquo;t have to go and find information when it&rsquo;s baked into chat.</p>

<h2>Conclusion</h2>

<p>We&rsquo;ve come along way with improving our pipeline and automating visibility. Our team is made up of 4 members; 3 in the office and 1 remote. The ultimate goal is to improve speed of deployment and visibility of events not just within the team but for everyone who is interested. Equally we want to continue to open source by as much as possible, allowing us to share our process with teams inside and outside of our organization. We can release code anywhere in the world and the process is completely centralised in chat. We want to continue to move fast and fix faster.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Managing Windows Features with Puppet]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/05/15/managing-windows-features-with-puppet/"/>
    <updated>2014-05-15T16:12:38+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/05/15/managing-windows-features-with-puppet</id>
    <content type="html"><![CDATA[<p>Back in June 2013, I wrote about <a href="http://tech.opentable.co.uk/blog/2013/06/14/windows-feature-management-with-powershell/">Windows Feature Management with PowerShell</a>. We have since released a Puppet module that will do this for us. We originally wrote PowerShell:</p>

<pre><code>Import-Module ServerManager
Add-WindowsFeature Web-Asp-Net
</code></pre>

<p>The Puppet module now wraps this code as follows:</p>

<pre><code>windowsfeature { 'Web-Asp-Net': }
</code></pre>

<p>The declaration windowsfeature is a specific Puppet type called a <a href="http://docs.puppetlabs.com/learning/definedtypes.html">define</a>. In developer terms, this is the equivalent of a helper method that can be reused. We can also make sure that Windows Features are <em>not</em> installed on the server as follows:</p>

<pre><code>windowsfeature { 'Telnet-Server': 
  ensure =&gt; absent 
}
</code></pre>

<p>In the backing code for the module, we do a check before we install / uninstall any windows features. This means that we will only make the changes we really need to. This ensures idempotency of the script. By using this class, we can build up a list of what features a server should have enabled / installed on it. As example manifest would look as follows:</p>

<pre><code>class my_windows_features {
  windowsfeature { 'Web-Asp-Net': }
  windowsfeature { 'Web-Net-Ext': }
  windowsfeature { 'Web-ISAPI-Ext': }
  windowsfeature { 'Web-ISAPI-Filter': }
  windowsfeature { 'Web-Mgmt-Tools': }
  windowsfeature { 'Web-Mgmt-Console': }
  windowsfeature { 'Telnet-Server': ensure =&gt; absent }
}
</code></pre>

<p>The server will have it&rsquo;s shipping list of Windows Features checked every 30 minutes by Puppet. We are sure that any changes encountered during that time will be applied as expected.  You can find more about our WindowsFeature module on the <a href="http://github.com/opentable/puppet-windowsfeature">github repo</a>. If you want to use the module, then you can install it using the Puppet Module tool via the <a href="http://forge.puppetlabs.com/opentable/windowsfeature">Puppet Forge</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Managing Windows Web Applications with Puppet]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/05/13/managing-windows-web-applications-with-puppet/"/>
    <updated>2014-05-13T15:25:16+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/05/13/managing-windows-web-applications-with-puppet</id>
    <content type="html"><![CDATA[<p>As part of our move towards a configuration management tool, we really wanted to start automating as much of our infrastructure as possible. This included our application configuration stack. IIS management is pretty easy with PowerShell. It would look something like this</p>

<pre><code>Import-Module WebAdministration
New-WebSite -Name "DemoSite" -Port 80 -IP * -PhysicalPath "c:\inetpub\wwwroot" -ApplicationPool "MyAppPool"
</code></pre>

<p>This would of course set up a website called &lsquo;DemoSite&rsquo; running on port 80 on the local machine. The cmdlets that come with PowerShell make this pretty easy. This is great if it is a one-off job to set up a site. We run our websites from a number of webservers, therefore, it would be silly to have to RDP into each webserver and run a script on it. This is why tools like Puppet, Chef, Ansible etc. exist. We needed a configuration management tool to do this work for us. It has a number of benefits:</p>

<ul>
<li>Orchestration</li>
<li>Idempotency</li>
<li>Makes sure that each server is configured in &lsquo;exactly&rsquo; the same way as no human intervention is needed</li>
<li>Developers can help the operations team by creating the scripts needed. This is great for collaboration between teams</li>
</ul>


<p>On investigating how we would do this with Puppet, we noticed that there were not many other people managing their site in this way. Therefore, we would have to turn our PowerShell scripts into Puppet modules to manage our system.</p>

<p>We have since created a Puppet module to manage IIS. To manage IIS with Puppet, we can now write the following code:</p>

<pre><code>iis::manage_site { 'DemoSite:
   site_path     =&gt; 'c:\inetpub\wwwroot',
   port          =&gt; '80',
   ip_address    =&gt; '*',
   app_pool      =&gt; 'MyAppPool'
}
</code></pre>

<p>This would produce <strong>exactly</strong> the same results as the code from above. But it has 1 difference. There are checks in the code behind this module that will mean the code will only execute when it is needed, i.e. when the site_path isn&rsquo;t correct or the app_pool isn&rsquo;t correct. This is idempotency. The script can be run again and again and again&hellip;.</p>

<p>To create an application binding, we used to do this in PowerShell:</p>

<pre><code>Import-Module WebAdministration
New-WebBinding -Name 'DemoSite' -Port '8080' -IPAddress '*'
</code></pre>

<p>This would set up an extra binding on port 8080 for the site, DemoSite. We replaced this code with our puppet equivalent:</p>

<pre><code>iis::manage_binding { 'DemoSite-8080':
  site_name   =&gt; 'DemoSite',
  protocol    =&gt; 'http',
  port        =&gt; '8080',
  ip_address  =&gt; '*',
}
</code></pre>

<p>To create a virtual application, we would write the PowerShell:</p>

<pre><code>Import-Module WebAdministration
New-WebApplication -Name 'VirtualApp' -Site 'DemoSite' -PhysicalPath 'c:\inetpub\wwwroot\MyVirtualApp' -ApplicationPool 'MyAppPool'
</code></pre>

<p>This will create a VirtualApp folder on the DemoSite, use the same application pool and then set the path of the folder. I can do the same thing in Puppet as follows:</p>

<pre><code>iis::manage_virtual_application {'VirtualApp':
  site_name   =&gt; 'DemoSite',
  site_path   =&gt; 'C:\inetpub\wwwroot\MyVirtualApplication',
  app_pool    =&gt; 'MyAppPool'
 }  
</code></pre>

<p>We can therefore, chain a manifest together that does all this for us in 1 go. It would look as follows:</p>

<pre><code>class mywebsite {
  iis::manage_app_pool {'MyAppPool':
    enable_32_bit           =&gt; true,
    managed_runtime_version =&gt; 'v4.0',
  } -&gt;

  iis::manage_site {'DemoSite':
    site_path   =&gt; 'C:\inetpub\wwwroot',
    port        =&gt; '80',
    ip_address  =&gt; '*',
    app_pool    =&gt; 'MyAppPool'
  } -&gt;

  iis::manage_virtual_application {'VirtualApp':
    site_name  =&gt; 'DemoSite',
    site_path  =&gt; 'C:\inetpub\wwwroot\MyVirtualApp',
    app_pool   =&gt; 'MyAppPool'
  } -&gt; 

  iis::manage_binding {'DemoSite-8080':
    site_name  =&gt; 'DemoSite',
    protocol   =&gt; 'http',
    port       =&gt; '8080',
    ip_address =&gt; '*'
  }
}
</code></pre>

<p>The module does more than just these tasks and I could give more and more examples of what we wrote, but you can find more about our IIS module on the <a href="http://github.com/opentable/puppet-iis">github repo</a>. If you want to use the module, then you can install it using the Puppet Module tool via the <a href="http://forge.puppetlabs.com/opentable/iis">Puppet forge</a>.</p>

<p>We love to hear feedback on things that the module should support. We like Pull Requests even more :)</p>
]]></content>
  </entry>
  
</feed>
