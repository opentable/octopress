<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Engineering | OpenTable Tech UK Blog]]></title>
  <link href="http://tech.opentable.co.uk/blog/categories/engineering/atom.xml" rel="self"/>
  <link href="http://tech.opentable.co.uk/"/>
  <updated>2015-02-07T14:21:51+00:00</updated>
  <id>http://tech.opentable.co.uk/</id>
  <author>
    <name><![CDATA[OpenTable]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Beginner's guide to REST services]]></title>
    <link href="http://tech.opentable.co.uk/blog/2015/02/02/a-beginners-guide-to-rest-services/"/>
    <updated>2015-02-02T11:53:25+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2015/02/02/a-beginners-guide-to-rest-services</id>
    <content type="html"><![CDATA[<h2>Why this post?</h2>

<p>As a junior, I always find it easier to just sit and write code than actually stop to think about the theoretical basis that lie under the applications I work on. <strong>REST</strong>  is one of those terms I heard a lot about, so I decided to try to sum up what it means and how it affects the choices we make everyday as software engineers.</p>

<h2>Introduction to REST</h2>

<p>REST stands for Representational State Transfer, and it can be defined as an architectural style used to build Web Services that are lightweight, maintainable, and scalable. A service that is designed by REST principles can be called a <strong>RESTful service</strong>.</p>

<p>It has been described first in 2000 by Roy Fielding, in a <a href="http://www.ics.uci.edu/~fielding/pubs/webarch_icse2000.pdf">dissertation</a> called &ldquo;Architectural Styles and the Design of Network-based Software Architectures&rdquo;. The basic idea was to describe the interactions between the components of a distributed system, putting constraints on them and emphasizing the importance of an uniform interface, that is abstracted from the single components.</p>

<p>REST is often applied to the design and development of web services, which is the scenario I&rsquo;ll try to address in this post.</p>

<p>The purpose of a web service can be summed up as follows: it exposes <strong>resources</strong> to a <strong>client</strong> so that it can have access to them (examples of typical resources include pictures, video files, web pages and business data).</p>

<p>Common features of a service that is built in a REST style are:</p>

<ul>
<li>Representations</li>
<li>Messages</li>
<li>URIs</li>
<li>Uniform Interface</li>
<li>Statelessness</li>
<li>Links between resources</li>
<li>Caching</li>
</ul>


<h2>Representations &ndash; what are they?</h2>

<p>REST style does not put a constraint into the way resources are represented, as long as their format is understandable by the client.</p>

<p>Good examples of data formats in which a resource could be returned from a service are <a href="http://www.json.org/"><strong>JSON</strong></a> (JavaScript Object Notation, which nowadays is the coolest one) and <a href="http://www.w3.org/XML/"><strong>XML</strong></a> (Extensible Markup Language, used for more complex data structures). Say for instance a REST service has to expose the data related to a song, with its attributes. A way of doing it in JSON could be:</p>

<p>```json
{</p>

<pre><code>"ID": 1,
"title": "(You gotta) Fight for your right (To party)",
"artist": "Beastie Boys",
"album": "Licensed To Ill",
"year": 1986,
"genre": "Hip-Hop"
</code></pre>

<p>}
```</p>

<p>Easy, huh?</p>

<p>Anyway, a service can represent a resource in a number of ways at the same time, leaving the client to choose which one is better suited for its needs. The important thing is that there is agreement on what format to send/expect.</p>

<p>The format that the client needs will be part of the <strong>request</strong> sent by the client.</p>

<p>The resource will be eventually sent by the service as part of what we call a <strong>response</strong>.</p>

<p>It has to be kept in mind that a resource should be completely described by the representation, since this is the only information the client will have. It has to be exaustive, but without exposing classified or useless information about the entity at the same time.</p>

<h2>Messages A.K.A. client and service chatting</h2>

<p>Q: So, how exactly do client and service exchange requests and responses?</p>

<p>A: They send messages.</p>

<p>In fact, to be more specific, the client will send an <strong>HTTP request</strong> to the service, specifying the following details:</p>

<ul>
<li>The <strong>method</strong> that is called on the resource. It can correspond to a <em>GET</em>, a <em>POST</em>, a <em>PUT</em>, a <em>DELETE</em>, an <em>OPTIONS</em> or a <em>HEAD</em> operation.</li>
<li>The <strong>URI</strong> of the request. It identifies what is the resource on which the client wants to use the method. More on that later. For now let&rsquo;s say it is the only way the client knows how to call the needed resource.</li>
<li>The <strong>HTTP version</strong>, which is usually <a href="http://tools.ietf.org/html/rfc2616"><em>HTTP/1.1</em></a>.</li>
<li>The <strong>request headers</strong>, which are the additional information passed, with the request, to the service. These fields are basically request modifiers, similar to the parameters sent to a programming language method, and they depend on the type of request sent. More on that later.</li>
<li>The <strong>request body</strong>: is the actual content of a message. In a RESTful service, itâ€™s where the representation of resources sit. A body will not be present in a GET request, for instance, since it is a request to retrieve a resource rather than to create one, whereas a POST request will most likely have one.</li>
</ul>


<p>The request will then generate an <strong>HTTP response</strong> to the client, that will contain the following elements:</p>

<ul>
<li>The <strong>HTTP version</strong>, same as above.</li>
<li>The <strong>response code</strong>: which is a three-digit status code sent back to the client. Can be of the <strong>1xx</strong> format (informational), <strong>2xx</strong> (success), <strong>3xx</strong> (redirect), <strong>4xx</strong> (client error), <strong>5xx</strong> (server error).</li>
<li>The <strong>response header</strong>, which contains metadata and settings related to the message.</li>
<li>The <strong>response body</strong>: contains the representation (if the request was successful).</li>
</ul>


<h2>URIs, home of the resources</h2>

<p>A requirement of REST is that each resource has to correspond to an <a href="http://en.wikipedia.org/wiki/Uniform_resource_identifier">URI</a> address, which unsurprisingly stands for Uniform Resource Identifier. Having URIs associated to resources is key, because they are the addresses on which the client is allowed to perform the operations on the resources. It is important to stress that according to REST an URI should describe a resource, but never the operation performed on it.</p>

<p>The addresses are usually constructed hierarchically, to allow readability. A typical resource URL could be written as: <code>http://serviceName/resourceName/resourceID</code></p>

<p>Basic guidelines to build well-structured URIs are:</p>

<ul>
<li>Resources should be named with plural nouns, no verbs, using conventions throughout the whole service.</li>
<li>Query URIs <code>http://serviceName/resourceName?id=resourceID</code> should be used only when really necessary. They are not deprecated by REST style, but they are less readable than the normal URIs, and are ignored by search engines. On the upside, they allow the client to send parameters to the service, to refine the request for a specific subset of resources, or resources in a specific format.</li>
</ul>


<h2>Uniform interface, various operations</h2>

<p>Ok, so now that a client knows where a resource is reachable, how is it going to handle the resource? What are the operations that it can perform?</p>

<p>HTTP provides a set of methods that allow the client to perform standard operations on the service:</p>

<table style="margin-bottom:16px;">
    <tr>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Method</th>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Operation performed</th> 
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Quality</th>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">GET</td>
        <td style="padding:5px 10px;">Read a resource</td> 
        <td style="padding:5px 10px;">Safe</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">POST</td>
        <td style="padding:5px 10px;">Insert a new resource, or update an existing one</td> 
        <td style="padding:5px 10px;">Not idempotent</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">PUT</td>
        <td style="padding:5px 10px;">Insert a new resource, or update an existing one</td> 
        <td style="padding:5px 10px;">Idempotent (see below)</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">DELETE</td>
        <td style="padding:5px 10px;">Delete a resource</td> 
        <td style="padding:5px 10px;">Idempotent</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">OPTIONS</td>
        <td style="padding:5px 10px;">List allowed operations on a resource</td> 
        <td style="padding:5px 10px;">Safe</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">HEAD</td>
        <td style="padding:5px 10px;">Return only the response header, no body</td> 
        <td style="padding:5px 10px;">Safe</td>
    </tr>
</table>


<p>The key difference between <em>POST</em> and <em>PUT</em> is that no matter how many times a <em>PUT</em> operation is performed, the result will be the same (this is what <em>idempotent</em> means), whereas with a <em>POST</em> operation a resource will be added or updated multiple times.</p>

<p>Another difference is that a client that sends a <em>PUT</em> request always need to know the exact URI to operate on, I.E. assigning a name or an ID to a resource. If the client is not able to do so, it has no choice but to use a POST request.</p>

<p>Finally, if the resource already exists, <em>POST</em> and <em>PUT</em> will update it in an identical fashion.</p>

<p>These operations, according to REST, should be available to the client as hyperlinks to the above described URIs, and that is how the client/service interface is constrained to be <em>uniform</em>.</p>

<h2>Statelessness of the client side</h2>

<p>A RESTful service does not maintain the application state client-side. This only allows the client to perform requests that are resource specific, and does not allow the client to perform operations that assume prior knowledge of past requests. The client only knows what to do based on the ability to read the hypertext it receives, knowing its media type.</p>

<p>This leads me to mention an important constraint of REST, that was also <a href="http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven">enforced by Fielding</a> after publishing his dissertation: hyperlinks within hypertext are the only way for the client to make state transitions and perform operations on resources. This constraint is also known as <strong>HATEOAS</strong> (Hypermedia As The Engine Of Application State).</p>

<h2>Links between resources</h2>

<p>In the case of a resource that contains a list of resources, REST suggests to include links to the single resources on the representation, to keep it compact and avoid redundant data.</p>

<h2>Caching to optimize time and efficiency</h2>

<p>Allows to store responses and return them if the same request is performed again. It has to be handled carefully to avoid returning stale results. The headers that allow us to perform controls over caching are:</p>

<table style="margin-bottom:16px;">
    <tr>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Header</th>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Application</th>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Date</td>
        <td style="padding:5px 10px;">Finding out when this representation was generated</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Last Modified</td>
        <td style="padding:5px 10px;">Date and time when the server modified the representation</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Cache-Control</td>
        <td style="padding:5px 10px;">HTTP 1.1 header used to control caching, can contain directives</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Expires</td>
        <td style="padding:5px 10px;">Expiration date (supports HTTP 1.0)</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Age</td>
        <td style="padding:5px 10px;">Duration since the resource was fetched from server</td>
    </tr>
</table>


<p>Cache-Control values can be tweaked to control if a cached result is still valid or stale. For example, the <em>max-age</em> value indicates for how many seconds from the moment expressed by the Date header a cached result will be valid.</p>

<h2>Conclusion</h2>

<p>REST is a language-agnostic style that abstracts over components and allows to build scalable, reusable and relatively lightweight web services. Thinking about it, it seems that REST is very close to an accurate description of the characteristics that made the World Wide Web so popular.</p>

<p>That of course is encouraging developers from all over the world to comply to these very basic ideas, owned by no one but at the same time used by everyone. Fascinating!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Acceptance Now]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/05/20/acceptance-now/"/>
    <updated>2014-05-20T15:00:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/05/20/acceptance-now</id>
    <content type="html"><![CDATA[<p><em>When is acceptance-only testing a good idea, and how can its problems be overcome?</em></p>

<p>In <a href="/blog/2014/04/16/look-ma-no-unit-tests">a recent post</a>, I espoused some of the benefits my team enjoyed by reducing our test-base to a single layer of acceptance tests, with no separate unit or integration tests. It caused <a href="http://www.reddit.com/r/programming/comments/237fr1/look_ma_no_unit_tests/">some minor controversy</a>, which was not to be unexpected. At the time, I knew I had left out some details for brevity&rsquo;s sake. In this postâ€”spurred on by some interesting <a href="https://twitter.com/NathanGloyn/status/456756552092098561">questions</a> and <a href="http://www.reddit.com/r/programming/comments/237fr1/look_ma_no_unit_tests/cgujuv7">commentary</a>â€”I&rsquo;d like to offer a more constructive view on the subject, and dig a little deeper into the nitty gritty of how we made it work.</p>

<p>I&rsquo;ll also point out some other hidden benefits of moving to acceptance-only testing, and suggest synergistic practices that can help decide if this is the right approach for your project.</p>

<h2>Seams</h2>

<p>After-the-fact unit testing requires us to <a href="https://www.youtube.com/watch?v=wEhu57pih5w">find the seams</a> along which code can be isolated and tested. Where those seams don&rsquo;t exist,  the temptation is to refactor code until they do, using patterns like <a href="http://martinfowler.com/articles/injection.html">dependency injection</a>, and following <a href="http://en.wikipedia.org/wiki/Single_responsibility_principle">SRP</a> and other <a href="http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod">SOLID</a> patterns.</p>

<p><a href="http://en.wikipedia.org/wiki/Test-driven_development">Test-first unit testing</a> aka TDD naturally tends to maximise seams, and results in highly decoupled code with small, specific tests. TDD must result in 100% unit test coverage, if practiced according to <a href="http://en.wikipedia.org/wiki/Test-Driven_Development_by_Example">the gospel</a>. In other words, TDD results in the best kind of code for later <a href="http://sourcemaking.com/refactoring">modification without risk</a>, and the best tests for detailed, granular feedback. Unit tests also tend to run very quickly, sometimes fast enough to <a href="http://misko.hevery.com/2009/05/07/configure-your-ide-to-run-your-tests-automatically/">run every time you hit &ldquo;Save&rdquo;</a>.</p>

<p><strong>Acceptance testing has far fewer seams available.</strong> Typically only one: the application boundary. Yes: <em>Your tests must invoke the entire application each time they are run.</em> Thus running acceptance tests is potentially very slow for even moderately sized projects with few dependenciesâ€”let alone large ones with many. Another problem is that, when an acceptance test fails, the failure could be at <em>any</em> layer in the stack. You immediately lose the pinpoint specificity afforded by unit tests.</p>

<p>Why, then, is it sometimes a good idea to forgo unit- in favour of acceptance-tests?</p>

<p>As we will see, the first issue, performance, can be mitigated. The second, granularity, is more difficult to overcome. But sometimes <em>that might be okay.</em></p>

<h2>Performance</h2>

<p>In most cases I have seen, acceptance tests are <em>really</em> slow. In some cases, there might be nothing you can do, but usually there is. Front-end automation suites using <a href="http://docs.seleniumhq.org/">Selenium</a> are temporally incorrigible, but they can be parallelised. Complex transactional pieces might be irreducible, but they can be helped with mock data. No matter which box your code is in, there is probably an escape route. Following are some of the techniques we used to overcome performance bottlenecks in our acceptance test suite&hellip;</p>

<h3>Sandbox Data</h3>

<p><em>This is probably a topic that deserves its own post, but I&rsquo;ll try to give a high-level treatment here.</em>
In our project, we took on the overhead of providing mock &ldquo;sandbox&rdquo; data. For our consumers this was a required feature anyway, so implementing it began early in the project, well before we <a href="/blog/2014/04/16/look-ma-no-unit-tests">deleted all the unit tests</a>. It turned out this was an important enabler in moving to acceptance-only, since it allowed us to run tests much faster by <em>sometimes</em> circumventing data access.</p>

<p>Since this project was written in C# using strict TDD, our data layer already had <a href="http://en.wikipedia.org/wiki/Interface_(computer_science)#Software_interfaces_in_object-oriented_languages">an interface</a> for each data source. In the <em>unit</em> tests this allowed us to easily stub out the data. We reused the same interfaces to build up our sandbox, with dependency injection at runtime to choose between real and mock data. (I like to call this a &lsquo;pseudoseam&rsquo; in that it allows us to isolate data access at runtime, just like an ordinary seam allows you to isolate classes and methods in test.)</p>

<p><em>Sandbox data is hard to implement. A lot of the effort that would have gone into unit tests and their maintenance was instead pumped into writing good, wholesome, fake data for the sandbox.</em> However, sandbox data, unlike unit tests, solves three problems at once:</p>

<ul>
<li>It lets your <em>consumers</em> test in a predictable way without making real transactions;</li>
<li>It enables you to record specific data conditions from The Real Worldâ„¢, increasing your understanding of that data;</li>
<li>It lets you test internal business logic independently from real data, fast.</li>
</ul>


<p><em>Snip!</em> I went into too much detail on sandbox data here, saving that for a future post.</p>

<h3>Loosen Isolation</h3>

<p>Isolation between test runs is really important. If the order you run tests in can ever alter the results, then you have shared state, and you can no longer trust that your tests are testing the same thing each time they are run.</p>

<p>Usually, in unit testing, we rely on the test runner to respect directives in code that enforce isolation in this way. In NUnit with C# we use attributes for <a href="http://www.nunit.org/index.php?p=setup&amp;r=2.2.10">set-up</a> and <a href="http://www.nunit.org/index.php?p=teardown&amp;r=2.2.10">tear-down</a>, for example. We usually throw away the entire object graph before each test. <em>Sometimes before each assertion.</em> For acceptance testing, where spinning up your test subject tends to take longer, it can be helpful to bend the rules somewhat.</p>

<p>In an ideal world, each test run would begin on a new, freshly installed OS, thus eliminating any possibility of differing test results due to environmental issuesâ€”the system clock and network state notwithstanding. For unit testing we rarely if ever take this extremist approach. More usual is to rely on the test runner to enforce &ldquo;similar enough&rdquo; initial conditions each time a test is runâ€“commonly relying on the developer to write this set-up and tear-down code correctly.</p>

<p>In acceptance testing it can be very beneficial for performance to loosen this one step further and re-use the same application instance (process) between test runs. Sandboxed data, especially if it is immutable, can help enormously to avoid the pitfalls of shared state. When running your acceptance tests against real data, where shared state is a real concern, you may discover interesting bugs that would otherwise have gone unnoticed if you were using only unit tests. <strong>Upon the discovery of issues with real data, you must implement the same failing condition in your sandbox data so that you don&rsquo;t accidentally introduce regressions later.</strong></p>

<p>The way we initially achieved this in our acceptance tests was by using the <code>TestFixtureSetup</code> attribute from NUnit to invoke the application, and run it to a point where it had generated an interesting result. Then, each &lsquo;test&rsquo; is in fact a single assertion on the state of the world at that point. Like this:</p>

<p>```csharp
[TestFixture]
public class my_acceptance_tests
{</p>

<pre><code>[TestFixtureSetUp]
public void do_stuff()
{
    _result = InvokeTheApplicationsWithSomeGivenParams();
}
[Test]
public void it_is_cool()
{
    Assert.That(_result.Coolness, Is.GreaterThan(1337));
}
[Test]
public void it_has_2_bananas()
{
    Assert.That(_result.Bananas, Is.EqualTo(2));
}
.
.
.
</code></pre>

<p>}
```</p>

<p>We eventually refined this to use constructors to set up the initial state, and did some other not-necessarily-normal things to coax our acceptance tests into a reasonably elegant suite. More on this in an upcoming post on sandbox data.</p>

<h2>Granularity</h2>

<p>Granularity, in this case, refers to the level of detail revealed by a failing test. When a <em>unit</em> test fails, if it&rsquo;s written correctly, you should immediately know which method in which class went wrong. Often, the call stack in the exception message will tell you exactly which line of code was at fault. You can immediately jump to the offending code.</p>

<p>With acceptance tests, when something goes wrong, it could be anywhere in your program. Any of the tens, hundreds, thousands, or even millions of lines of code in your program could be at fault. This is clearly less than ideal, however there are ways to mitigate the pain:</p>

<h3>Minimise Code, Maximise Seams</h3>

<p>Clearly, the fewer lines of code in your app, the easier it will be to hunt down obscure test failures. However some problems are too big to be solved with few lines of code. What then to do? One answer, which we are beginning to explore in a new project, may be to write many small programs, each of which solves only a small part of the problem domain. This approach is known as <a href="http://martinfowler.com/articles/microservices.html">microservices</a>, and certainly has its own complexities in threading together many small pieces at OS or network level. However, a microservices architecture has additional potential benefits tangential to its affinity with acceptance-only testing. I&rsquo;m planning a blog post on this subject soon, once we have more data.</p>

<h3>Debugfu</h3>

<p>This isn&rsquo;t really a way to make your tests more granular, but it can help mitigate the problems of low granularity in acceptance tests. If the test won&rsquo;t tell you which line of code went wrong, attach a debugger to find out! If you&rsquo;re using Visual Studio, you are blessed with a best-of-breed debugger. Use it, trace through the execution and try to spot what went wrong. Use bookmarks and breakpoints to index your code. Does one area of code cause problems time and time again? There is probably something wrong with it, see if it can be rewritten more clearly. Users of  IntelliJ IDEA, Eclipse, or a myriad other IDEs, will also have access to usable debuggers.</p>

<h3>Sandbox Data (Again)</h3>

<p>Sandbox data allows you to run your tests against very specific data conditions. Often your code will have a different execution path depending on data, and this is really valuable knowledge when trying to nail down the cause of a test failure. Sandbox data, that can be selected by your tests, will improve the percieved granularity of such, by limiting the potential execution paths.</p>

<h2>Benefits</h2>

<p>I mentioned a few of the benefits of moving to acceptance-only testing in my last post. However, a few more have come to light since then which are worth mentioning.</p>

<h3>Acceptance Tests Are Language-Agnostic</h3>

<p>After writing v1 of our API, and acceptance-testing the living daylights out of it, we realised that we were still probably maintaining too much code, this time in the application itself. We decided to port the whole thing to JavaScript using Node to see what it would be like.</p>

<p>It worked, and <em>we didn&rsquo;t have to touch a single test,</em> even though those tests were written in C#, and the application was in JavaScript.</p>

<p>Just imagine the overhead of porting hundreds of unit tests over to a different language, along with the application. If that had been a requirement of our experiment, it would not have happened, and we would not have learned what we did. <em>(In the end we did keep using the C# implementation in production, but the speedy rewrite was still a valuable learning aid.)</em></p>

<h3>Acceptance Tests Behave Exactly Like Your Users</h3>

<p>When an acceptance-level test passes, you can be confident that a whole user journey using your application is working correctly. That&rsquo;s a huge win. When one fails, you can be pretty confident that something important to your users is not working properly and needs attention. Also, very valuable knowledge. This contrasts somewhat with unit-level tests that might tell you something internal is awry with your application, but its real impact to consumers will still often be unknown. Should you fix it? If there are multiple failures, which are the most important? Unit tests will rarely answer these questions for you.</p>

<p>Of course, you will fix it, or else be unable to confidently release your softwareâ€“but surely, at times, you will be fixing something that does not matter, or is no longer relevant to your consumers. Unit tests in this way can encourage code rot, making it very difficult to unpick dependencies that are no longer needed. With acceptance tests, you only need to unpick the dependencies in your application, not also in the tests.</p>

<h2>Synergy</h2>

<p>Much of this is new to me, and certainly isn&rsquo;t without contention. However, the problems with acceptance-only testing, and specifically the solutions to those problems, indicate certain synergistic practices that may improve its viability:</p>

<h3>Thin Layers</h3>

<p>The project we first tried moving to acceptance-only testing on was a very thin layerâ€“a facade over a collection of internal services. It had minimal business logic, and thus few potential execution paths. This certainly allowed us to keep the number of acceptance tests lower than might be expected for a large, complex application, that might branch off into numerous modes of operation. Of course one should probably try to minimise the <a href="http://en.wikipedia.org/wiki/Cyclomatic_complexity">cyclomatic complexity</a> of a code-base anyway, for one&rsquo;s own sanity.</p>

<h3>Statelessness</h3>

<p>If your system maintains a lot of state, acceptance testing can be much harder. This is because you will likely have to set-up much of that state for each test, increasing both developer effort and execution time. Both of which are bad. However, my new favourite thing, immutable sandbox data, may well be your friend in this case.</p>

<h3>Microservices</h3>

<p>We have only just begun experimenting with microservices ourselves, however I think it stands to reason that if each application is small overall, then the number of test cases for each will be small as well. This means the whole suite will run faster, and give you more granular feedback. I differentiate microservices from &lsquo;thin layers&rsquo; in that a microservice may well do data access, input parsing, validation, HTTP handling, and a bit of business logicâ€“but over a very narrow domainâ€“i.e. a thin vertical. A thin layer, on the other hand will perform only one kind of functionâ€“e.g. HTTP handlingâ€“but across multiple facets of the system. If thin layers are the lines of latitude, then microservices can be sections of the lines of longitude.</p>

<h2>Too Short; Read Also</h2>

<p>I hope this article has been a little more useful than the previous one. I have tried to explain more specifically what we actually did, from end-to-end, and how we overcame problems along the way. However, I&rsquo;ve really only scratched the surface. I will hopefully get the chance flesh out some of the ideas here in the coming months. In the mean time, there are plenty of <a href="http://www.shino.de/2012/07/02/atdd-by-example/">books</a> and <a href="http://jonkruger.com/blog/2012/02/20/when-acceptance-tests-are-better-than-unit-tests/">blog posts</a> on the subject of acceptance testing. In addition, Martin Fowler has writen <a href="http://martinfowler.com/articles/microservices.html">a great primer on microservices</a> that&rsquo;s really got me thinking about their utility alongside acceptance-only testing and sandbox data.</p>

<p>Thanks for reading :)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Look ma, no unit tests!]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/04/16/look-ma-no-unit-tests/"/>
    <updated>2014-04-16T17:00:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/04/16/look-ma-no-unit-tests</id>
    <content type="html"><![CDATA[<p><em><strong>UPDATE:</strong> I&rsquo;ve written a <a href="/blog/2014/05/19/acceptance-now">follow-up to this post</a> with a bit more detail into how we made acceptance-only testing work in practice.</em></p>

<p>At OpenTable we strive to deliver change as quickly and correctly as possible. To do this effectively we are always looking for <a href="/blog/2014/02/28/api-benchmark/">new</a> <a href="/blog/2013/08/16/grunt-plus-vagrant-equals-acceptance-test-heaven/">tools</a> <a href="/blog/2014/04/07/upgrading-puppet-with-puppet/">and</a> <a href="/blog/2014/02/10/the-adoption-of-configuration-management/">methods</a> that allow us, the developers, to respond quickly and accurately to changing requirements and environments.</p>

<p>There are a number of practices that we already make use of, helping us to be the most effective team I&rsquo;ve ever worked in:</p>

<ul>
<li>We operate in small teams who each own <em>most</em> of their own vertical.</li>
<li>We use continuous delivery to ship code to production within minutes.</li>
<li>We have a high degree of high-quality test coverage.</li>
<li>We are getting better and better at monitoring All The Things.</li>
<li><a href="/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot/">We use Chatops</a>, so communication is central to our work, and keeps remote workers/teams in the loop.</li>
</ul>


<p>All of the above are truly empowering for the dev team, and are conducive to an amazingly stress-free working environment. However, these practices only address the infrastructure, culture, and ceremony surrounding our work. What if there was something else? Something about the way we write the code itself, that could increase our velocity yet further, without compromising our integrity&hellip;</p>

<blockquote><p>There are a number of practices that we already make use of, helping us to be the most effective team I&rsquo;ve ever worked in&hellip; What if there was something else?</p></blockquote>

<p>Well, on a recent project, we found one such way: <em><em>we decided to delete all of the unit and integration tests</em></em>.</p>

<p><em>What?! Are we quite mad?</em> You may be thinking&hellip; Well, it took me a little time to get used to this idea as well, but read on and you&rsquo;ll see that it was actually the most sane thing we could have possibly done  .</p>

<h2>Survival of the testedest</h2>

<p>In the beginning, the project had 100% unit test coverage, there were no external dependencies, and the world was Good.</p>

<p>Soon afterwards, a tall shadow appeared in the glorious unit-tested sunset. External dependencies had arrived. Like good little developers we added integration tests. It hurt, our codebase grew, we had occasional false-failures, but we were travelling the path well trodden. We had evaded the First Menace, and surrounded ourselves with heavy armour, we were safe. Things seemed to be Good.</p>

<p><em>Meanwhile&hellip;</em></p>

<p>We realised that some of the things that would be important to our consumers were still not covered by our tests. Things like actual HTTP responses, serialisation, and the like. These are things that don&rsquo;t always need to be tested explicitly, but since this was a third-party-developer-facing system, we really wanted to be sure that the interface worked exactly as we wanted, HTTP headers, character encoding, date formatting, the lot.</p>

<p>So, playing the role of our consumers, we engineered high-level acceptance tests, behaving byte-for-byte as we expected our customers to do.</p>

<p>Now, with the triple-action protection of three layers of tests, we felt our project was the most minty-fresh piece of haute engineering we had ever laid keyboards on.</p>

<p>We were wrong.</p>

<h2>Tests, tests, tests, duplication.</h2>

<p>Up to this point, we had operated in a near-vacuum. That was fine, we had been working quickly to implement a sub-set of an existing and well-used API, so we knew which were the most important features that needed porting. We continued, largely happy with our creation, for some time.</p>

<p><em>Then, gazing up from the receding tide of the third trimester were the hungry eyes of the Second Menace. Our users were upon us!</em></p>

<p>Our early adopters were great, giving us a lot of helpful feedback and helping us shape the API into a genuinely usable v1. However, responding to this change required a greater degree of flexibility in the code than we had required up to this point. Our triple-chocolate-crunch of pithy tests was starting to really slow us down, and rot our teeth. The main reason for this: duplication.</p>

<p>We had tried from the start to avoid any duplication in our tests, but this was all but impossible to achieve. You just can&rsquo;t test an API call end-to-end in an acceptance-test style, without inadvertently testing all of the underlying logic for that call. Code which was already covered by unit tests, and often integration tests as well. Therefore each move we made came with the burden of updating multiple tests. Often materially very similar tests, but written to test a different layer of the same cake. We were between an immovable monolith and a very heavy boulder&mdash;and had a hoarde of features we still wanted to smash, who were freely bounding over the mountain tops, and out of reach.</p>

<p>It was time to cut ourselves free.</p>

<h2>Ripping off the plaster</h2>

<p>The idea that we might not need all these layers of tests was first mooted by fellow OpenTable engineer Arnold Zokas. My initial reaction was one of slight incredulity. Delete all those tests that we&rsquo;ve so carefully caressed and cajoled into a thing of beauty?! Strip off the armour?! I wasn&rsquo;t immediately convinced. However, the pain of implementing new features was starting to burn, so I was interested.</p>

<blockquote><p>I wasn&rsquo;t immediately convinced.</p></blockquote>

<p>We talked about it&mdash;what was necessary about the unit tests? What was their real worth? We had to test many of those things from the outside-in anyway, with the acceptance tests, so why test them twice? The logic started to stack up. I was convinced this was the right thing to do.</p>

<p>Take a deep breath. <em>RIP!</em> Aah, there, done.</p>

<p>There was a little bleeding, some gaps in our acceptance tests that had to be filled, some complex set-up logic from the integration tests that had to be ported to work with the acceptance tests. A few days' worth of cleanup and patching in the background, and&hellip; tentatively&hellip; we were done.</p>

<p>For me at least, this was a bold move. But it shouldn&rsquo;t have seemed so, we knew all of our endpoints were acceptance-tested, including every supported API call. My primary worry was how we were going to nail down the exact cause of bugs with no code-level testing. This turned out to be nowhere near as bad as I expected.</p>

<blockquote><p>We knew all of our endpoints were acceptance-tested, including every supported API call.</p></blockquote>

<h2>What just happened?</h2>

<p>I like to visualise this as if we were building <a href="http://en.wikipedia.org/wiki/Gateway_Arch">a giant arch</a>. At first, you build a temporary structure with scaffolding (the unit and integration tests). As time goes on you construct a hardened permanent structure (the software). On top of the software, you layer your <a href="http://en.wikipedia.org/wiki/Structural_health_monitoring">structural integrity monitors</a> (acceptance tests). Eventually, there is no need for the scaffolding any more; the structure is self-supporting, and future modifications can rely on this&mdash;time to punch out the middle!</p>

<p><em>Of course, there are other considerations, like logging, monitoring, and providing sandbox data, which all contributed to making this feasable&mdash;but that&rsquo;s for another post.</em></p>

<h2>Was it worth it?</h2>

<p>Unequivocally, yes. Since making this decision, we have been unhindered by our tests, and they are back to being a much loved part of the project. We have had no problems that would have been caught by unit tests, and we can still do TDD with our acceptance tests. In addition, I think removing the crutch of unit tests may have improved our discipline somewhat: <em>it keeps us thinking in the context of the end-user at all times, so we never spend time working on a feature that isn&rsquo;t directly useful to our consumers.</em></p>

<blockquote><p>It keeps us thinking in the context of the end-user at all times, so we never spend time working on a feature that isn&rsquo;t directly useful to our consumers.</p></blockquote>

<h2>YMWMCV</h2>

<p>Of course, every project is unique (just like every other project), so <em>your mileage will most certainly vary.</em> We were working on a stateless facade over a small but crucial subset of the business&mdash;making reservations. For relatively small, stateless projects, this approach has worked brilliantly. However, when things do go wrong at development time, they could be at any layer in the stack, and you often need to attach a debugger to find out what happened. This is less than ideal, but in our case was a very cost-effective compromise.</p>

<p>The upshot, for me at least, is that you shouldn&rsquo;t be afraid to shirk convention when the project demands it. By really analysing what each part of your project is doing, you can cut the cruft, helping you move faster <em>without</em> breaking stuff.</p>

<blockquote><p>By really analysing what each part of your project is doing, you can cut the cruft, helping you move faster <em>without</em> breaking stuff.</p></blockquote>
]]></content>
  </entry>
  
</feed>
