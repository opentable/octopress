
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>OpenTable Tech UK Blog</title>
  <meta name="author" content="OpenTable">

  
  <meta name="description" content="We are pleased to announce the version 1.0 release of Hobknob, our open-source feature toggle management system. With it comes a few additions and &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://tech.opentable.co.uk/blog/page/2/">
  <link href="/favicon-32.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="OpenTable Tech UK Blog" type="application/atom+xml">
  <script src="//fonts.otstatic.com/dwh4rpg.js"></script>
  <script>try{Typekit.load();}catch(e){}</script>

  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-2621903-16']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">OpenTable Tech UK Blog</a></h1>
  
    <h2>The technology blog for OpenTable UK.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:tech.opentable.co.uk" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/authors">Authors</a></li>
  <li><a href="/about">About this blog</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/22/hobknob-v1-dot-0-now-with-authorization/">Hobknob v1.0: Now with authorization</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-22T14:00:31+01:00" pubdate data-updated="true">Oct 22<span>nd</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>We are pleased to announce the version 1.0 release of <a href="https://github.com/opentable/hobknob">Hobknob</a>, our open-source feature toggle management system. With it comes a few additions and several improvements.</p>

<p>This post will expand on some of the changes, in particular, authorisation via access control lists.
For an introduction to Hobknob, see our previous post: <a href="http://tech.opentable.co.uk/blog/2014/09/04/introducing-hobknob-feature-toggling-with-etcd/">Introducing Hobknob: Feature toggling with etcd</a>.</p>

<h2>Authorisation with ACLs</h2>

<p>A much requested feature was the ability to control who can add/update/delete toggles on an application by application basis. We achieve this via the use if an Access Control List for each application. Users that are part of the ACL for an application are known as application owners.</p>

<p><img src="/images/posts/hobknob-owners.png" alt="Hobknob Owner List" /></p>

<p>Application owners can (for an owned application):</p>

<ul>
<li>Add toggles</li>
<li>Set the value of a toggle</li>
<li>Delete toggles</li>
<li>Add additional owners</li>
<li>Remove owners</li>
</ul>


<p>Everyone can:</p>

<ul>
<li>Add an application</li>
<li>See toggles</li>
<li>See application owners</li>
<li>See the audit trail for a toggle</li>
</ul>


<p>When a user creates an new application, they are automatically added as an owner for that application.
The user can then add other application owners by clicking the &lsquo;Add user&rsquo; button in the Owners panel and entering the users email address.</p>

<p><strong>Note:</strong> this feature is only available when authentication is enabled. If Hobknob is not configured to require authentication, everyone has owner permissions to all applications. See the <a href="https://github.com/opentable/hobknob#configuring-authentication">readme</a> for more information on how to configure authentication.</p>

<h2>Deleting Toggles</h2>

<p>Feature toggles can now be deleted. This ability is available on the toggle view (get there by clicking a toggle name in the application view).</p>

<p><img src="/images/posts/hobknob-delete.png" alt="Hobknob Toggle Delete" /></p>

<p>You&rsquo;ll notice the delete toggle button in the Danger Zone panel (we didn&rsquo;t steal that idea from Github, honest). You&rsquo;ll need to confirm the delete by clicking the delete button a second time.</p>

<p><strong>Warning:</strong> Deleting a toggle will perform a &lsquo;hard&rsquo; delete, that is, the key is deleted in etcd. The audit will persist however, and can be accessed via this route: <code>/#!/applications/app-name/toggle-name</code>. You are also allowed to re-add a toggle, and the audit will be appended to an existing audit for that toggle name.</p>

<p><strong>Note:</strong> If authentication is enabled, you must be an application owner to delete a toggle.</p>

<h2>Makeover</h2>

<p>Gone is the &lsquo;Add Toggle&rsquo; modal dialog from the previous version. This is replaced by two separate inline forms.</p>

<p>Applications are now added by clicking &lsquo;Add&rsquo; in the sidebar.</p>

<p><img src="/images/posts/hobknob-newapplication.png" alt="Hobknob New Application" /></p>

<p>Toggles are added by clicking &lsquo;New Toggle&rsquo; in the Toggles panel for an application.</p>

<p><img src="/images/posts/hobknob-newtoggle-v2.png" alt="Hobknob New Toggle" /></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/06/puppetconf-2014-part-3/">PuppetConf 2014 - Part 3</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-06T13:43:36+01:00" pubdate data-updated="true">Oct 6<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Day 2</h2>

<p>This is our summary of PuppetConf 2014. In our <a href="http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-2/">previous post</a> we gave an overview of the first day of the conference. This post will provide an
overview of the final day.</p>

<p>There were even more inspiring keynotes and lots more talks which have given us plenty of ideas to go home and think about.</p>

<h3>Key Notes</h3>

<h4>Animating the Puppet: Creating a Culture of Puppet Adoption &ndash; Dan Spurling (<a href="https://twitter.com/spurling">@spurling</a>), Getty Images &ndash; <a href="http://www.slideshare.net/PuppetLabs/keynote-animating-the-puppet-creating-a-culture-of-puppet-adoption-puppetconf-2014">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="/images/posts/puppetconf-dan.jpg">
</div>


<p>Dan Spuring, VP of Tech Services at Getty came out of the gate with a strong message. His <a href="http://www.urbandictionary.com/define.php?term=GSD">GSD</a> t-shirt
giving you a clear understanding of who he is. His talk about creating a culture of Puppet adoption at his company was a great story of how challenging it
can be to move various business units with projects of various ages to a configuration-management (with Puppet) ethos.</p>

<p>I think it is good to hear that they are rolling cm out into that huge backlog of legacy infrastructure that we all try to pretend isn’t there.
How do you make it integrate into existing processes? How do you sell the DevOps message at the same time as introducing a tool like Puppet into the mix as
part of that message? Dan gave some thoughts on this and it was good to hear some of that from someone who appears to be on the other side of that challenge.</p>

<p>One of the analogies that he used I that found quite useful was that undertaking a project like this is like moving a boulder. It requires an executive sponsor to
get the thing moving at all and then it requires everyone pulling in the same direction if it’s ever doing to get anywhere.</p>

<p>The big take-away was that you need to puppetize right away &ndash; that you can’t wait for the right environment or conditions to start doing it, you just need
start now and demonstrate it. This echo’s the Continuous Delivery ideal of &ldquo;if it hurts, then do it more often&rdquo;.</p>

<h4>Decentralize Your Infrastructure &ndash; Alan Green, Sony Computer Entertainment America &ndash; <a href="http://www.slideshare.net/PuppetLabs/keynote-decentralize-your-infrastructure-alan-green-sony-computer-entertainment-america">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="/images/posts/puppetconf-alan.jpg">
</div>


<p>Alan’s talk posed an interesting argument: decentralise and let your developers choose the tools and services that they want &ndash; just make it easy for them to
do so. This obviously flies in the face of conventional sysadmin wisdom of trying to centralise, standardise and control everything but for an organisation the
size of scale of SCEA this is just never going to work. Sony has many different studios, each has their own special requirements and tooling that they need to
try and support.</p>

<p>The story of the interaction with these studios is a great classic sysadmin story that is worth repeating. It starts with something we have all heard before &ldquo;I
need to X right now because it’s preventing me from releasing this game on time”. The reaction here is to either say Yes and risk burning out your people getting
it done or No risk your career if the release date gets pushed. As a sysadmin you&rsquo;re on the back-foot at this point &ndash; you pretty much have to do whatever it takes.
If you decentralize your infrastructure you get to turn the tables &#8220;No I don’t have tool X but we do have tool Y and Z that will meet your needs&rdquo;. This gives
the engineers/managers the choice to make rather than you &ndash; they can go out on their own and implement their first choice tool and it will take a bit longer or
they can have something supported by the team right now. Alan also made a interesting call-back to Kate Matsudaira’s keynote of the previous day when he said that
it’s all about honesty and trust. Be truthful with your engineers about what you are capable of achieving or not.</p>

<p>This is the sort of thing we do here at OpenTable and it’s been working very well. You need to design puppet to be as flexible as possible and to support those
teams that need support in their puppet implementations. Having a diverse set of tools is not a bad thing &ndash; especially when you are dealing with creative people &ndash;
it keeps them creative and you can push that creativity back into the product. You&rsquo;re also decentralising control, giving teams the ability to move their
infrastructure as fast as they need to move the product &ndash; meaning that your business is going to move faster get meet it’s ROI (because managers care about that
sort of thing)</p>

<h4>Q&amp;A with Luke Kanies</h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="/images/posts/puppetconf-luke-2.jpg">
</div>


<p>The last &ldquo;keynote&rdquo; of the conference brought Luke back to the stage for a Q&amp;A with the audience. Allowing people to text in questions live led to some amusement
and once the silly questions were out of the way ( what is your favourite book?, what is your favourite animal? ) we got down to some of the big questions that
people really wanted answers to.</p>

<p><strong>Q</strong>: What is the roadmap for Puppet Apps?<br/>
<strong>A</strong>: I would be surprised if we release more than one per quarter, I’d rather put out four than 20, with five releases for each app. We are a small company,
and we have to try not to get overextended to the point where we can’t evolve the apps. They have to be evolved to be successful.</p>

<p>This seems fair, they is a lot of work involved in putting together something that is polished and tested and ready for market.</p>

<p><strong>Q</strong>: What is the future of Open Source Puppet?<br/>
<strong>A</strong>: My goal is to keep the two products complementary, and to understand each is used for different reasons .. We’re trying to change how the market works
and thinks and this is done better with software that’s absolutely everywhere.</p>

<p>He probably gets asked this all the time. The more features that are poured into Enterprise it would be easy to think that the OSS efforts are diminishing and
that there is even motivation for them to close-source. My conversations with various parties suggest that this is far from the case and I think that open source
puppet community will continue to be vibrant for a long time yet.</p>

<p><strong>Q</strong>: Where does Puppet fit into environments that don’t require convergence, where instead of adjusting the container you just re-provision?<br/>
<strong>A</strong>: Containers are a result of 10 to 15 years of investment in virtualization, so it’s easy to switch from the virtualization world to the containers world —
but a container can’t do everything.</p>

<p>This is a very pragmatic argument and he’s right. Containers are a very exciting space right now and there is no doubt that it will be a big part of the future
but the community and tooling needs to mature and there is also going to be a very long tail of “traditional” virtualisation technologies around for a very
long time yet.</p>

<p><strong>Q</strong>: Are there any plan to integration remote orchestration into Puppet?<br/>
<strong>A</strong>: It’s an area we are investing heavily in, and I’m personally investing heavily in. … I’m a big fan of small independent tools that do one job and do it
correctly, rather than big huge tools that do a lot. I want to make our orchestration better, not by adding to Puppet, but by adding tools. I don’t want to add
more functionality to Puppet, but add functionality to the Puppet ecosystem.</p>

<p>MCollective has been in the puppet eco-system for a while now. It’s going to be getting a lot more attention over the next year so I am very excited to see how
this evolves.</p>

<h3>Tech Talks</h3>

<h4>Continuous Integration for Infrastructure as Code &ndash; Gareth Rushgrove, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/continuouslytestinginfrastructure">Slides</a></h4>

<p>Arguable one of the most interesting talks of the conference. This talk took the idea of infrastructure TDD to the next level. What would it be like to be able
to test common expectations of your infrastructure (monitoring, backups, machines in each region, budget limitations). There are lots of built-in assumptions
that we make about of infrastructure and a lot of business decisions that have been difficult to codify. This talk raising the challenge of providing a complete
API for your infrastructure and then testing against it.</p>

<ul>
<li>usual tools (serverspec, wrecker)

<ul>
<li>for containers</li>
<li>TDD</li>
</ul>
</li>
<li>Policy Driven development</li>
<li>Infrastructure as an API</li>
<li>common expectations (budget etc)</li>
<li>clojure</li>
<li>can you generate serverspec tests from PuppetDB data??? &ndash; yes!</li>
<li>rake test::role::web_server</li>
<li><a href="https://github.com/garethr/serverspec-puppetdb">serverspec-puppetdb</a></li>
<li>rspec outputter &ndash; monitoring &ndash; using it as a bridge</li>
</ul>


<h4>Experiences from Running Masterless Puppet &ndash; Erik Dalén, Spotify &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf-2014-1">Slides</a></h4>

<p>Erik (this years MVP) always has a lot of interesting insights about Puppet from scaling out the infrastructure at Spotify and this talk is no exception.
This talk explains their decision to go masterless and the challenges in doing so. It seems that they have put in a lot of work in writing services to manage
things like hiera data and managing secrets. It is great to see how this approach scales, one can only hope that future work by PuppetLabs with the Apps project
improves this as option for most people.</p>

<ul>
<li>scaling workflow rather than puppet masters</li>
<li>complex modules dependencies make it easy to break things</li>
<li>r10k is still a fixed environment (upgrade apache and progress at the same time)</li>
<li>they use their own tool for secret management</li>
</ul>


<h4>Getting Started with Puppet on Windows &ndash; Josh Cooper, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf2014-gettingstartedwindowsfinal140925174855phpapp01">Slides</a></h4>

<p>This was a basic introduction to Puppet on windows. It covers what is possible and the many edge cases that you might run into. It was also the time to
re-announce the recent support for 64-bit puppet on windows. Thanks to Josh we also got a shout-out for the work we have done with our
<a href="forge.puppetlabs.com/opentable">forge modules</a></p>

<ul>
<li>Basic intro</li>
<li>powershell, registry_key</li>
<li>installing &ndash; mention of 64-bit</li>
<li>puppet resource</li>
<li>supported modules</li>
<li>community modules (inc OT)</li>
<li>geppetto vs VS</li>
<li>problems

<ul>
<li>quotes</li>
<li>case sensitivity</li>
<li>UAC</li>
</ul>
</li>
</ul>


<h4>Test Driven Development with Puppet &ndash; Gareth Rushgrove, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/tddforpuppet-39598529">Slides</a></h4>

<p>This is Gareth’s basic introduction to TDD with Puppet. It covers the latest tooling and how to build yourself a recent CI pipeline for your modules so that
they are forge-ready. Useful for anyone who is new to the space or who hasn’t released any modules yet.</p>

<ul>
<li>TDD</li>
<li><a href="http://rspec-puppet.com/">rspec-puppet</a></li>
<li><a href="http://puppet-lint.com/">puppet-lint</a></li>
<li><a href="https://github.com/guard/guard-rspec">guard</a></li>
<li><a href="https://github.com/gds-operations/puppet-syntax">puppet-syntax</a></li>
<li><a href="https://github.com/puppetlabs/beaker">beaker</a> (vagrant + serverspec)</li>
<li><a href="https://travis-ci.org/">travis</a></li>
<li><a href="https://github.com/garethr/puppet-module-skeleton">puppet module skeleton</a></li>
</ul>


<h4>Using Docker with Puppet &ndash; James Turnbull, Kickstarter &ndash; <a href="http://www.slideshare.net/PuppetLabs/using-docker-with-puppet-puppetconf-2014">Slides</a></h4>

<p>James gave a good introduction to Docker. Showing off the things that Docker is good at and also detailing some of the things that it isn’t.
He also showed how and when to use Puppet in this environment. For anyone moving from a  traditional set-up to a Docker based one then this talk is a must.</p>

<ul>
<li>what is docker</li>
<li>dockerfile</li>
<li>dockerhub</li>
<li>what it does</li>
<li>what it doesn’t

<ul>
<li>low-level</li>
<li>resource dependencies</li>
<li>what runs, when</li>
</ul>
</li>
<li>don’t install puppet inside your containers</li>
<li>puppet apply</li>
</ul>


<h4>Tools and Virtualization to Manage our Operations at Puppet Labs &ndash; Cody Herriges, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/tools-and-virtualization-to-manage-our-operations-at-puppet-labs-puppetconf-2014">Slides</a></h4>

<p>Cody, is a member of the PuppetLabs operations team and wow they seriously have their work cut out for them. They have to manage pretty much every network,
vm technology and cloud platform available. This gives some of the challenges in doing that and some of the tools they have built to help them in
achieving that.</p>

<ul>
<li>all the VM technologies</li>
<li>all the cloud platforms</li>
<li>all the network providers</li>
<li>automation</li>
<li>monitoring (ELK)</li>
<li>vmpooler (<a href="https://github.com/puppetlabs/vmpooler">https://github.com/puppetlabs/vmpooler</a>)</li>
</ul>


<h3>Other Talks</h3>

<ul>
<li>The Switch as a Server &ndash; Leslie Carr, Cumulus Networks &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-switch-as-a-server-puppetconf-2014">Slides</a></li>
<li>Intro to Using MCollective &ndash; Devon Peters, Jive Software &ndash; <a href="http://www.slideshare.net/PuppetLabs/intro-to-using-mcollective-puppetconf-2014">Slides</a></li>
<li>How Puppet Enables the Use of Lightweight Virtualized Containers &ndash; Jeff McCune, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/how-puppet-enables-the-use-of-lightweight-virtualized-containers-jeff-mc-cune-puppet-labs">Slides</a></li>
<li>Server Locality Using Razor and LLDP &ndash; Jonas Rosland, EMC &ndash; <a href="http://www.slideshare.net/PuppetLabs/server-locality-withrazorandlldp">Slides</a></li>
<li>Node Classifier Fundamentals &ndash; Dan Lidral-Porter, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/node-classifier-fundamentals-dan-lidralporter-puppet-lab">Slides</a></li>
<li>What&rsquo;s Next for Puppet Enterprise &ndash; Lindsey Smith, Puppet Labs &amp; Susannah Axelrod, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/whats-next-for-puppet-enterprise-and-beyond">Slides</a></li>
<li>The DevOps Field Guide to Cognitive Biases (2nd Edition) &ndash; Lindsay Holmwood, Bulletproof Networks</li>
<li>Delegated Configuration with Multiple Hiera Databases &ndash; Robert Terhaar, Atlantic Dynamic &ndash; <a href="http://www.slideshare.net/PuppetLabs/rob-terhaar-puppetconf2014">Slides</a></li>
<li>Understanding OpenStack Deployments &ndash; Chris Hoge, OpenStack Foundation &ndash; <a href="http://www.slideshare.net/PuppetLabs/understanding-openstack-deployments-puppetconf-2014">Slides</a></li>
<li>Implementing Puppet at a South American Government Agency, Challenges and Solutions &ndash; Pablo Wright, Edrans &ndash; <a href="http://www.slideshare.net/PuppetLabs/implementing-puppet-at-a-south-american-government-agency-challenges-and-solutions-pablo-wright-edrans">Slides</a></li>
<li>Infrastructure as Software &ndash; Dustin J. Mitchell, Mozilla, Inc. &ndash; <a href="http://www.slideshare.net/PuppetLabs/infrastructure-as-software-dustin-j-mitchell-mozilla-inc?">Slides</a></li>
<li>Dev to Delivery with Puppet &ndash; Sam Bashton, Bashton Ltd. &ndash; <a href="http://www.slideshare.net/PuppetLabs/dev-to-delivery-with-puppet-sam-bashton-bashton-ltd">Slides</a></li>
<li>Get Puppet Enterprise into Your Company &ndash; Iko Saadhoff, KPN</li>
<li>The Puppet Master on the JVM &ndash; Chris Price, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-puppet-master-on-the-jvm-puppetconf-2014">Slides</a></li>
<li>The Grand Puppet Sub-Systems Tour &ndash; Nicholas Fagerlund, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-grand-puppet-subsystems-tour-nicholas-fagerlund-puppet-labs">Slides</a></li>
<li>Building Community: One Puppet Module at a Time &ndash; Diane Mueller, Red Hat &amp; Diego Castro, Getup Cloud</li>
<li>Puppet for Everybody! &ndash; Federated and Hierarchical Puppet Enterprise &ndash; Chris Bowles, University of Texas at Austin &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppet-for-everybody-federated-and-hierarchical-puppet-enterprise-puppetconf-2014">Slides</a></li>
<li>Puppetizing Multitier Architecture &ndash; Reid Vandewiele, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetizing-multitier-architecture-puppetconf-2014">Slides</a></li>
<li>The Evolving Design Patterns of Puppet Enterprise &ndash; Jonathan Spinks, Sourced Group &amp; John Painter, Sourced Group &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-evolving-design-patterns-of-puppet-enterprise-jonathan-spinks-sourced-group-john-painter-sourced-group">Slides</a></li>
<li>From Development to Testing to Deployment with Puppet Enterprise and Microsoft Azure &ndash; Ross Gardler, Microsoft Open Technologies, Inc. &ndash; <a href="http://www.slideshare.net/PuppetLabs/from-development-to-testing-to-deployment-with-puppet-enterprise-and-microsoft-azure-ross-gardler-microsoft-open-technologies-inc">Slides</a></li>
<li>Exploring the Final Frontier of Data Center Orchestration: Network Elements &ndash; Jason Pfeifer, Cisco &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf-cisco">Slides</a></li>
<li>An In-Depth Introduction to the Puppet Enterprise Console &ndash; Ruth Linehan, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/an-indepth-introduction-to-the-puppet-enterprise-console-ruth-linehan-puppet-labs">Slides</a></li>
<li>Packaging Software, Puppet Labs Style &ndash; Melissa Stone, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/packaging-software-puppet-labs-style-puppetconf-2014">Slides</a></li>
<li>Orchestrated Functional Testing with Puppet-spec and Mspectator &ndash; Raphaël Pinson, Camptocamp &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf-mspectator-talk">Slides</a></li>
<li>Fully Automate Application Delivery with Puppet and F5 &ndash; Colin Walker, F5 &ndash; <a href="http://www.slideshare.net/PuppetLabs/i-control-rest-presentation-for-puppet">Slides</a></li>
<li>Managing the File and Exposing the API &ndash; Christopher Webber, Chef Software</li>
<li>Case Study: Developing a Vblock Systems Based Private Cloud Platform with Puppet and VMware vCloud Suite &ndash; Peng Liu &amp; Paul Harb, VCE &ndash; <a href="http://www.slideshare.net/VCE_Computing/puppet-confvce-preso20140925">Slides</a></li>
<li>Got Logs? Get Answers with Elasticsearch ELK &ndash; Jordan Sissel, Elasticsearch &ndash; <a href="http://www.slideshare.net/PuppetLabs/got-logs-get-answers-with-elasticsearch-elk-puppetconf-2014">Slides</a></li>
<li>Managing Network Security Monitoring at Large Scale with Puppet &ndash; Michael Pananen &amp; Chris Nyhuis, Vigilant Technology Services &ndash; <a href="http://www.slideshare.net/PuppetLabs/managing-network-security-monitoring-at-large-scale-with-puppet-puppetconf-2014">Slides</a></li>
<li>Building and Testing from Scratch a Puppet Environment with Docker &ndash; Carla Souza, Reliant &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppet-conf2014">Slides</a></li>
</ul>


<h3>Other Interesting Links</h3>

<ul>
<li><a href="http://blog.superk.org/2014/09/puppet-conf-2014-review.html">http://blog.superk.org/2014/09/puppet-conf-2014-review.html</a></li>
<li><a href="http://www.olindata.com/blog/2014/09/first-impressions-new-cfacter">http://www.olindata.com/blog/2014/09/first-impressions-new-cfacter</a></li>
<li><a href="http://cwebber.net/blog/2014/09/26/i-am-not-a-coder/">http://cwebber.net/blog/2014/09/26/i-am-not-a-coder/</a></li>
<li><a href="http://www.slideshare.net/PuppetLabs/tag/puppetconf-2014">http://www.slideshare.net/PuppetLabs/tag/puppetconf-2014</a></li>
<li><a href="http://puppetlabs.com/blog/puppetconf-2014-day-1-tips-treats-and-tweets">http://puppetlabs.com/blog/puppetconf-2014-day-1-tips-treats-and-tweets</a></li>
<li><a href="http://puppetlabs.com/blog/puppetconf-2014-day-2-luke-q-and-a-devops-containers-and-more">http://puppetlabs.com/blog/puppetconf-2014-day-2-luke-q-and-a-devops-containers-and-more</a></li>
<li><a href="http://puppetlabs.com/blog/puppetconf-2014-day-1-tips-treats-and-tweets">http://puppetlabs.com/blog/puppetconf-2014-day-1-tips-treats-and-tweets</a></li>
<li><a href="http://puppetlabs.com/blog/puppet-conf-2014-wrap-up">http://puppetlabs.com/blog/puppet-conf-2014-wrap-up</a></li>
<li><a href="https://forge.puppetlabs.com/approved/criteria">https://forge.puppetlabs.com/approved/criteria</a></li>
<li><a href="http://puppetlabs.com/blog/puppet-server-bringing-soa-to-a-puppet-master-near-you">http://puppetlabs.com/blog/puppet-server-bringing-soa-to-a-puppet-master-near-you</a></li>
<li><a href="https://github.com/puppetlabs/puppetlabs-strings/">https://github.com/puppetlabs/puppetlabs-strings/</a></li>
<li><a href="http://bitergia.dev.puppetlabs.com/browser/">http://bitergia.dev.puppetlabs.com/browser/</a></li>
<li><a href="https://www.flickr.com/photos/pleia2/sets/72157648049231891">https://www.flickr.com/photos/pleia2/sets/72157648049231891/</a></li>
<li><a href="http://theshipshow.com/2014/10/the-pulse-of-puppetconf-2014/">http://theshipshow.com/2014/10/the-pulse-of-puppetconf-2014/</a></li>
<li><a href="http://www.theregister.co.uk/2014/09/23/puppetconf_2014_keynote/">http://www.theregister.co.uk/2014/09/23/puppetconf_2014_keynote/</a></li>
<li><a href="http://www.infoq.com/news/2014/09/puppet-approved-modules">http://www.infoq.com/news/2014/09/puppet-approved-modules</a></li>
<li><a href="https://github.com/ferventcoder/puppet-chocolatey-presentation">https://github.com/ferventcoder/puppet-chocolatey-presentation</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/06/puppetconf-2014-part-2/">PuppetConf 2014 - Part 2</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-06T12:30:58+01:00" pubdate data-updated="true">Oct 6<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2>Day 1</h2>

<p>This is our summary of PuppetConf 2014. In our <a href="http://tech.opentable.co.uk/blog/2014/10/06/puppetconf-2014-part-1/">previous post</a> we gave an overview of the contributor summit. This post will provide an overview
of the first day of PuppetConf.</p>

<p>As you might expect there were great keynotes with plenty of announcements and too many talks for us to attend. We have provided an outline for all the talks
we did attend and links to those we didn&rsquo;t.</p>

<h3>KeyNotes</h3>

<h4>Nearly a Decade of Puppet: What We’ve Learned and Where We’re Going Next &ndash; Luke Kanies, PuppetLabs &ndash; <a href="http://www.slideshare.net/PuppetLabs/luke-kanies-keynote-nearly-a-decade-of-puppet-what-weve-learned-and-where-were-going-next-puppetconf-2014">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="/images/posts/puppetconf-luke-1.jpg">
</div>


<p>The big keynote of the event to kick off the first day from the author of Puppet himself. This was obviously going to be a tweet worthy affair full of photos
and big announcements and it did not disappoint.</p>

<p>Native Clients (CFactor + C++ rewrite of agents) are coming in the very near future. This is not only a matter of improving the performance for existing users part
of philosophy of PuppetLabs to become ubiquitous across as many devices and platforms as possible. This is one of those improvements that is really setting up
PuppetLabs for the future.</p>

<p>Puppet Server (a.k.a the Clojure rewrite). This is PuppetLabs big move away from Ruby on onto the JVM. Being on the JVM means they can slowly rewrite the
codebase while also maintaining compatibility thanks to JRuby. They have gained a lot of experience with Clojure thanks to the PuppetDB &amp; TrapKeeper projects and given how
successful that project has been it has helped ease many of the fears people have in moving the JVM. Puppet Server is also a self contained application so there
is no longer any need to worry about the whole apache/passanger yak shave. There was even a demo on the metrics that are now exposed by Puppet Server &ndash; yes
you can now plug Puppet into graphite.</p>

<p>There have been plenty of follow-ups on this that you might be interested in reading:</p>

<ul>
<li><a href="http://www.infoworld.com/article/2687553/devops/puppet-server-drops-ruby-for-clojure.html">http://www.infoworld.com/article/2687553/devops/puppet-server-drops-ruby-for-clojure.html</a></li>
<li><a href="http://puppetlabs.com/blog/puppet-server-bringing-soa-to-a-puppet-master-near-you">http://puppetlabs.com/blog/puppet-server-bringing-soa-to-a-puppet-master-near-you</a></li>
<li><a href="http://puppetlabs.com/blog/new-era-application-services-puppet-labs">http://puppetlabs.com/blog/new-era-application-services-puppet-labs</a></li>
<li><a href="https://github.com/puppetlabs/puppet-server">https://github.com/puppetlabs/puppet-server</a></li>
<li><a href="http://www.informationweek.com/cloud/software-as-a-service/puppet-servers-big-revamp/d/d-id/1315934">http://www.informationweek.com/cloud/software-as-a-service/puppet-servers-big-revamp/d/d-id/1315934</a></li>
</ul>


<p>Puppet Apps was the next big announcement. Puppet Apps is actually a fantastic piece of marketing around the idea that they are refactoring to a more micro-services
style approach &ndash; splitting up the monolith that is currently the Puppet master into smaller applications that have their own release cadence and can be scaled
separately.</p>

<p>The first announcement from the &ldquo;Apps&rdquo; initiative is Puppet Node Manager the new node classifier which will roll out in the Q1 of 2015 as an add-on
to Puppet Enterprise. Given that Puppet has allowed external node classifiers to be written for a long time now (and there are many open source ones out there)
it is good to see PuppetLabs stepping up and trying to own this more and improve the experience.</p>

<p><a href="http://puppetlabs.com/about/press-releases/puppet-labs-kicks-puppetconf-announcements-major-updates-industrys-most-popular">http://puppetlabs.com/about/press-releases/puppet-labs-kicks-puppetconf-announcements-major-updates-industrys-most-popular</a></p>

<p>Another huge announcement (of which we got a preview at the contributors summit) was Puppet Approved Modules. Luke and the rest of PuppetLabs have the huge
idea that 80% of what you&rsquo;re going to want to configure on your systems should be possible with what is available on the forge. Some of the bigger pieces have
been covered by the module engineers at PuppetLabs under the existing Puppet Support Modules program. This has been fantastic in driving for consensus around
configuration making installation of certain products (like apache) easier for people.</p>

<p>The reality is that if PuppetLabs want to achieve its 80% goal they are are not going to be able to do that with the engineers and resources they have
available to them. Nor do they have the expertise to know about all the software out there. This is where the Puppet Approved program comes in. Its aim is to
provide the same standard of quality that you see in the Supported modules but for modules written by the community. It is easy for users of the forge to
be able to pick out high quality, actively maintained modules and know what they are getting. As a user this is very exciting and as a module author, while
there will be plenty of work for me to do, I am glad that the community is moving in this direction.</p>

<p>Speaking of the community, Luke used this opportunity to announce the finalists and the winner of the Most Valued Puppetier (MVP) competition.</p>

<p>Finalists:</p>

<ul>
<li>Daniele Sluijters (<a href="https://twitter.com/daenney">@daenney</a>)</li>
<li>Felix Frank</li>
<li>Tim Sharp (<a href="https://twitter.com/rodjek">@rodjek</a>)</li>
</ul>


<p>Winner</p>

<ul>
<li>Erik Dalén (<a href="(https://twitter.com/erik_dalen">@erik_dalen</a>)</li>
</ul>


<p>The last part of the keynote was talking about some of the wider thoughts as we look to the next ten years of Puppet and what comes next. There is going to be
more focus on the ubiquity of Puppet, on devices more network device partners and solving problems like orchestration. The next ten years is going to be about
taking Puppet beyond the single node. We are already thinking of machines as cattle and not pets &ndash; Puppet should also better reflect that change.</p>

<p>I for one am very excited by all this and look forward to seeing what comes out over the next few years.</p>

<h4>The Phoenix Project: Lessons Learned &ndash; Gene Kim, IT Revolution Press &ndash; <a href="http://www.slideshare.net/PuppetLabs/keynote-the-phoenix-project-lessons-learned-puppetconf-2014">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="/images/posts/puppetconf-gene.jpg">
</div>


<p>This was a great overview of Gene’s research of DevOps and how that intersects with high performing organisations. There were many interesting results that came
out the the survey that he did in joint co-operation with PuppetLabs many of which he shared during this talk.</p>

<p>I think the one that stands out and often tweeted is the following:</p>

<p><em>&ldquo;High performers have 30x more deployments and 8000x faster lead time, 2x the change success rate and 12x faster recovery&rdquo;</em></p>

<p>Read that again &ndash; wow.</p>

<p>This talk as one might expect was all about DevOps, its history, why and how it works. Even if you&rsquo;re fully familiar with the whole culture of DevOps there are
plenty of things to be learnt from this keynote and I look forward to re-watching it when the video lands on YouTube.</p>

<h4>Trust Me &ndash; Kate Matsudaira, Popforms &ndash; <a href="http://www.slideshare.net/PuppetLabs/keynote-trust-me-puppetconf-2014">Slides</a></h4>

<div style="float:right;margin:0 10px 10px 10px;width:50%">
  <img src="/images/posts/puppetconf-kate.jpg">
</div>


<p>Following the theme of culture, Kate’s talk was a refreshing look at the culture of trust within an organisation. Far from being the usual &ldquo;this is what my
company culture looks like&rdquo; sort-of talk, this talk had a lot of practical advice. Discussion of how to build relationships, how to raise your profile within
the organisation and how to improve yourself as a manger. &ldquo;If you use your 1-on-1 to talk about status, you&rsquo;re wasting time. Get to know your boss, solicit
feedback on your performance.&rdquo; &ndash; Great advice like this is littered throughout the talk.</p>

<p>She says that trust is like money and that you need to be wise in how you spend that trust. Most organisations are not a meritocracy and we need to stop thinking that they are. Your relationships within the organisation are just as important as the quality of the work that you do.  There needs to be balance between these two things &ndash; are your relationships as good as the work that you do?</p>

<p>If you want to improve yourself and advance your career, either as an engineer or as a manager then you should absolutely take the time to listen to this talk.</p>

<p><strong>Bonus</strong>: the slides rock! (I won’t spoilt it &ndash; take a look)</p>

<h3>Track Talks</h3>

<h4>The Puppet Debugging Kit: Building Blocks for Exploration and Problem Solving &ndash; Charlie Sharpsteen, Puppet Labs (<a href="https://twitter.com/csharpsteen">@csharpsteen</a>) &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-puppet-debugging-kit-building-blocks-for-exploration-and-problem-solving-charlie-sharpsteen-puppet-labs">Slides</a></h4>

<p>Interesting tool, has some cross-over with the Beaker testing tool. PDK is more for focused manual testing rather than automated acceptance tests.</p>

<ul>
<li><a href="https://github.com/Sharpie/puppet-debugging-kit">https://github.com/Sharpie/puppet-debugging-kit</a></li>
<li>vagrant + oscar (<a href="https://github.com/adrienthebo/oscar">https://github.com/adrienthebo/oscar</a>)</li>
<li>oscar is a collection of vagrant plugins</li>
<li>vagrant-config_builder &ndash;> adds role to share vagrant config  (similar to the beaker nodeset file)</li>
<li>PDK is a set of oscar roles</li>
<li>facter / hiera and Puppet running off GitHub</li>
<li>beaker vs oscar &ndash; oscar is optimised for manual testing. There is room to share stuff here.</li>
</ul>


<h4>Cloudy with a Chance of Fireballs: Provisioning and Certificate Management in Puppet &ndash; Eric Sorenson (<a href="https://twitter.com/ahpook">@ahpook</a>), Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/sorenson-fireballspuppet-conf2014">Slides</a></h4>

<ul>
<li>Apple iCloud uses Puppet + autosign</li>
<li>auto sign doesn&rsquo;t work very well for the cloud</li>
<li>Amazon IAM can be applied by machines &ndash; IAM so instance can read it’s own tags (if it has ec2-client-utils installed)</li>
<li>puts instance_id, ami_id and role into /etc/puppet/csr_attriubutes.yaml</li>
<li>can validate the metadata in the cert using x509</li>
<li>true_node_data = true &amp; immutable_node_data = true</li>
<li>closes security hole of setting certname to fact on agent</li>
</ul>


<h4>Beaker: Automated, Cloud-Based Acceptance Testing &ndash; Alice Nodelman (<a href="https://twitter.com/alicenode">@alicenode</a>), Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/beaker-automated-cloudbased-acceptance-testing-puppetconf-2014">Slides</a></h4>

<p>Having contributed to this tool, I was a little bias in attending this talk. Still plenty of interesting new things that came up though.
If you haven’t heard of beaker yet you will also be interested in our <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">previous blog post</a>.</p>

<ul>
<li>basic introduction to what beaker is and how to use it.</li>
<li>rspec vs test dsl &ndash; both are still supported methods of writing tests.</li>
<li>junit export &ndash; useful when integrating with Jenkins</li>
<li><code>on host as</code> &ndash; is a feature that is coming soon so that you can run a command on a host with a given user account</li>
</ul>


<h4>Puppet Language 4.0 &ndash; Henrik Lindberg (<a href="https://twitter.com/hel">@hel</a>), Puppet Labs  &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppet-language-40-puppetconf-2014">Slides</a></h4>

<p>Lots and lots of interesting information here about the new Puppet 4 syntax and jokes about some of the terrible edge cases of the past. It is good to
know now that with Puppet 4 there is a formal specification for the language so we should no longer see these sorts of weird edge cases of the past.
There are also lots of new features in the language: some to deal with long standing pain points (interation), some to help in the move away from ruby
(Puppet templates) and some to prevent authors themselves writing buggy manifests (the type system). Puppet 4 is going to be an exciting this to use.</p>

<ul>
<li>pain-points / cleanup (specification)

<ul>
<li>numbers are numbers (and not strings)</li>
<li>Type references</li>
</ul>
</li>
<li>heredoc</li>
<li>Puppet templates</li>
<li>iteration (each, map, filter, reduce, slice, with)</li>
<li>local defaults</li>
<li>Type system</li>
</ul>


<h4>7 Puppet Horror Stories in 7 Years &ndash; Kris Buytaert (<a href="https://twitter.com/KrisBuytaert">@KrisBuytaert</a>), Inuits &ndash; <a href="http://www.slideshare.net/KrisBuytaert/7-years-of-puppet-horror-stories">Slides</a></h4>

<p>This was more of an interactive talk, trying to get members of the audience to try and predict what the actual problem was. For more senior Puppetiers
this was a fun talk, reminding us of the challenges many of us have faced. For newer Puppet developers this was likely acting as a good warning and
foreshadowing of things that may arise if your not careful (or are very unlucky).</p>

<ul>
<li>SSL</li>
<li>Full Disk</li>
<li>Puppet Bugs</li>
<li>DNS (everything is a DNS problem)</li>
</ul>


<h4>Killer R10K Workflow &ndash; Phil Zimmerman (<a href="https://twitter.com/phil_zimmerman">@phil_zimmerman</a>), Time Warner Cable &ndash; <a href="http://www.slideshare.net/PuppetLabs/killer-r10k-39571913">Slides</a></h4>

<p>This was a good introduction to r10k and the reasons you would want to use it. The workflow is pretty straightforward and I think that for anyone managing Puppet at scale this is going to be something to look at.</p>

<ul>
<li>some good use cases for r10k

<ul>
<li>upgrading modules</li>
<li>not having to wait for all role tests to run</li>
<li>deploying everything to all masters (even hiera)</li>
</ul>
</li>
<li>workflow

<ul>
<li>ci per module</li>
<li>release job per module (tags)</li>
<li>deploy job per module (cap task to wrap r10k for masters/nodes)</li>
</ul>
</li>
</ul>


<h3>Other Talks from the Day</h3>

<ul>
<li>Infrastructure-as-Code with Puppet Enterprise in the Cloud &ndash; Evan Scheessele, HP &ndash; <a href="http://www.slideshare.net/PuppetLabs/infrastructure-ascode-with-puppet-enterprise-in-the-cloud-evan-scheessele-hp">Slides</a></li>
<li>Getting Started with Puppet &ndash; Michael Stahnke, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/getting-started-with-puppet-puppetconf-2014">Slides</a></li>
<li>Plan, Deploy &amp; Manage Modern Applications Leveraging vCloud Automation Center and Puppet &ndash; Pradnesh Patil, VMware &ndash; <a href="http://www.slideshare.net/PuppetLabs/plan-deploy-manage-modern-applications-leveraging-vcloud-automation-center-and-puppet-puppetconf-2014">Slides</a></li>
<li>Writing and Publishing Puppet Modules &ndash; Colleen Murphy, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/writing-and-publishing-puppet-modules-colleen-murphy-puppet-labs">Slides</a></li>
<li>To the Future! &ndash; Goals for Puppet 4 &ndash; Andrew Parker, Puppet Labs &amp; Kylo Ginsberg, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/to-the-future-goals-for-puppet-and-facter-1">Slides</a></li>
<li>Managing and Scaling Puppet &ndash; Miguel Zuniga, Symantec &ndash; <a href="http://www.slideshare.net/PuppetLabs/managing-and-scaling-puppet-puppetconf-2014-39542923">Slides</a></li>
<li>What Developers and Operations Can Learn from Design: 6 Ways to Work Better Together &ndash; Ashley Hathaway, IBM Watson &ndash; <a href="http://www.slideshare.net/PuppetLabs/what-developers-and-operations-can-learn-from-design-6-ways-to-work-better-together-puppetconf-2014">Slides</a></li>
<li>Performance Tuning Your Puppet Infrastructure &ndash; Nic Benders, New Relic &ndash; <a href="http://www.slideshare.net/PuppetLabs/performance-tuning-your-puppet-infrastructure-nic-benders-new-relic">Slides</a></li>
<li>&ldquo;Sensu and Sensibility&rdquo; &ndash; The Story of a Journey From #monitoringsucks to #monitoringlove &ndash; Tomas Doran, Yelp &ndash; <a href="http://www.slideshare.net/PuppetLabs/130pm-210pm-tomas-doran-track-1-puppetconf2014-sensu">Slides</a></li>
<li>DevOps Means Business &ndash; Gene Kim, IT Revolution Press &amp; Nicole Forsgren Velasquez, Utah State University &ndash; <a href="http://www.slideshare.net/PuppetLabs/devops-means-business-gene-kim-it-revolution-press-nicole-forsgren-velasquez-utah-state-university">Slides</a></li>
<li>Auditing/Security with Puppet &ndash; Robert Maury, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/auditingsecurity-with-puppet-puppetconf-2014">Slides</a></li>
<li>Absolute Beginners Guide to Puppet Through Types &ndash; Igor Galić, Brainsware OG &ndash; <a href="http://www.slideshare.net/PuppetLabs/absolute-beginners-guide-to-puppet-through-types-igor-galic-brainsware-og">Slides</a></li>
<li>Plugging Chocolatey into Your Puppet Infrastructure &ndash; Rob Reynolds, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/plugging-chocolatey-into-your-puppet-infrastructure-rob-reynolds-puppet-labs">Slides</a></li>
<li>PuppetDB: One Year Faster &ndash; Deepak Giridharagopal, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppetconf-2014">Slides</a></li>
<li>The Puppet Community: Current State and Future Plans &ndash; Dawn Foster, Puppet Labs &amp; Kara Sowles, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-puppet-community-current-state-and-future-plans-dawn-foster-puppet-labs-kara-sowles-puppet-labs">Slides</a></li>
<li>Continuous Delivery of Puppet-Based Infrastructure &ndash; Sam Kottler, Digital Ocean &ndash; <a href="http://www.slideshare.net/PuppetLabs/continuous-delivery-of-puppetbased-infrastructure-puppetconf-2014">Slides</a></li>
<li>The Seven Habits of Highly Effective Puppet Users &ndash; David Danzilio, Constant Contact &ndash; <a href="http://www.slideshare.net/PuppetLabs/the-seven-habits-of-highly-effective-puppet-users-puppetconf-2014">Slides</a></li>
<li>Fact-Based Monitoring &ndash; Alexis Le-Quoc, Datadog &ndash; <a href="http://www.slideshare.net/PuppetLabs/fact-based-monitoring-puppetconf-2014">Slides</a></li>
<li>Test-Driven Puppet Development &ndash; Nan Liu, Bodeco &ndash; <a href="http://www.slideshare.net/PuppetLabs/testdriven-puppet-development-puppetconf-2014">Slides</a></li>
<li>A Practical Guide to Modules &ndash; Lauren Rother, Puppet Labs &amp; Morgan Haskel, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/a-practical-guide-to-modules-lauren-rother-puppet-labs-morgan-haskel-puppet-labs">Slides</a></li>
<li>Leveraging the PuppetDB API: Puppetboard &ndash; Daniele Sluijters, Nedap</li>
<li>Puppet Availability and Performance at 100K Nodes &ndash; John Jawed, eBay/PayPal &ndash; <a href="http://www.slideshare.net/PuppetLabs/puppet-availability-and-performance-at-100k-nodes-puppetconf-2014">Slides</a></li>
<li>DevOps and Software Defined Networking &ndash; John Willis, Pacific Crest</li>
<li>Razor, the Provisioning Toolbox &ndash; David Lutterkort, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/razor-the-provisioning-toolbox-puppetconf-2014">Slides</a></li>
<li>How to Puppetize Google Cloud Platform &ndash; Katharina Probst, Google, Matt Bookman, Google &amp; Ryan Coleman, Puppet Labs &ndash; <a href="http://www.slideshare.net/PuppetLabs/how-to-puppetize-google-cloud-platform-katharina-e">Slides</a></li>
<li>Continuous Infrastructure: Modern Puppet for the Jenkins Project &ndash; R.Tyler Croy, Jenkins &ndash; <a href="http://www.slideshare.net/PuppetLabs/continuous-infrastructure-modern-puppet-for-the-jenkins-project-rtyler-croy-jenkins">Slides</a></li>
<li>How to Measure Everything: A Million Metrics Per Second with Minimal Developer Overhead &ndash; Jos Boumans, Krux &ndash; <a href="http://www.slideshare.net/PuppetLabs/how-to-measure-everything-a-million-metrics-per-second-with-minimal-developer-overhead-puppetco">Slides</a></li>
<li>How to Open Source Your Puppet Configuration &ndash; Elizabeth Krumbach Joseph, HP &ndash; <a href="http://www.slideshare.net/PuppetLabs/how-to-open-source-your-puppet-configuration-elizabeth-krumbach-joseph-hp">Slides</a></li>
<li>Manageable Puppet Infrastructure &ndash; Ger Apeldoorn, Freelance Puppet Consultant &ndash; <a href="http://www.slideshare.net/PuppetLabs/manageable-puppet-infrastructure-ger-apeldoorn-freelance-puppet-consultant">Slides</a></li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/10/06/puppetconf-2014-part-1/">PuppetConf 2014 - Part 1</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-10-06T11:47:57+01:00" pubdate data-updated="true">Oct 6<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><img src="/images/posts/puppetconf2014.jpg" alt="The start of PuppetConf 2014" /></p>

<p>It has been one week since our attendance at this years PuppetConf and we have just now caught up on all the great talks that were
given and the projects demonstrated over the 3 day period. Here&rsquo;s our summary of the event (split into 3 parts), hopefully you will
find as much inspiration in the content as we have.</p>

<h2>Day 0 &ndash; Contributor Summit</h2>

<p>For the first time, this years Puppet Contributor Summit was held the day prior to the conference itself and I think this was a great idea.
Most of the Puppetlabs staff and many of the high profile community members were in town for PuppetConf anyway so it made sense. There was
roughly 60-70 people in attendance both senior contributors and people new to the community so it was a great mix that led to some
fantastic discussions.</p>

<p>The day itself had two tracks: a module track for forge modules and a core track for people contributing to puppet and factor.</p>

<p>Those of you who have seen our <a href="http://forge.puppetlabs.com/opentable">forge module page</a> will understand why we chose to stay in the module track.
Although I heard there were many great discussions to be had with regards to Puppet 4 in the core track.</p>

<p>Each track was split into three sections: a brief introduction from the track lead Ryan Coleman (<a href="https://twitter.com/ryanycoleman">@ryanycoleman</a>),
followed by some lighting talks and then several hours of hacking and open discussions.</p>

<h3>Lightning Talks:</h3>

<p>Here is a quick overview of the lightening talks from the module track:</p>

<h4>Puppet Analytics (Spencer Krum <a href="https://twitter.com/nibalizer">@nibalizer</a>)</h4>

<p>Spencer gave a quick demonstration of his latest project <a href="http://puppet-analytics.org/">puppet-analytics</a>. This problem that this tool was aiming to
solve was that at the present time the are no good analytics for the forge modules. The number of downloads listed for each module is very inaccurate
and can be easily inflated by (for example) an automated CI process. The point of this web app and it’s corresponding client
<a href="https://github.com/nibalizer/puppet-analytics-client">puppet-analytics-client</a> was to be built into an existing tool chain and for end users to report
which modules and versions they were using. It also has the added benefit that we could also get stats for teams using private forges.</p>

<p>Ryan also commented that PuppetLabs has some metrics it uses for it’s own modules that can be found here:
<a href="http://forge-module-metrics.herokuapp.com/">http://forge-module-metrics.herokuapp.com/</a></p>

<h4>Puppet Community (Daniele Sluijters <a href="https://twitter.com/daenney">@daenney</a>)</h4>

<p>Discussion of the shared namespace for community modules: <a href="http://puppet-community.github.io/">puppet-community</a>. This talk was about a community project
to keep modules in a shared namespace so that everyone can work on them independent of company ownership. There are limitations right now with with
regards to the forge e.g. no shared accounts and no easy migration path to move modules between namespaces but working with Ryan on that.</p>

<p>This is how the boxen project works and it seems to work pretty well.</p>

<h4>Beaker Testing Windows Environments (Liam Bennett <a href="https://twitter.com/liamjbennett">@liamjbennett</a>) &ndash; me!!</h4>

<p>My talk on hacking beaker to work better for testing windows environments.</p>

<p>Demos and PRs. Discussed more in some of our earlier posts: <a href="http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-2-the-windows-story/">Testing Puppet with Beaker pt.2 &ndash; The Windows story</a>
and <a href="http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-3-testing-roles/">Testing Puppet with Beaker pt.3 &ndash; Testing Roles</a></p>

<h4>Module Anti-Patterns (Peter Souter <a href="https://twitter.com/petems">@petems</a>) &ndash; <a href="http://www.slideshare.net/petems/puppet-module-anti-patterns">Slides</a></h4>

<p>Some interesting patterns here that are still quite preverlant in the modules found on the forge. Hopefully improved tooling and the new Puppet Approved
program will help here.</p>

<h4>Puppetlabs ModuleSync tool (Colleen Murphy <a href="https://twitter.com/pdx_krinkle">@pdx_krinkle</a>)</h4>

<p>A demonstration of the the tool <a href="https://github.com/puppetlabs/modulesync">puppetlabs-modulesync</a> which aims to take out some of the pain of managing common
static build files across a number of modules (e.g. a common Rakefile or .travis.yml which the same across almost all modules)</p>

<p>Having used this on a number of our modules now I can say that this in extremely useful and I don’t know how we managed without it. A key use case for us was
adding support for puppet 3.7 into our test matrix of our travis.yml file. 1 line change &ndash; 1 command &ndash; 18 modules updated.</p>

<h4>Strict Variables (Tomas Doran <a href="https://twitter.com/bobtfish">@bobtfish</a>)</h4>

<p>Tomas has one very good point to make here: enable <a href="https://docs.puppetlabs.com/references/latest/configuration.html#strictvariables">strict_variables</a>. Many
languages have a strict option and Puppet’s makes sure to check for those unknown variable references. The latest version of the
<a href="https://github.com/puppetlabs/puppetlabs_spec_helper">puppetlabs_spec_helper</a> supports adding this setting with an environment variable so that you can now
add this into your testing matrix.</p>

<p>We have enabled this on our open source modules and it did indeed surfice a few bugs so go and do it now.</p>

<h4>Puppet Documentation Linting (Peter Souter <a href="https://twitter.com/petems">@petems</a>)</h4>

<p>While we have very good linking for our puppet manifests themselves thanks to the <a href="http://puppet-lint.com/">puppet-lint</a> project. We still do not have any
coverage for our documentation of those manifests. That is where Peter’s <a href="https://github.com/petems/puppet-doc-lint">puppet-doc-lint</a> project comes in and aims
to lint each of you manifests for correct rdoc documentation.</p>

<p>This only supports puppet 3.4.3 right now but it is a useful tool and demonstrations something functional in an area that is missing from the current crop of
community tooling. This is going to become more useful as we want to have good documentation for Puppet Approved status.</p>

<p>It is also worth noting that PuppetLabs themselves have been doing some work in this area with
<a href="https://github.com/puppetlabs/puppetlabs-strings/">puppet-strings</a>. This projects works on puppet 3.6 + and support yard doc but is roughly the same idea.</p>

<h4>Quick Survey (Michael Stahnke <a href="https://twitter.com/stahnma">@stahnma</a>)</h4>

<p>Michael here decided to use the opportunity of having everyone in the room to ask a few questions regarding the state of puppet use and the platforms it’s
deployed on. Not too many surprises here: Debian (mostly ubuntu 12) and RedHat (mostly centos 6) dominate with a small grouping of other platforms like AIX and
Solaris in toe. Some poor individuals still have ubuntu 8 and 10 in production but I won’t name name’s because we have all been in that position before. No
mention of windows but then I did bring that up in my own talk so I think that was covered already.</p>

<h3>Hacking and Discussions</h3>

<p>The second part of the day was the hacking and discussions part. This was more un-conference style with variables tables put together to discuss various topics,
try and resolves issues or hack on projects. There were four main areas that I noticed: module testing, windows, docker, forge improvements (apologies if I
missed your topic/table).</p>

<h4>Module Testing</h4>

<p>This was probably the most common topic and several tables were set up around this idea but a huge range of things were discussed. Some people wanted to know
about how to get up and running with beaker tests using vagrant+vagrant cloud, some wanted to discuss specific platform issues (windows, docker, solaris), other
 wanted to discuss how best to scale out the tests once you have them.</p>

<p>There was some discussion based around the tools like puppet-doc-lint that were demonstrated during the lightning talks and it’s good to see these missing
aspects of the testing tool chain getting some light.</p>

<p>It’s nice to think that we have moved to this stage now where we have all the tools to support a full development tool chain for puppet and that most of the
discussion was around improving and maturing what we have.</p>

<h4>Docker</h4>

<p>Docker is one of those tools that can be considered the latest hotness so it’s no surprise that it gained some interest here also. Many people wanted to see it
in use and demonstrated and to discuss it’s use either from the point of view of being able to test with it or test against it.</p>

<p>I see this topic getting a lot more coverage in the future as more and more teams move into this space.</p>

<h4>Windows</h4>

<p>Led by myself, Drewi Wilson and Travis Fields (<a href="https://twitter.com/tefields">@tefields</a>) the two aims for this discussion were to gather input/feedback from people using
the existing windows modules and to try and discover areas in the windows ecosystem that were not currently managed (either well or at all) by Puppet.</p>

<p>We got some fantastic feedback we got regarding our OpenTable modules &ndash; thank you to everyone who was there any everyone else who reached out about that.</p>

<p>We also managed to start to populate a list of things that need some work. You can contribute to that list
<a href="https://docs.google.com/document/d/1bwgTo4D7lL8REA1s-IIKlfMrvY434Xn0cyZ7b1X-TwQ">here</a></p>

<p>There was also some discussion of using MCollective on Windows. This has been a little painful in the past (I should blog about this in the future) but it will
be getting a little more love going forward. Generally PuppetLabs is very aware of the orchestration space and will be looking into solving this problem with
it’s tools going forward.</p>

<h4>Forge Improvements</h4>

<p>Given that this was the Module track it was obvious that at some point we would all want to discuss improvement that we would want to see in the puppet forge. Ryan
led the table here and there was lots of be said by all.</p>

<p>A couple of interesting documents emerged that you might be interested in:</p>

<ul>
<li><a href="https://docs.google.com/document/d/1N8U_8UnIGFHC1Q6aTyLgx1d6wvvjuyTT1EO-OYSIu3k">Suggestions for the Puppet Approved module criteria</a></li>
<li><a href="https://docs.google.com/document/d/1gwoM8xHnWaRQ3Jqce0oursI_ts5BWnHEUVXRQuIh6Yk">Forge Improvements</a></li>
</ul>


<p>There was also some discussion of how best to pull stats out from the forge. Many people either scrape the API, use the API to take a dump of the whole of the forge
but none of these approaches are best for either the user or for the forge site itself. PuppetLabs uses various approaches to this internally depending on the use
case. Such use cases include: &ldquo;who is using my module?&rdquo; or &ldquo;who is using the bit of code?&rdquo;. There should be improvements to the forge to make answering these sorts
of questions a little easier in the future.</p>

<h2>Summary</h2>

<p>The contributor summit was personally one of the most useful days of the conference. Being able to see the lastest tooling and discuss the latest problems is always
very useful to module authors like ourselves. Hopefully you&rsquo;ll find this summary as useful as we do.</p>

<p>Next up Day 1 &ndash; PuppetConf proper..</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/04/introducing-hobknob-feature-toggling-with-etcd/">Introducing Hobknob: Feature toggling with etcd</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-09-04T20:09:52+01:00" pubdate data-updated="true">Sep 4<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>The ability to dynamically turn features on/off in software without the need to redeploy code is extremely beneficial. Whether you are trialing a new feature or using branch by abstraction to avoid creating feature branches, the use of feature toggles can aid continuous delivery and provide a mechanism to reduce mean time to resolution when an issue occurs.</p>

<p>With a relatively large engineering department with multiple teams spread across the US and UK the need to manage feature toggles has evolved to the point whereby individual teams have developed their own implementations. Most of these are simple config files.</p>

<p>We decided to unify this effort by providing a central place to store feature toggles, provide a dashboard to be able to turn these toggles on/off and provide language specific clients to integrate into our software components.</p>

<p>The results of this was <a href="https://github.com/opentable/hobknob">Hobknob</a>.</p>

<h2>Why etcd?</h2>

<p>We made the decision to use <a href="https://github.com/coreos/etcd">etcd</a>. Etcd is &ldquo;a highly-available key value store for shared configuration&rdquo; (<a href="https://github.com/coreos/etcd#etcd">https://github.com/coreos/etcd#etcd</a>). It provides a HTTP API to store and retrieve data. This is what makes it perfect for a feature toggling solution used by multiple components. It means that we didn&rsquo;t have to write an intermediate API on top of a data store for consumers.</p>

<p>So, for example, to store a feature toggle in etcd:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -L http://127.0.0.1:4001/v2/keys/v1/toggles/restaurant-api/testtoggle -XPUT -d value="true"</span></code></pre></td></tr></table></div></figure>


<p>To retrieve a feature toggle:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>curl -L http://127.0.0.1:4001/v2/keys/v1/toggles/restaurant-api/testtoggle</span></code></pre></td></tr></table></div></figure>


<h2>The Hobknob Clients</h2>

<p>To aid adoption we created, and open sourced, several hobknob clients in multiple languages:</p>

<ul>
<li>NodeJs (NPM) &ndash; <a href="https://github.com/opentable/hobknob-client-nodejs">https://github.com/opentable/hobknob-client-nodejs</a></li>
<li>.NET (Nuget) &ndash; <a href="https://github.com/opentable/hobknob-client-net">https://github.com/opentable/hobknob-client-net</a></li>
<li>Go &ndash; <a href="https://github.com/opentable/hobknob-client-go">https://github.com/opentable/hobknob-client-go</a></li>
<li>Java (Maven) &ndash; <a href="https://github.com/opentable/hobknob-client-java">https://github.com/opentable/hobknob-client-java</a></li>
</ul>


<p>The clients all store a configurable in-memory cache that is periodically updated on a polling interval. They are all read-only and updates only occur on the dashboard where they can be audited.</p>

<p>We decided to create a simple <a href="https://github.com/opentable/hobknob-demo">demo application</a> to show off how easy it is to use Hobknob in your applications. In order to try the demo you will need to start up Hobknob (see instructions below). The demo app uses the NodeJS client which is as simple as:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var client = new Client("hobknob-demo", {
</span><span class='line'>  etcdHost: etcdHost,
</span><span class='line'>  etcdPort: etcdPort,
</span><span class='line'>  cacheIntervalMs: 5000
</span><span class='line'>});</span></code></pre></td></tr></table></div></figure>


<p>In the route definition it uses the client to request the toggle named <em>show-first-and-last-name-input</em> and passes the toggle value through to the view:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>var result = hobknobClient.getOrDefault('show-first-and-last-name-input', true);
</span><span class='line'>res.render('server', {
</span><span class='line'>                  page: 'server',
</span><span class='line'>              useTwoFieldNameInput: value
</span><span class='line'>              });</span></code></pre></td></tr></table></div></figure>


<p>The view then uses the value to decide whether to display one or two textboxes on the page:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>if useTwoFieldNameInput
</span><span class='line'>  input.form-control.demo-input-small(type='text', placeholder='First name', name='firstname')
</span><span class='line'>  input.form-control.demo-input-small(type='text', placeholder='Last name', name='lastname')
</span><span class='line'>else
</span><span class='line'>  input.form-control.demo-input-large(type='text', placeholder='Full name', name='fullname')</span></code></pre></td></tr></table></div></figure>


<h2>The Hobknob Dashboard</h2>

<p>Hobknob is a NodeJS/AngularJS app. If you want to play with Hobknob the simplest way to get started is to use Vagrant. If you don&rsquo;t have it installed then get it from <a href="http://www.vagrantup.com/">http://www.vagrantup.com/</a>.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone https://github.com/opentable/hobknob
</span><span class='line'>cd hobknob
</span><span class='line'>vagrant up</span></code></pre></td></tr></table></div></figure>


<p>You should now be able to open the dashboard on <a href="http://127.0.0.1:3006">http://127.0.0.1:3006</a></p>

<p><img src="/images/posts/hobknob-dashboard.png" alt="Hobknob dashboard" /></p>

<p>All actions in the dashboard are audited. So when you create or update a toggle by turning it on/off an audit is written for that toggle. Clicking on a toggle in the dashboard takes you to the audit view:</p>

<p><img src="/images/posts/hobknob-audit.png" alt="Hobknob audit" /></p>

<h3>Authentication</h3>

<p>By default Hobknob ships with authentication disabled. As a result all auditing will be recorded as &ldquo;Anonymous&rdquo;. Currently, we only support Google OAuth. To enable this follow the instructions <a href="https://github.com/opentable/hobknob/blob/master/README.md#configuring-authentication">here</a></p>

<h3>Session Storage</h3>

<p>By default Hobknob ships using in-memory session storage. You don&rsquo;t want to use this when you have a load balanced infrastructure. Hobknob supports both redis and etcd itself as a session store. To use either of these simply npm install the relevent connect middleware (<a href="https://github.com/visionmedia/connect-redis">connect-redis</a> or <a href="https://github.com/opentable/connect-etcd">connect-etcd</a>). To learn more follow the instructions <a href="https://github.com/opentable/hobknob/blob/master/README.md#configuring-session">here</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-3-testing-roles/">Testing Puppet with Beaker pt.3 - Testing Roles</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-09-01T13:09:05+01:00" pubdate data-updated="true">Sep 1<span>st</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In the first two parts of this blog series we have focusing on testing puppet <em>modules</em> with beaker. As an open source contributor there is always
a large test matrix so this makes absolute sense. But what about the other large use-case for beaker &ndash; what about our day-to-day internal code base?
Not all of this is modules, in fact a large portion of it is other puppet code &ndash; roles, profiles, facts, hiera data etc. All of this needs testing
as well.</p>

<p>In this blog post I will be showing how we have started using beaker to test our puppet roles and profiles for both Linux and Windows.</p>

<h2>Master-vs-Masterless</h2>

<p>Prior to this post all our beaker testing has been master-less i.e. using using puppet agent apply. This is perfectly adequate for most use cases when
testing modules in isolation but doesn&rsquo;t always work when testing an internal code base (unless you are masterless there as well then please skip to the next section).</p>

<p>At OpenTable we do use a central puppet master to compile our catalogs. So when testing our puppet roles we wanted to make sure that we were also testing
with a master-agent configuration. It is worth mentioning here that if (like us) you are testing windows agents then you are going to need to test with master-agent
approach due to the lack of a windows master.</p>

<p>Testing the master-agent configuration means configuring multi-node sets in beaker. There are not many examples of this but the principle is very much
the same as the single-node nodeset. Here is an example:</p>

<pre><code>HOSTS:
  ubuntu-server-12042-x64-master:
    roles:
      - master
    platform: ubuntu-12.04-amd64
    box: ubuntu-server-12042-x64-vbox4210-nocm
    box_url: http://puppet-vagrant-boxes.puppetlabs.com/ubuntu-server-12042-x64-vbox4210-nocm.box
    hypervisor: vagrant
    ip: '10.255.33.135'
  win-2008R2-std:
    roles:
      - default
      - agent
    platform: windows-server-amd64
    box: opentable/win-2008r2-standard-amd64-nocm
    box_version: = 1.0.0
    box_check_update: false
    hypervisor: vagrant
    user: vagrant
    ip: '10.255.33.129'
    communicator: bitvise
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>In this example you will see that we are specifying different &lsquo;roles&rsquo; for each host in the nodeset. What a role is in in this context is a tag for that node that allows
us to reference it directly later when running commands on the host. To avoid any further confusion, from this point onwards if I am referring to the role defined in the
nodeset file I will call it the &lsquo;nodeset role&rsquo; otherwise I am referring the the puppet role provided in the manifest. There are a couple of build-in nodeset roles that
Beaker already knows about: master, agent and default. The first two are pretty self explanatory but the last nodeset role &ndash; default &ndash; is the location where the tests
themselves run. In you don&rsquo;t specify the &lsquo;default&rsquo; nodeset role on any of your host definitions then the tests will run on the first host that you specified in in the
nodeset file (which in the case of the example above would be wrong).</p>

<p>You may have a more complicated configuration that you wish to test and this allows you to specify arbitrary tags which can be very useful.</p>

<p>We can now use these nodeset roles to configure our master and agent.</p>

<p>In parts <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">[1]</a> and <a href="http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-2-the-windows-story">[2]</a> of this series we saw what a basic spec_acceptence file looks like. So let&rsquo;s start with that:</p>

<pre><code>require 'beaker-rspec/spec_helper'
require 'beaker-rspec/helpers/serverspec'
require 'winrm'

hosts.each do |host|

  if host['platform'] =~ /windows/
    include Serverspec::Helper::Windows
    include Serverspec::Helper::WinRM
  end

  version = ENV['PUPPET_VERSION'] || '3.5.1'
  install_puppet(:version =&gt; version)

  if host['roles'].include?('master')

    ... # Install a master

  else

    ... # Install an agent

  end
end

RSpec.configure do |c|

  c.before :suite do

    hosts.each do |host|
      c.host = host

      if host['platform'] =~ /windows/
        endpoint = "http://127.0.0.1:5985/wsman"
        c.winrm = ::WinRM::WinRMWebService.new(endpoint, :ssl, :user =&gt; 'vagrant', :pass =&gt; 'vagrant', :basic_auth_only =&gt; true)
        c.winrm.set_timeout 300
      end
    end
  end
end
</code></pre>

<p>We can see here how we use the host[&lsquo;roles&rsquo;] in order to select the appropriate code-path for configurting each nodeset role. Now let&rsquo;s
move onto how we configure each of those nodeset roles.</p>

<h2>Configuring the master</h2>

<p>There are a lot of things that go into building a puppetmaster:
 &ndash; puppetmaster packages
 &ndash; hiera backends
 &ndash; gems for addditional dependencies (eyaml + puppetdbquery)
 &ndash; downloading external modules</p>

<p>Now let&rsquo;s step through our new spec_acceptence file that supports this multi-node environment:</p>

<h3>Deploying the codebase</h3>

<p>Stage one is getting our puppet codebase onto the master, which includes all the files, internal modules and anything else we need to get the master up and
running. We do this like follows:</p>

<pre><code>files = [ 'environments','facts','hiera','roles', 'profiles', 'keys', 'app_modules', 'auth.conf','autosign.conf',
          'fileserver.conf', 'Gemfile','hiera.yaml','Puppetfile'
        ]

files.each do |file|
  scp_to master, File.expand_path(File.join(File.dirname(__FILE__), '..', file)), "/etc/puppet/#{file}"
end

# scp dist modules folder (this excludes stuff like spec and test folders)
dist_modules = Dir["#{dist_modules_root}/*/"].map { |a| File.basename(a) }
dist_modules.each do |module_name|
  dist_module_dir = "#{dist_modules_root}/#{module_name}"
  copy_module_to(master, :source =&gt; dist_module_dir, :module_name =&gt; module_name)
end
</code></pre>

<p>Here we are selecting all the files that we want and calling the scp_to method which will scp any file or directory to the host of choice, in this case our
master.</p>

<h3>The puppetmaster:</h3>

<pre><code>...

on master, "apt-get install -y rubygems git"
on master, "apt-get install -y puppet-common=#{version}-1puppetlabs1 puppetmaster-common=#{version}-1puppetlabs1 puppetmaster=#{version}-1puppetlabs1 "
on master, "echo '*' &gt; /etc/puppet/autosign.conf"

...
</code></pre>

<p>So we have already installed puppet at a previous stage in our script. At this point we are performing all the steps required to install the
puppetmaster: git, rubygems (if on an older distro) and the puppetmaster packages. We also making sure that we auto-signing if configured to
save us some pain later on. This step should really be configured as another beaker method that we can just call but for now it is still manual.
It is at this point that we have first introduced the &ldquo;on master&rdquo; this does what you think it might, it executes the command you pass it onto
the host with the nodeset role on &lsquo;master&rsquo;.</p>

<h3>Set the puppet.conf file:</h3>

<pre><code>...

config = {
  'main' =&gt; {
    'server'   =&gt; master_name,
    'certname' =&gt; master_name,
    'logdir'   =&gt; '/var/log/puppet',
    'vardir'   =&gt; '/var/lib/puppet',
    'ssldir'   =&gt; '/var/lib/puppet/ssl',
    'rundir'   =&gt; '/var/run/puppet'
  },
  'agent' =&gt; {
    'environment' =&gt; 'vagrant'
  }
}

configure_puppet(master, config)

...
</code></pre>

<p>Here we are configuring out puppet.conf file, making sure that it includes any customization we might need. This uses a configure_puppet method
that we have added to beaker to allow us to do this customization and in this case it is taking the hash to modify the puppet.conf file on the master
host.</p>

<h3>Install the required ruby gems:</h3>

<pre><code>...

on master, "gem install bundler"
on master, "gem install hiera-eyaml"
on master, "cd /etc/puppet &amp;&amp; bundle install --without development"

...
</code></pre>

<p>The average production-ready puppetmaster also requires a number of gems to function such as hiera-eyaml, deep_merge any many others
depending upon what backends and other custom puppet extensions you have implemented. Here we are installing all our dependencies from
the Gemfile we have already put onto the host.</p>

<h3>Installing modules:</h3>

<pre><code>...

on master, "cd /etc/puppet &amp;&amp; bundle exec librarian-puppet install"

...
</code></pre>

<p>The last major step is installing any external modules you have. You may be using librarian-puppet or r10k to do this. In our case it
is the former so we go ahead and make sure that our modules directory is full of all the modules we require.</p>

<h3>Networking:</h3>

<pre><code>...

master_name = "#{master}.test.local"
on master, "echo '10.255.33.135   #{master_name}' &gt;&gt; /etc/hosts"
on master, "hostname #{master_name}"
on master, "/etc/init.d/puppetmaster restart"

...
</code></pre>

<p>This last step is a small hack that you will probably require if you are running on vagrant. It just configures the host file to make
sure that it&rsquo;s hostname if configured correctly from certificate signing to work as expected. This might not be required in your
environment and I would try it without first but it&rsquo;s worth noting anyway.</p>

<h2>Configuring the agent</h2>

<p>So if you&rsquo;ve got to this point well done &ndash; most of the hard work is done. Configuring the agent(s) is pretty straightforward in comparison
to a puppetmaster and some of the steps are similiar:</p>

<h3>Set the puppet.conf file:</h3>

<pre><code>if host['roles'].include?('master')
  ...
else
  agent = host
  master = only_host_with_role(hosts, 'master')
  agent_name = agent.to_s.downcase
  master_fqdn = "#{master}.test.local"
  agent_fqdn = "#{agent_name}.test.local"

  if agent['platform'] =~ /windows/
    config = {
      'main' =&gt; {
        'server'   =&gt; master_fqdn,
        'certname' =&gt; agent_name,
        'logdir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\log',
        'vardir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\lib',
        'ssldir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\lib\\ssl',
        'rundir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\run'
      },
      'agent' =&gt; {
        'environment' =&gt; 'vagrant'
      }
    }
  else
    config = {
      'main' =&gt; {
        'server'   =&gt; master_fqdn,
        'certname' =&gt; agent_fqdn,
        'logdir'   =&gt; '/var/log/puppet',
        'vardir'   =&gt; '/var/lib/puppet',
        'ssldir'   =&gt; '/var/lib/puppet/ssl',
        'rundir'   =&gt; '/var/run/puppet'
      },
      'agent' =&gt; {
        'environment' =&gt; 'vagrant'
      }
    }
  end
  ...

  configure_puppet(agent, config)
end

...
</code></pre>

<p>Again here we are again using the configure_puppet method, this time change the puppe.conf file on the agent.</p>

<p>As you can see we are catering for both windows and linux hosts here. We are also making sure that the certname
and server are defined properly and match what we set-up for the master so that auto-signing works correctly.</p>

<h2>Testing the Role</h2>

<p>At this point we have now does all our prerequisites and we can spin up two machines to test against &ndash; 1 master and
1 agent. But when we are testing a role what is it that we actually want to test and why is this not covered in
earlier less-expensive puppet-rspec unit tests?</p>

<p>Well there are 3 key things that we wanted to test:
1. Idempotence <br/> This is pretty straight forward to test. Beaker provides a method run_agent_on that will run the puppet agent on a
  given host. This means we can test idempotency like this:</p>

<pre><code>   run_agent_on(agent, :catch_failures =&gt; true)
   expect(run_agent_on(agent, :catch_failures =&gt; true).exit_code).to be_zero
</code></pre>

<ol>
<li><p>Interaction of multiple modules and profiles <br/> This is the big motivator &ndash; we want to test that and make sure that the combinations
of profiles that we are applying work together and do not either break the catalog or operate in a non-idempotent way. We are also gaining
the ability to test that updates in modules (many of which are external from the puppet forge) do not break our roles in any way.</p></li>
<li><p>Postivie/Negative testing &ndash; do we clean up after ourselves if we remove something. <br/> This is not something that is often considered
very often, particularly in a world where machines are torn down and re-build so often but there still exists a use-case where this is not
always possible and we want to make sure that our roles and manifests are not littering our machines unnecessarily.</p></li>
</ol>


<p>Below is a full example of one of our linux profiles:</p>

<pre><code>require 'spec_helper_acceptance'

describe 'linux_base_profile', :if =&gt; fact('osfamily').eql?('Debian') do
  context 'linux base profile' do
    it 'should should run successfully' do

      agent = only_host_with_role(hosts, 'agent')
      master = only_host_with_role(hosts, 'master')

      pp = "node \"#{agent}\" { include profiles::linux::base }"
      on master, "echo '#{pp}' &gt;&gt; /etc/puppet/manifests/site.pp"

      run_agent_on(agent, :catch_failures =&gt; true)
      expect(run_agent_on(agent, :catch_failures =&gt; true).exit_code).to be_zero
    end

    context 'installation of ops tools' do

      describe package('sysstat') do
        it { should be_installed }
      end

      describe package('iotop') do
        it { should be_installed }
      end

      describe package('ngrep') do
        it { should be_installed }
      end

      describe package('lsof') do
        it { should be_installed }
      end

      describe package('unzip') do
        it { should be_installed }
      end
    end

    context 'managing puppet version' do

      describe file('/etc/apt/sources.list.d/puppetlabs.list') do
        it { should be_file }
        it { should be_owned_by 'root' }
        it { should be_mode 644 }
      end

      describe package('puppet') do
        it { should be_installed.by('apt').with_version('3.6.1-1puppetlabs1') }
      end
    end

    context 'manage sshd configuration' do

      describe process("sshd") do
        it { should be_running }
      end

      describe port(22) do
        it { should be_listening }
      end

      describe file('/etc/ssh/sshd_config') do
        its(:content) { should match /PermitRootLogin no/ }
        its(:content) { should match /PasswordAuthentication yes/ }
        its(:content) { should match /UseDNS no/ }
      end
    end
  end
end
</code></pre>

<p>We have a lot of roles and profiles that we would like to test. As you might imagine this could get quite verbose and repetitive pretty quickly. We are currently building
up <a href="https://www.relishapp.com/rspec/rspec-core/docs/example-groups/shared-context">shared_contexts</a> for each of our profiles which we can
then wrap up into roles to easily reflect our roles/profiles structure in the main codebase.</p>

<h2>Summary</h2>

<p>We are just at the very beginning of this journey with Beaker. As well as testing all our modules we are looking to scale our to test the roles and profiles in our whole
code base. These examples here are how we are doing it at the moment for our mixed-platform environment. We will continue to expand upon it and build it into our pipeline.
At this moment we are looking to expand beyond vagrant and run these against AWS instances but perhaps that is for the next post &hellip;</p>

<p>As usual for any questions or comments then please reach out to me on twitter <a href="https://twitter.com/liamjbennett">@liamjbennett</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-2-the-windows-story/">Testing Puppet with Beaker pt.2 - The Windows story</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-09-01T09:43:10+01:00" pubdate data-updated="true">Sep 1<span>st</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">part one</a> we discussed our first steps into the world of acceptance testing our Puppet manifests.
By using Beaker we able to test managing local users on our Linux boxes. This was a positive experience for us. It allowed us to get to grips with the basics of configuring
Beaker to run tests and configuring our node sets to run those tests against. In this post, we will be discussing how we went about getting Beaker working with Windows.</p>

<p>As many of your reading this will be aware, OpenTable currently has quite a large Windows infrastructure and we are using Puppet extensively to maintain that environment.
We are also moving forward with releasing as many of our modules open source onto the <a href="https://forge.puppetlabs.com/opentable">Puppet Forge</a> as possible (11 out of 18 of which are Windows
exclusive). What this means is that there was no way that we could ignore trying to use Beaker to test our manifests against Windows. We knew that we would have to support
many different versions and editions of Windows out there in the community as well that the ones we have to support internally.</p>

<p>This was going to be a challenge (configuration management with Windows usually is) but we were up for it.</p>

<h2>The Preliminaries</h2>

<h3>Serverspec</h3>

<p>The first step was looking at Serverspec. Serverspec is a Ruby gem that provides extensions to RSpec that allow you to test the actual state of your servers, either locally
or from the outside in via SSH. What we needed to know was did it support Windows? The answer was thankfully a resounding &ldquo;Yes!&rdquo;. All the <a href="http://serverspec.org/resource_types.html">resource types</a> that you might want to test including file, service and user are available and supported on Windows. There are also a couple of Windows specific ones such as iis_website, Windows_feature and Windows_registry_key. We even added our own to support <a href="https://github.com/serverspec/serverspec/pull/403/files">Windows_scheduled_task</a>. Interestingly Serverspec also supports WinRM as an alternative to SSH when you are testing from the outside-in but we will go back into that later. As long as your using Serverspec > 1.6 you will have all the Windows support you might need.</p>

<h3>Packer</h3>

<p>Step two was to build some Windows Vagrant boxes to test against. The documentation on the <a href="http://github.com/puppetlabs/beaker/wiki">wiki</a> was (at the time) a bit slim when it came
to building test boxes but we knew we needed Cygwin so we went ahead and created the boxes that we needed. All our boxes are created with Packer <a href="http://github.com/opentable/packer-images">and are open sourced on GitHub</a>. They have also been <a href="http://vagrantcloud.com/opentable">published to Vagrant Cloud</a>
so you can download pre-built images and get up and running quickly (version 1.x images contain the Cygwin installation).</p>

<h3>Beaker</h3>

<p>So far, so good. We hit a couple of issues in our initial test runs with Beaker: missing module_path, installation using the msi and 32-bit Windows support &ndash; but these were very
small issues and we were happy to be able to contribute back some changes (<a href="https://github.com/puppetlabs/beaker/pull/234">234</a>,
<a href="https://github.com/puppetlabs/beaker/pull/235">235</a>, <a href="https://github.com/puppetlabs/beaker/pull/236">236</a>). We were very happy and managed to get out first module tested,
the <a href="http://github.com/opentable/puppet-puppetversion/">cross-platform module puppet-puppetversion</a> for doing Puppet upgrades.</p>

<h2>The First Example</h2>

<p>Let&rsquo;s take a more detailed look at those puppetversion tests, how we configured Beaker to run and how it changed for the Windows support. I am going to assume at this point that
you already have some familiarity with Beaker; if not and this is your first steps into the testing tool then I would suggest going back and read <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">part one</a> of this series which contains a little bit of background to this and some useful resources for getting started.</p>

<p>The first thing that we needed to change for Windows was our <a href="https://raw.githubusercontent.com/opentable/puppet-puppetversion/master/spec/spec_helper_acceptance.rb">spec_accepentance.rb file</a>.</p>

<p>Step one was to include the appropriate Serverspec helpers. What this does is let Serverspec know that we are executing on Windows so that underlying resources work correctly. We are also telling
server spec here to communicate using WinRM.</p>

<pre><code>require 'beaker-rspec/spec_helper'
require 'beaker-rspec/helpers/serverspec'
require 'winrm'

hosts.each do |host|
  case host['platform']
    when /windows/
      include Serverspec::Helper::Windows
      include Serverspec::Helper::WinRM

      version = ENV['PUPPET_VERSION'] || '3.4.3'

      install_puppet(:version =&gt; version)

  else
    install_puppet
  end
end

...
</code></pre>

<p>The next step is to configure WinRM so that it can connect properly. In our case this meant connecting to Vagrant boxes.</p>

<pre><code>...

RSpec.configure do |c|
  ...
  hosts.each do |host|
    c.host = host

    if host['platform'] =~ /windows/
      endpoint = "http://127.0.0.1:5985/wsman"
      c.winrm = ::WinRM::WinRMWebService.new(endpoint, :ssl, :user =&gt; 'vagrant', :pass =&gt; 'vagrant', :basic_auth_only =&gt; true)
      c.winrm.set_timeout 300
    end

    ...
  end
end

...
</code></pre>

<p>Now let&rsquo;s look at one of the tests themselves.</p>

<p>The first part should look pretty familiar. We use the face(&lsquo;osfamily&rsquo;) helper in Beaker to make sure that the test itself is only ever executed for our Windows hosts. We are then running an apply_manifest two times in order to validate that the manifest is idempotent. The only different here is that we are specifying a custom Windows-specific module_path.</p>

<pre><code>...
require 'spec_helper_acceptance'

describe 'puppetversion', :unless =&gt; UNSUPPORTED_PLATFORMS.include?(fact('osfamily')) do
  ...

  context 'upgrade on windows', :if =&gt; fact('osfamily').eql?('windows') do

  it 'should install the new version' do
    pp = &lt;&lt;-PP
      class { 'puppetversion':
        version =&gt; '3.5.1',
        time_delay =&gt; 1
      }
    PP

    apply_manifest(pp, :modulepath =&gt; 'C:/ProgramData/PuppetLabs/puppet/etc/modules', :catch_failures =&gt; true)
    expect(apply_manifest(pp, :modulepath =&gt; 'C:/ProgramData/PuppetLabs/puppet/etc/modules', :catch_failures =&gt; true).exit_code).to be_zero
  end
</code></pre>

<p>The next part is where we actually perform the bulk of the tests. In the case of this module, we are testing
that the scheduled task has run and that Puppet has been upgraded to the appropriate version. We are making use
of the Windows_scheduled_task resource that we created earlier:</p>

<pre><code>  describe Windows_scheduled_task('puppet upgrade task') do
    it { should exist }
  end

  #This will fail if your laptop (and therefor the Vagrant vm) is not running on AC power
  describe package('puppet') do
    it {
      sleep(240) #Wait for the task to execute
      should be_installed.with_version('3.5.1')
    }
  end

  describe Windows_scheduled_task('puppet upgrade task') do
    it {
      pp = &lt;&lt;-PP
        class { 'puppetversion':
          version =&gt; '3.5.1'
        }
      PP

      apply_manifest(pp, :modulepath =&gt; 'C:/ProgramData/PuppetLabs/puppet/etc/modules', :catch_failures =&gt; true)

      should_not exist
    }
  end
end
</code></pre>

<p>The final part was to configure the nodeset file for the Windows box that we wanted to run our test against:</p>

<pre><code>HOSTS:
  windows-2008R2-serverstandard-x64:
  roles:
    - agent
  platform: windows-server-amd64
  box : opentable/win-2008r2-standard-amd64-nocm
  box_url : opentable/win-2008r2-standard-amd64-nocm
  hypervisor : vagrant
  user: vagrant
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>This was working well for us so we continued on to our next module.</p>

<p>The next module we chose to look at was <a href="http://github.com/opentable/puppet-Windowsfeature">puppet-Windowsfeature</a>.</p>

<p>The test we have implemented in this moudle looks like this:</p>

<pre><code>require 'spec_helper_acceptance'

describe 'Windowsfeature' do
  context 'windows feature should be installed' do
    it 'should install .net 3.5 feature' do

      pp = &lt;&lt;-PP
        Windowsfeature { 'as-net-framework': }
      PP

      apply_manifest(pp, :catch_failures =&gt; true)
      expect(apply_manifest(pp, :catch_failures =&gt; true).exit_code).to be_zero
    end

    describe Windows_feature('as-net-framework') do
      it { should be_installed.by('powershell') }
    end
  end
end
</code></pre>

<p>This module was a little more tricky. Why? Installing Windows features requires elevated permissions. What this meant is that when Beaker attempted to SSH into our Windows box and our Puppet module ran its underlying PowerShell we were faced with a harsh and non-descriptive <strong>&ldquo;Access is denied error&rdquo;</strong>.</p>

<h2>SSH</h2>

<p>Not all SSH daemons are created equal. To understand the &ldquo;Access is denied error&rdquo; we were seeing and why it happens you need to understand a little bit about how sshd on Cygwin works. You can read all about the details from the Cygwin forum archives (<a href="http://cygwin.com/ml/cygwin/2004-09/msg00087.html">[1]</a>, <a href="http://cygwin.com/ml/cygwin/2006-06/msg00862.html">[2]</a>, <a href="http://thread.gmane.org/gmane.os.cygwin/128472">[3]</a>, <a href="https://cygwin.com/cygwin-ug-net/ntsec.html#ntsec-nopasswd1">[4]</a>) but TLDR; is that you need to use a username and password rather than private key authentication in order to get reasonable admin permissions. Having said all that, and having read all the documentation about the issue above we were still facing the same problem so we had to look at alternative options.</p>

<p>There are several paths you can go down and I want to tell you about them here to save you a similar yak shave:</p>

<h3>OpenSSH for Windows</h3>

<p>A thinner alternative than having to install the full Cygwin stack using OpenSSH for Windows is the same OpenSSH implementation. The issue here however is that it doesn’t contain some of the Unix binaries required for Beaker to function. You can work around this if you have Git for Windows installed on your machine by putting its bin directory on your path but overall this doesn’t really solve any of the issues we were facing. We get a lighter footprint on the machine but still the same error as before</p>

<h3>Bitvise SSH Server</h3>

<p><a href="http://www.bitvise.com/ssh-server-download">Bitvise SSH Server</a> is an alternative SSH implementation (of which there are many more listed on <a href="http://en.wikipedia.org/wiki/Comparison_of_SSH_servers">Wikipedia</a>). It resolves the permissions issue (it deals with the elevation internally) and has the benefit that it provides a proper command shell rather than a emulated bash shell. It also means we don’t have to have any binaries on there that we don’t need &ndash; a big plus. It would mean that we needed to make a few small changes to Beaker in order to replace some of the internal bash command with Windows alternatives but this was not a big task to do and is something we could contribute back.</p>

<h3>WinRM</h3>

<p>Could we do away with SSH altogether? It eliminates the problem we were facing and also means we don’t have to install anything on our Windows boxes &ndash; it’s all built in already. This would be an ideal solution but does all our tooling support it? I’ll discuss this in a little bit more detail later.</p>

<h3>Not use Beaker at all</h3>

<p>The nuclear option. If we couldn’t get anything to work we could not use Beaker at all, we could try and use Test Kitchen (with <a href="https://github.com/neillturner/kitchen-puppet">test-kitchen-puppet</a>) or some hand-rolled solution. Not the best idea, for us or the community but we though we might have to go down this path at one point. We even added <a href="https://github.com/liamjbennett/kitchen-puppet/tree/Windows_support">Windows support to test-kitchen-puppet</a> as part our diversion in this direction.</p>

<h2>What worked in the end:</h2>

<p>So we went down all these avenues and decided that the best option for us was to use Bitvise. It fixed the problem we were facing but it meant that we had some work ahead of us:
1. We had to rebuild all our Windows images with Bitvise rather than Cygwin (<a href="https://vagrantcloud.com/opentable">vagrantcloud.com/opentable</a> &ndash; version 2.x images now have this)
2. We had to make some changes to Beaker to support using a standard Windows command shell rather than a Unix shell:</p>

<ul>
<li> <a href="https://github.com/puppetlabs/beaker/pull/419">https://github.com/puppetlabs/beaker/pull/419</a> – configure_puppet method</li>
<li> <a href="https://github.com/puppetlabs/beaker/pull/420">https://github.com/puppetlabs/beaker/pull/419</a> – host_entry method</li>
<li> <a href="https://github.com/puppetlabs/beaker/pull/418">https://github.com/puppetlabs/beaker/pull/419</a> – Vagrant box_version</li>
<li> <a href="https://github.com/puppetlabs/beaker/pull/424">https://github.com/puppetlabs/beaker/pull/419</a> – Bitvise SSH</li>
</ul>


<p>With it finally working we had something that looked like this:</p>

<h2>Second Example</h2>

<p>Well with all the changes we implemented there were actually very few changes that we needed to make to our actual tests code.</p>

<p>No adjustments are needed for our spec_acceptance.rb file.</p>

<p>No adjustments are required for our spec file (show above).</p>

<p>The main change we made was in the nodeset file:</p>

<pre><code>HOSTS:
  win-2008R2-std:
  roles:
    - default
    - agent
  platform: Windows-server-amd64
  box: opentable/win-2008r2-standard-amd64-nocm
  hypervisor: vagrant
  user: vagrant
  ip: '10.255.33.129'
  communicator: bitvise
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>The biggest change you will see here is the addition of the &lsquo;communicator&rsquo; option. What this does is to allows us to actively select to use either Cygwin or Bitvise. This means that in our case we want to use Bitvise SSH as this fixes the error we were seeing and it&rsquo;s the version of SSH now installed on your newer Vagrant boxes. Bitvise is the only supported option at the moment (in our Beaker fork) but it is likely that this will soon support WinRM as well.</p>

<p>Things to note here:
* The name of the Windows host defined in the node set must be the same as the name of the Windows hostname &ndash; if it is not then when Vagrant boots up it will change the hostname
which will put Windows into a &ldquo;restart pending&rdquo; state.</p>

<h2>The Future and WinRM</h2>

<p>Most modern versions of Windows server have WinRM enabled by default but if you are using an older version or you are attempting to test a client then you will need to make sure
that this is enabled on your boxes. This is still the direction that we would like to go long-term as it is the most Windows-friendly approach but there are few road blocks in
the way of doing so right now:</p>

<ol>
<li><p>Packer doesn’t set support WinRM as a communication method. This is being actively worked on and you can follow the work here:</p>

<ul>
<li><a href="https://github.com/mitchellh/packer/issues/451">https://github.com/mitchellh/packer/issues/451</a></li>
<li><a href="https://github.com/dylanmei/packer-communicator-winrm">https://github.com/dylanmei/packer-communicator-winrm</a></li>
</ul>
</li>
<li><p>Beaker doesn’t yet support WinRM as a communication protocol. This is currently being discussed internally after we raised the idea. The work that we have completed for
Bitvise support will go some way it allowing other providers, such as wirm going forward and therefore WinRM support should be coming in the near future.</p></li>
</ol>


<h2>Summary</h2>

<p>Using Beaker to test modules for Windows has been a long and complicated journey. I have attempted to cover here all the problems that you might run into when trying to do this for yourselves and provide some good examples to get you going. You will soon see this being rolled out to all of the OpenTable open source modules shortly so you will have some complete working examples to reference. We will continue to work with PuppetLabs in improving Beaker (and its Windows support) in order to make this a easy process for everyone.</p>

<p>For any questions or comments then please reach out to me on twitter <a href="https://twitter.com/liamjbennett">@liamjbennett</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/08/what-can-i-do/">What can I do?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-08T13:52:30+01:00" pubdate data-updated="true">Aug 8<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I&rsquo;ve noticed recently a lot of content in my social media network is based on the current escalating problems in the Middle East.</p>

<p>Whenever I see such content from a friend or follower, I&rsquo;m reminded of Stephen Covey&rsquo;s <a href="http://www.amazon.co.uk/gp/product/B00GOZV3TM/ref=as_li_tl?ie=UTF8&amp;camp=1634&amp;creative=19450&amp;creativeASIN=B00GOZV3TM&amp;linkCode=as2&amp;tag=helenpieli-21&amp;linkId=RTIUTRBYIE6APBJA">The 7 Habits of Highly Effective People</a> &ndash; Circle of Influence, Circle of Concern.</p>

<p>Covey says that in order to remain truly effective, we should focus our time and energy on situations we can influence. Don&rsquo;t worry about things we cannot control, such as the weather, the economy or indeed foreign conflicts.</p>

<p>Covey&rsquo;s work implies that if we identify the areas that are inside our Circle of Influence we will be more productive and make more of an impact;</p>

<p><img src="http://www.chowamigo.co.uk/images/what_can_i_do.png" alt="image" /></p>

<p>If I&rsquo;m honest I find this work quite harsh in its approach. Indeed it might not sit well with everyone. However, I personally found myself much less burdened once I could identify which problems I had control over.</p>

<h2>Expand your influence</h2>

<p>I still find myself watching the news, reading press reports and shaking my head at the global problems we&rsquo;ve yet to solve. Covey&rsquo;s stance is a tough one to take &ndash; don&rsquo;t waste energy on X, your time and resources are better utilised by focusing on what you can change.</p>

<p>Some examples:</p>

<ul>
<li><p>I&rsquo;m concerned about global warming => recycle waste as best you can, be energy efficient with your devices, insulate your home more effectively.</p></li>
<li><p>I&rsquo;m concerned about my finances => learn new skills, look for a better role, try to control your spending, reduce unnecessary outgoings.</p></li>
<li><p>I&rsquo;m concerned about my health => get a check up, start to eat healthily, take regular exercise, reduce tobacco and alcohol intake.</p></li>
</ul>


<p>What I like about Covey&rsquo;s advice is that it&rsquo;s sending a clear message &ndash; are we able to make the sun shine at weekends? Can we solve the UK economic crisis by next month? Is there really anything we can do to bring about peace in the Middle East? The likely answer for most of us is no. We  simply have no control on areas outside of our power.</p>

<p>But if you step back into your circle of influence and be pro-active, you can make a real difference.</p>

<h2>Further reading</h2>

<p><a href="http://www.ndoherty.com/circle-of-influence-circle-of-concern">http://www.ndoherty.com/circle-of-influence-circle-of-concern</a></p>

<p><a href="http://www.upcyclededucation.com/2012/09/circle-of-concern-circle-of-influence.html">http://www.upcyclededucation.com/2012/09/circle-of-concern-circle-of-influence.html</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/20/acceptance-now/">Acceptance Now</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-05-20T15:00:00+01:00" pubdate data-updated="true">May 20<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><em>When is acceptance-only testing a good idea, and how can its problems be overcome?</em></p>

<p>In <a href="/blog/2014/04/16/look-ma-no-unit-tests">a recent post</a>, I espoused some of the benefits my team enjoyed by reducing our test-base to a single layer of acceptance tests, with no separate unit or integration tests. It caused <a href="http://www.reddit.com/r/programming/comments/237fr1/look_ma_no_unit_tests/">some minor controversy</a>, which was not to be unexpected. At the time, I knew I had left out some details for brevity&rsquo;s sake. In this post—spurred on by some interesting <a href="https://twitter.com/NathanGloyn/status/456756552092098561">questions</a> and <a href="http://www.reddit.com/r/programming/comments/237fr1/look_ma_no_unit_tests/cgujuv7">commentary</a>—I&rsquo;d like to offer a more constructive view on the subject, and dig a little deeper into the nitty gritty of how we made it work.</p>

<p>I&rsquo;ll also point out some other hidden benefits of moving to acceptance-only testing, and suggest synergistic practices that can help decide if this is the right approach for your project.</p>

<h2>Seams</h2>

<p>After-the-fact unit testing requires us to <a href="https://www.youtube.com/watch?v=wEhu57pih5w">find the seams</a> along which code can be isolated and tested. Where those seams don&rsquo;t exist,  the temptation is to refactor code until they do, using patterns like <a href="http://martinfowler.com/articles/injection.html">dependency injection</a>, and following <a href="http://en.wikipedia.org/wiki/Single_responsibility_principle">SRP</a> and other <a href="http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod">SOLID</a> patterns.</p>

<p><a href="http://en.wikipedia.org/wiki/Test-driven_development">Test-first unit testing</a> aka TDD naturally tends to maximise seams, and results in highly decoupled code with small, specific tests. TDD must result in 100% unit test coverage, if practiced according to <a href="http://en.wikipedia.org/wiki/Test-Driven_Development_by_Example">the gospel</a>. In other words, TDD results in the best kind of code for later <a href="http://sourcemaking.com/refactoring">modification without risk</a>, and the best tests for detailed, granular feedback. Unit tests also tend to run very quickly, sometimes fast enough to <a href="http://misko.hevery.com/2009/05/07/configure-your-ide-to-run-your-tests-automatically/">run every time you hit &ldquo;Save&rdquo;</a>.</p>

<p><strong>Acceptance testing has far fewer seams available.</strong> Typically only one: the application boundary. Yes: <em>Your tests must invoke the entire application each time they are run.</em> Thus running acceptance tests is potentially very slow for even moderately sized projects with few dependencies—let alone large ones with many. Another problem is that, when an acceptance test fails, the failure could be at <em>any</em> layer in the stack. You immediately lose the pinpoint specificity afforded by unit tests.</p>

<p>Why, then, is it sometimes a good idea to forgo unit- in favour of acceptance-tests?</p>

<p>As we will see, the first issue, performance, can be mitigated. The second, granularity, is more difficult to overcome. But sometimes <em>that might be okay.</em></p>

<h2>Performance</h2>

<p>In most cases I have seen, acceptance tests are <em>really</em> slow. In some cases, there might be nothing you can do, but usually there is. Front-end automation suites using <a href="http://docs.seleniumhq.org/">Selenium</a> are temporally incorrigible, but they can be parallelised. Complex transactional pieces might be irreducible, but they can be helped with mock data. No matter which box your code is in, there is probably an escape route. Following are some of the techniques we used to overcome performance bottlenecks in our acceptance test suite&hellip;</p>

<h3>Sandbox Data</h3>

<p><em>This is probably a topic that deserves its own post, but I&rsquo;ll try to give a high-level treatment here.</em>
In our project, we took on the overhead of providing mock &ldquo;sandbox&rdquo; data. For our consumers this was a required feature anyway, so implementing it began early in the project, well before we <a href="/blog/2014/04/16/look-ma-no-unit-tests">deleted all the unit tests</a>. It turned out this was an important enabler in moving to acceptance-only, since it allowed us to run tests much faster by <em>sometimes</em> circumventing data access.</p>

<p>Since this project was written in C# using strict TDD, our data layer already had <a href="http://en.wikipedia.org/wiki/Interface_(computer_science)#Software_interfaces_in_object-oriented_languages">an interface</a> for each data source. In the <em>unit</em> tests this allowed us to easily stub out the data. We reused the same interfaces to build up our sandbox, with dependency injection at runtime to choose between real and mock data. (I like to call this a &lsquo;pseudoseam&rsquo; in that it allows us to isolate data access at runtime, just like an ordinary seam allows you to isolate classes and methods in test.)</p>

<p><em>Sandbox data is hard to implement. A lot of the effort that would have gone into unit tests and their maintenance was instead pumped into writing good, wholesome, fake data for the sandbox.</em> However, sandbox data, unlike unit tests, solves three problems at once:</p>

<ul>
<li>It lets your <em>consumers</em> test in a predictable way without making real transactions;</li>
<li>It enables you to record specific data conditions from The Real World™, increasing your understanding of that data;</li>
<li>It lets you test internal business logic independently from real data, fast.</li>
</ul>


<p><em>Snip!</em> I went into too much detail on sandbox data here, saving that for a future post.</p>

<h3>Loosen Isolation</h3>

<p>Isolation between test runs is really important. If the order you run tests in can ever alter the results, then you have shared state, and you can no longer trust that your tests are testing the same thing each time they are run.</p>

<p>Usually, in unit testing, we rely on the test runner to respect directives in code that enforce isolation in this way. In NUnit with C# we use attributes for <a href="http://www.nunit.org/index.php?p=setup&amp;r=2.2.10">set-up</a> and <a href="http://www.nunit.org/index.php?p=teardown&amp;r=2.2.10">tear-down</a>, for example. We usually throw away the entire object graph before each test. <em>Sometimes before each assertion.</em> For acceptance testing, where spinning up your test subject tends to take longer, it can be helpful to bend the rules somewhat.</p>

<p>In an ideal world, each test run would begin on a new, freshly installed OS, thus eliminating any possibility of differing test results due to environmental issues—the system clock and network state notwithstanding. For unit testing we rarely if ever take this extremist approach. More usual is to rely on the test runner to enforce &ldquo;similar enough&rdquo; initial conditions each time a test is run–commonly relying on the developer to write this set-up and tear-down code correctly.</p>

<p>In acceptance testing it can be very beneficial for performance to loosen this one step further and re-use the same application instance (process) between test runs. Sandboxed data, especially if it is immutable, can help enormously to avoid the pitfalls of shared state. When running your acceptance tests against real data, where shared state is a real concern, you may discover interesting bugs that would otherwise have gone unnoticed if you were using only unit tests. <strong>Upon the discovery of issues with real data, you must implement the same failing condition in your sandbox data so that you don&rsquo;t accidentally introduce regressions later.</strong></p>

<p>The way we initially achieved this in our acceptance tests was by using the <code>TestFixtureSetup</code> attribute from NUnit to invoke the application, and run it to a point where it had generated an interesting result. Then, each &lsquo;test&rsquo; is in fact a single assertion on the state of the world at that point. Like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='csharp'><span class='line'><span class="na">[TestFixture]</span>
</span><span class='line'><span class="k">public</span> <span class="k">class</span> <span class="nc">my_acceptance_tests</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'><span class="na">    [TestFixtureSetUp]</span>
</span><span class='line'>    <span class="k">public</span> <span class="k">void</span> <span class="nf">do_stuff</span><span class="p">()</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="n">_result</span> <span class="p">=</span> <span class="n">InvokeTheApplicationsWithSomeGivenParams</span><span class="p">();</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'><span class="na">    [Test]</span>
</span><span class='line'>    <span class="k">public</span> <span class="k">void</span> <span class="nf">it_is_cool</span><span class="p">()</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="n">Assert</span><span class="p">.</span><span class="n">That</span><span class="p">(</span><span class="n">_result</span><span class="p">.</span><span class="n">Coolness</span><span class="p">,</span> <span class="n">Is</span><span class="p">.</span><span class="n">GreaterThan</span><span class="p">(</span><span class="m">1337</span><span class="p">));</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'><span class="na">    [Test]</span>
</span><span class='line'>    <span class="k">public</span> <span class="k">void</span> <span class="nf">it_has_2_bananas</span><span class="p">()</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="n">Assert</span><span class="p">.</span><span class="n">That</span><span class="p">(</span><span class="n">_result</span><span class="p">.</span><span class="n">Bananas</span><span class="p">,</span> <span class="n">Is</span><span class="p">.</span><span class="n">EqualTo</span><span class="p">(</span><span class="m">2</span><span class="p">));</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="p">.</span>
</span><span class='line'>    <span class="p">.</span>
</span><span class='line'>    <span class="p">.</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>We eventually refined this to use constructors to set up the initial state, and did some other not-necessarily-normal things to coax our acceptance tests into a reasonably elegant suite. More on this in an upcoming post on sandbox data.</p>

<h2>Granularity</h2>

<p>Granularity, in this case, refers to the level of detail revealed by a failing test. When a <em>unit</em> test fails, if it&rsquo;s written correctly, you should immediately know which method in which class went wrong. Often, the call stack in the exception message will tell you exactly which line of code was at fault. You can immediately jump to the offending code.</p>

<p>With acceptance tests, when something goes wrong, it could be anywhere in your program. Any of the tens, hundreds, thousands, or even millions of lines of code in your program could be at fault. This is clearly less than ideal, however there are ways to mitigate the pain:</p>

<h3>Minimise Code, Maximise Seams</h3>

<p>Clearly, the fewer lines of code in your app, the easier it will be to hunt down obscure test failures. However some problems are too big to be solved with few lines of code. What then to do? One answer, which we are beginning to explore in a new project, may be to write many small programs, each of which solves only a small part of the problem domain. This approach is known as <a href="http://martinfowler.com/articles/microservices.html">microservices</a>, and certainly has its own complexities in threading together many small pieces at OS or network level. However, a microservices architecture has additional potential benefits tangential to its affinity with acceptance-only testing. I&rsquo;m planning a blog post on this subject soon, once we have more data.</p>

<h3>Debugfu</h3>

<p>This isn&rsquo;t really a way to make your tests more granular, but it can help mitigate the problems of low granularity in acceptance tests. If the test won&rsquo;t tell you which line of code went wrong, attach a debugger to find out! If you&rsquo;re using Visual Studio, you are blessed with a best-of-breed debugger. Use it, trace through the execution and try to spot what went wrong. Use bookmarks and breakpoints to index your code. Does one area of code cause problems time and time again? There is probably something wrong with it, see if it can be rewritten more clearly. Users of  IntelliJ IDEA, Eclipse, or a myriad other IDEs, will also have access to usable debuggers.</p>

<h3>Sandbox Data (Again)</h3>

<p>Sandbox data allows you to run your tests against very specific data conditions. Often your code will have a different execution path depending on data, and this is really valuable knowledge when trying to nail down the cause of a test failure. Sandbox data, that can be selected by your tests, will improve the percieved granularity of such, by limiting the potential execution paths.</p>

<h2>Benefits</h2>

<p>I mentioned a few of the benefits of moving to acceptance-only testing in my last post. However, a few more have come to light since then which are worth mentioning.</p>

<h3>Acceptance Tests Are Language-Agnostic</h3>

<p>After writing v1 of our API, and acceptance-testing the living daylights out of it, we realised that we were still probably maintaining too much code, this time in the application itself. We decided to port the whole thing to JavaScript using Node to see what it would be like.</p>

<p>It worked, and <em>we didn&rsquo;t have to touch a single test,</em> even though those tests were written in C#, and the application was in JavaScript.</p>

<p>Just imagine the overhead of porting hundreds of unit tests over to a different language, along with the application. If that had been a requirement of our experiment, it would not have happened, and we would not have learned what we did. <em>(In the end we did keep using the C# implementation in production, but the speedy rewrite was still a valuable learning aid.)</em></p>

<h3>Acceptance Tests Behave Exactly Like Your Users</h3>

<p>When an acceptance-level test passes, you can be confident that a whole user journey using your application is working correctly. That&rsquo;s a huge win. When one fails, you can be pretty confident that something important to your users is not working properly and needs attention. Also, very valuable knowledge. This contrasts somewhat with unit-level tests that might tell you something internal is awry with your application, but its real impact to consumers will still often be unknown. Should you fix it? If there are multiple failures, which are the most important? Unit tests will rarely answer these questions for you.</p>

<p>Of course, you will fix it, or else be unable to confidently release your software–but surely, at times, you will be fixing something that does not matter, or is no longer relevant to your consumers. Unit tests in this way can encourage code rot, making it very difficult to unpick dependencies that are no longer needed. With acceptance tests, you only need to unpick the dependencies in your application, not also in the tests.</p>

<h2>Synergy</h2>

<p>Much of this is new to me, and certainly isn&rsquo;t without contention. However, the problems with acceptance-only testing, and specifically the solutions to those problems, indicate certain synergistic practices that may improve its viability:</p>

<h3>Thin Layers</h3>

<p>The project we first tried moving to acceptance-only testing on was a very thin layer–a facade over a collection of internal services. It had minimal business logic, and thus few potential execution paths. This certainly allowed us to keep the number of acceptance tests lower than might be expected for a large, complex application, that might branch off into numerous modes of operation. Of course one should probably try to minimise the <a href="http://en.wikipedia.org/wiki/Cyclomatic_complexity">cyclomatic complexity</a> of a code-base anyway, for one&rsquo;s own sanity.</p>

<h3>Statelessness</h3>

<p>If your system maintains a lot of state, acceptance testing can be much harder. This is because you will likely have to set-up much of that state for each test, increasing both developer effort and execution time. Both of which are bad. However, my new favourite thing, immutable sandbox data, may well be your friend in this case.</p>

<h3>Microservices</h3>

<p>We have only just begun experimenting with microservices ourselves, however I think it stands to reason that if each application is small overall, then the number of test cases for each will be small as well. This means the whole suite will run faster, and give you more granular feedback. I differentiate microservices from &lsquo;thin layers&rsquo; in that a microservice may well do data access, input parsing, validation, HTTP handling, and a bit of business logic–but over a very narrow domain–i.e. a thin vertical. A thin layer, on the other hand will perform only one kind of function–e.g. HTTP handling–but across multiple facets of the system. If thin layers are the lines of latitude, then microservices can be sections of the lines of longitude.</p>

<h2>Too Short; Read Also</h2>

<p>I hope this article has been a little more useful than the previous one. I have tried to explain more specifically what we actually did, from end-to-end, and how we overcame problems along the way. However, I&rsquo;ve really only scratched the surface. I will hopefully get the chance flesh out some of the ideas here in the coming months. In the mean time, there are plenty of <a href="http://www.shino.de/2012/07/02/atdd-by-example/">books</a> and <a href="http://jonkruger.com/blog/2012/02/20/when-acceptance-tests-are-better-than-unit-tests/">blog posts</a> on the subject of acceptance testing. In addition, Martin Fowler has writen <a href="http://martinfowler.com/articles/microservices.html">a great primer on microservices</a> that&rsquo;s really got me thinking about their utility alongside acceptance-only testing and sandbox data.</p>

<p>Thanks for reading :)</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/19/continuous-delivery-automating-deployment-visibility/">Continuous Delivery: Automating Deployment Visibility</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-05-19T17:17:40+01:00" pubdate data-updated="true">May 19<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In our continued effort to drive towards a service oriented architecture each of our teams are continuously improving their deployment processes. Recently our team has focussed on automating as much as possible, putting as much into chat as we can and improving our logging/metrics.</p>

<p>The image below shows at a high level what our teams current deployment pipeline looks like and this post will attempt to summarise some recent changes that have allowed us to automate visibility.</p>

<p><img src="/images/posts/release-pipeline.png" width="900" height="350" title="image" alt="images"></p>

<h2>Kicking off a deployment</h2>

<p>I wrote previously that we started <a href="http://tech.opentable.co.uk/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot/">using chatops</a> to increase visibility operationally. Hubot is central to this and we wrote a small script to kick off deployments within <a href="https://www.hipchat.com/">Hipchat</a></p>

<p><img src="/images/posts/hubot-deploy-restaurant.png" width="350" height="350" title="image" alt="images"></p>

<p>We have two TeamCity instances. The first is used as a build and deployment system to our pre-production servers. The second is used as a deployment system to our production servers. Artifacts from our non-production instance are stored in <a href="http://www.jfrog.com/home/v_artifactory_opensource_overview">Artifactory</a> and our production deployment makes an API call to non-production TeamCity to ask for the last successfully pinned build. Pinning a build only occurs when we&rsquo;re happy that the build is ready to be shipped (passing unit and acceptance tests). The above Hubot command will pin the non-production build, given that the build succeeded, and add a build to the queue in production.</p>

<p>To configure Hubot to do this we wrote a command to setup aliases providing the build id of the build to pin (non-production) and the build id of the build to kick off (production).</p>

<p><img src="/images/posts/hubot-deploy-alias.png" width="350" height="350" title="image" alt="images"></p>

<h2>Deployment visibility</h2>

<p>Our production deployments must be auditable and it&rsquo;s important that we know what went out with each release and keep a log of this for our Risk Management team. We do this by creating a ticket in <a href="https://www.atlassian.com/software/jira">JIRA</a>, internally known as a CCB, and this gives us a central store of all deployments by all teams.</p>

<p>In the past these tickets were manually created for each release. We soon realised that this was something we could automate. To do so we created a new &ldquo;deployment-info&rdquo; endpoint for our service. This simply contains the SHA of the last commit released along with a time stamp. The first step of our production deploy is to query this endpoint and then using the Github API to get all the commits since that last SHA. These commits are then logged to JIRA to create a CCB ticket using the JIRA API. Each of these steps are automated from TeamCity using grunt tasks. You can find information of the grunt tasks on github as follows:</p>

<ul>
<li><a href="https://github.com/opentable/grunt-ccb">https://github.com/opentable/grunt-ccb</a></li>
<li><a href="https://github.com/opentable/grunt-github-manifest">https://github.com/opentable/grunt-github-manifest</a></li>
<li><a href="https://github.com/opentable/grunt-package-github">https://github.com/opentable/grunt-package-github</a></li>
</ul>


<h2>Build Notifications to Kibana</h2>

<p>Once we have a CCB we fire a start and end event from TeamCity containing the build number to Redis which is then piped into <a href="http://logstash.net/">Logstash</a>. An event is sent before and after deploying the code to all nodes. This is hugely beneficial because it allows us to plot releases against our graphs in <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a>. Kibana recently added a new feature called Markers. Essentially these are tags that display at the bottom of a graph.</p>

<p><img src="/images/posts/kibana-tags.png" width="350" height="350" title="image" alt="images"></p>

<p>You can find information on this Markers module on github &ndash; <a href="https://github.com/opentable/grunt-deployment-logger">https://github.com/opentable/grunt-deployment-logger</a></p>

<p>This has already proved incredibly useful for the team and has allowed us to visually correlate issues or changes in key metrics (response times/requests per second) to releases. The following image shows how these look over several graphs.</p>

<p><img src="/images/posts/kibana-dashboard.png" width="900" height="900" title="image" alt="images"></p>

<h2>Hipchat build complete notification</h2>

<p><img src="/images/posts/hubot-notification.png" width="350" height="200" title="image" alt="images"></p>

<p>Once our deployment pipeline has completed we send a notification to our teams room in Hipchat (as a final step in TeamCity) to inform the team that the release has completed. It&rsquo;s great to see a deployment start and end in chat. Having a central log of key operations in our team means that we don&rsquo;t have to go and find information when it&rsquo;s baked into chat.</p>

<h2>Conclusion</h2>

<p>We&rsquo;ve come along way with improving our pipeline and automating visibility. Our team is made up of 4 members; 3 in the office and 1 remote. The ultimate goal is to improve speed of deployment and visibility of events not just within the team but for everyone who is interested. Equally we want to continue to open source by as much as possible, allowing us to share our process with teams inside and outside of our organization. We can release code anywhere in the world and the process is completely centralised in chat. We want to continue to move fast and fix faster.</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/3/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/02/07/dismantling-the-monolith-microsites-at-opentable/">Dismantling the monolith - Microsites at Opentable</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/02/a-beginners-guide-to-rest-services/">A Beginner&#8217;s guide to REST services</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/01/23/on-strongly-typed-logging/">On Strongly Typed Logging</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/01/19/building-a-living-styleguide-at-opentable/">Building a living styleguide at OpenTable</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/01/01/explaining-flux-architecture-with-macgyver-dot-js/">Explaining Flux architecture with macgyver.js</a>
      </li>
    
  </ul>
</section>
<section>
	<a class="twitter-timeline" data-dnt="true" href="https://twitter.com/opentabletechuk"  data-widget-id="351711375858466817">Tweets by @opentabletechuk</a>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
</section>
<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/opentable">@opentable</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'opentable',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>Copyright &copy; 2015 - OpenTable</p></footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
