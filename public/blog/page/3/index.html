
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>OpenTable Tech UK Blog</title>
  <meta name="author" content="OpenTable">

  
  <meta name="description" content="In the first two parts of this blog series we have focusing on testing puppet modules with beaker. As an open source contributor there is always
a &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://tech.opentable.co.uk/blog/page/3/">
  <link href="/favicon-32.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="OpenTable Tech UK Blog" type="application/atom+xml">
  <script src="//fonts.otstatic.com/zys4lfz.js"></script>
  <script>try{Typekit.load();}catch(e){}</script>

  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-2621903-16']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">OpenTable Tech UK Blog</a></h1>
  
    <h2>The technology blog for OpenTable UK.</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:tech.opentable.co.uk" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/authors">Authors</a></li>
  <li><a href="/about">About this blog</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-3-testing-roles/">Testing Puppet with Beaker pt.3 - Testing Roles</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-09-01T13:09:05+01:00" pubdate data-updated="true">Sep 1<span>st</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In the first two parts of this blog series we have focusing on testing puppet <em>modules</em> with beaker. As an open source contributor there is always
a large test matrix so this makes absolute sense. But what about the other large use-case for beaker &ndash; what about our day-to-day internal code base?
Not all of this is modules, in fact a large portion of it is other puppet code &ndash; roles, profiles, facts, hiera data etc. All of this needs testing
as well.</p>

<p>In this blog post I will be showing how we have started using beaker to test our puppet roles and profiles for both Linux and Windows.</p>

<h2>Master-vs-Masterless</h2>

<p>Prior to this post all our beaker testing has been master-less i.e. using using puppet agent apply. This is perfectly adequate for most use cases when
testing modules in isolation but doesn&rsquo;t always work when testing an internal code base (unless you are masterless there as well then please skip to the next section).</p>

<p>At OpenTable we do use a central puppet master to compile our catalogs. So when testing our puppet roles we wanted to make sure that we were also testing
with a master-agent configuration. It is worth mentioning here that if (like us) you are testing windows agents then you are going to need to test with master-agent
approach due to the lack of a windows master.</p>

<p>Testing the master-agent configuration means configuring multi-node sets in beaker. There are not many examples of this but the principle is very much
the same as the single-node nodeset. Here is an example:</p>

<pre><code>HOSTS:
  ubuntu-server-12042-x64-master:
    roles:
      - master
    platform: ubuntu-12.04-amd64
    box: ubuntu-server-12042-x64-vbox4210-nocm
    box_url: http://puppet-vagrant-boxes.puppetlabs.com/ubuntu-server-12042-x64-vbox4210-nocm.box
    hypervisor: vagrant
    ip: '10.255.33.135'
  win-2008R2-std:
    roles:
      - default
      - agent
    platform: windows-server-amd64
    box: opentable/win-2008r2-standard-amd64-nocm
    box_version: = 1.0.0
    box_check_update: false
    hypervisor: vagrant
    user: vagrant
    ip: '10.255.33.129'
    communicator: bitvise
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>In this example you will see that we are specifying different &lsquo;roles&rsquo; for each host in the nodeset. What a role is in in this context is a tag for that node that allows
us to reference it directly later when running commands on the host. To avoid any further confusion, from this point onwards if I am referring to the role defined in the
nodeset file I will call it the &lsquo;nodeset role&rsquo; otherwise I am referring the the puppet role provided in the manifest. There are a couple of build-in nodeset roles that
Beaker already knows about: master, agent and default. The first two are pretty self explanatory but the last nodeset role &ndash; default &ndash; is the location where the tests
themselves run. In you don&rsquo;t specify the &lsquo;default&rsquo; nodeset role on any of your host definitions then the tests will run on the first host that you specified in in the
nodeset file (which in the case of the example above would be wrong).</p>

<p>You may have a more complicated configuration that you wish to test and this allows you to specify arbitrary tags which can be very useful.</p>

<p>We can now use these nodeset roles to configure our master and agent.</p>

<p>In parts <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">[1]</a> and <a href="http://tech.opentable.co.uk/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-2-the-windows-story">[2]</a> of this series we saw what a basic spec_acceptence file looks like. So let&rsquo;s start with that:</p>

<pre><code>require 'beaker-rspec/spec_helper'
require 'beaker-rspec/helpers/serverspec'
require 'winrm'

hosts.each do |host|

  if host['platform'] =~ /windows/
    include Serverspec::Helper::Windows
    include Serverspec::Helper::WinRM
  end

  version = ENV['PUPPET_VERSION'] || '3.5.1'
  install_puppet(:version =&gt; version)

  if host['roles'].include?('master')

    ... # Install a master

  else

    ... # Install an agent

  end
end

RSpec.configure do |c|

  c.before :suite do

    hosts.each do |host|
      c.host = host

      if host['platform'] =~ /windows/
        endpoint = "http://127.0.0.1:5985/wsman"
        c.winrm = ::WinRM::WinRMWebService.new(endpoint, :ssl, :user =&gt; 'vagrant', :pass =&gt; 'vagrant', :basic_auth_only =&gt; true)
        c.winrm.set_timeout 300
      end
    end
  end
end
</code></pre>

<p>We can see here how we use the host[&lsquo;roles&rsquo;] in order to select the appropriate code-path for configurting each nodeset role. Now let&rsquo;s
move onto how we configure each of those nodeset roles.</p>

<h2>Configuring the master</h2>

<p>There are a lot of things that go into building a puppetmaster:
 &ndash; puppetmaster packages
 &ndash; hiera backends
 &ndash; gems for addditional dependencies (eyaml + puppetdbquery)
 &ndash; downloading external modules</p>

<p>Now let&rsquo;s step through our new spec_acceptence file that supports this multi-node environment:</p>

<h3>Deploying the codebase</h3>

<p>Stage one is getting our puppet codebase onto the master, which includes all the files, internal modules and anything else we need to get the master up and
running. We do this like follows:</p>

<pre><code>files = [ 'environments','facts','hiera','roles', 'profiles', 'keys', 'app_modules', 'auth.conf','autosign.conf',
          'fileserver.conf', 'Gemfile','hiera.yaml','Puppetfile'
        ]

files.each do |file|
  scp_to master, File.expand_path(File.join(File.dirname(__FILE__), '..', file)), "/etc/puppet/#{file}"
end

# scp dist modules folder (this excludes stuff like spec and test folders)
dist_modules = Dir["#{dist_modules_root}/*/"].map { |a| File.basename(a) }
dist_modules.each do |module_name|
  dist_module_dir = "#{dist_modules_root}/#{module_name}"
  copy_module_to(master, :source =&gt; dist_module_dir, :module_name =&gt; module_name)
end
</code></pre>

<p>Here we are selecting all the files that we want and calling the scp_to method which will scp any file or directory to the host of choice, in this case our
master.</p>

<h3>The puppetmaster:</h3>

<pre><code>...

on master, "apt-get install -y rubygems git"
on master, "apt-get install -y puppet-common=#{version}-1puppetlabs1 puppetmaster-common=#{version}-1puppetlabs1 puppetmaster=#{version}-1puppetlabs1 "
on master, "echo '*' &gt; /etc/puppet/autosign.conf"

...
</code></pre>

<p>So we have already installed puppet at a previous stage in our script. At this point we are performing all the steps required to install the
puppetmaster: git, rubygems (if on an older distro) and the puppetmaster packages. We also making sure that we auto-signing if configured to
save us some pain later on. This step should really be configured as another beaker method that we can just call but for now it is still manual.
It is at this point that we have first introduced the &ldquo;on master&rdquo; this does what you think it might, it executes the command you pass it onto
the host with the nodeset role on &lsquo;master&rsquo;.</p>

<h3>Set the puppet.conf file:</h3>

<pre><code>...

config = {
  'main' =&gt; {
    'server'   =&gt; master_name,
    'certname' =&gt; master_name,
    'logdir'   =&gt; '/var/log/puppet',
    'vardir'   =&gt; '/var/lib/puppet',
    'ssldir'   =&gt; '/var/lib/puppet/ssl',
    'rundir'   =&gt; '/var/run/puppet'
  },
  'agent' =&gt; {
    'environment' =&gt; 'vagrant'
  }
}

configure_puppet(master, config)

...
</code></pre>

<p>Here we are configuring out puppet.conf file, making sure that it includes any customization we might need. This uses a configure_puppet method
that we have added to beaker to allow us to do this customization and in this case it is taking the hash to modify the puppet.conf file on the master
host.</p>

<h3>Install the required ruby gems:</h3>

<pre><code>...

on master, "gem install bundler"
on master, "gem install hiera-eyaml"
on master, "cd /etc/puppet &amp;&amp; bundle install --without development"

...
</code></pre>

<p>The average production-ready puppetmaster also requires a number of gems to function such as hiera-eyaml, deep_merge any many others
depending upon what backends and other custom puppet extensions you have implemented. Here we are installing all our dependencies from
the Gemfile we have already put onto the host.</p>

<h3>Installing modules:</h3>

<pre><code>...

on master, "cd /etc/puppet &amp;&amp; bundle exec librarian-puppet install"

...
</code></pre>

<p>The last major step is installing any external modules you have. You may be using librarian-puppet or r10k to do this. In our case it
is the former so we go ahead and make sure that our modules directory is full of all the modules we require.</p>

<h3>Networking:</h3>

<pre><code>...

master_name = "#{master}.test.local"
on master, "echo '10.255.33.135   #{master_name}' &gt;&gt; /etc/hosts"
on master, "hostname #{master_name}"
on master, "/etc/init.d/puppetmaster restart"

...
</code></pre>

<p>This last step is a small hack that you will probably require if you are running on vagrant. It just configures the host file to make
sure that it&rsquo;s hostname if configured correctly from certificate signing to work as expected. This might not be required in your
environment and I would try it without first but it&rsquo;s worth noting anyway.</p>

<h2>Configuring the agent</h2>

<p>So if you&rsquo;ve got to this point well done &ndash; most of the hard work is done. Configuring the agent(s) is pretty straightforward in comparison
to a puppetmaster and some of the steps are similiar:</p>

<h3>Set the puppet.conf file:</h3>

<pre><code>if host['roles'].include?('master')
  ...
else
  agent = host
  master = only_host_with_role(hosts, 'master')
  agent_name = agent.to_s.downcase
  master_fqdn = "#{master}.test.local"
  agent_fqdn = "#{agent_name}.test.local"

  if agent['platform'] =~ /windows/
    config = {
      'main' =&gt; {
        'server'   =&gt; master_fqdn,
        'certname' =&gt; agent_name,
        'logdir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\log',
        'vardir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\lib',
        'ssldir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\lib\\ssl',
        'rundir'   =&gt; 'C:\\ProgramData\\PuppetLabs\\puppet\\var\\run'
      },
      'agent' =&gt; {
        'environment' =&gt; 'vagrant'
      }
    }
  else
    config = {
      'main' =&gt; {
        'server'   =&gt; master_fqdn,
        'certname' =&gt; agent_fqdn,
        'logdir'   =&gt; '/var/log/puppet',
        'vardir'   =&gt; '/var/lib/puppet',
        'ssldir'   =&gt; '/var/lib/puppet/ssl',
        'rundir'   =&gt; '/var/run/puppet'
      },
      'agent' =&gt; {
        'environment' =&gt; 'vagrant'
      }
    }
  end
  ...

  configure_puppet(agent, config)
end

...
</code></pre>

<p>Again here we are again using the configure_puppet method, this time change the puppe.conf file on the agent.</p>

<p>As you can see we are catering for both windows and linux hosts here. We are also making sure that the certname
and server are defined properly and match what we set-up for the master so that auto-signing works correctly.</p>

<h2>Testing the Role</h2>

<p>At this point we have now does all our prerequisites and we can spin up two machines to test against &ndash; 1 master and
1 agent. But when we are testing a role what is it that we actually want to test and why is this not covered in
earlier less-expensive puppet-rspec unit tests?</p>

<p>Well there are 3 key things that we wanted to test:
1. Idempotence <br/> This is pretty straight forward to test. Beaker provides a method run_agent_on that will run the puppet agent on a
  given host. This means we can test idempotency like this:</p>

<pre><code>   run_agent_on(agent, :catch_failures =&gt; true)
   expect(run_agent_on(agent, :catch_failures =&gt; true).exit_code).to be_zero
</code></pre>

<ol>
<li><p>Interaction of multiple modules and profiles <br/> This is the big motivator &ndash; we want to test that and make sure that the combinations
of profiles that we are applying work together and do not either break the catalog or operate in a non-idempotent way. We are also gaining
the ability to test that updates in modules (many of which are external from the puppet forge) do not break our roles in any way.</p></li>
<li><p>Postivie/Negative testing &ndash; do we clean up after ourselves if we remove something. <br/> This is not something that is often considered
very often, particularly in a world where machines are torn down and re-build so often but there still exists a use-case where this is not
always possible and we want to make sure that our roles and manifests are not littering our machines unnecessarily.</p></li>
</ol>


<p>Below is a full example of one of our linux profiles:</p>

<pre><code>require 'spec_helper_acceptance'

describe 'linux_base_profile', :if =&gt; fact('osfamily').eql?('Debian') do
  context 'linux base profile' do
    it 'should should run successfully' do

      agent = only_host_with_role(hosts, 'agent')
      master = only_host_with_role(hosts, 'master')

      pp = "node \"#{agent}\" { include profiles::linux::base }"
      on master, "echo '#{pp}' &gt;&gt; /etc/puppet/manifests/site.pp"

      run_agent_on(agent, :catch_failures =&gt; true)
      expect(run_agent_on(agent, :catch_failures =&gt; true).exit_code).to be_zero
    end

    context 'installation of ops tools' do

      describe package('sysstat') do
        it { should be_installed }
      end

      describe package('iotop') do
        it { should be_installed }
      end

      describe package('ngrep') do
        it { should be_installed }
      end

      describe package('lsof') do
        it { should be_installed }
      end

      describe package('unzip') do
        it { should be_installed }
      end
    end

    context 'managing puppet version' do

      describe file('/etc/apt/sources.list.d/puppetlabs.list') do
        it { should be_file }
        it { should be_owned_by 'root' }
        it { should be_mode 644 }
      end

      describe package('puppet') do
        it { should be_installed.by('apt').with_version('3.6.1-1puppetlabs1') }
      end
    end

    context 'manage sshd configuration' do

      describe process("sshd") do
        it { should be_running }
      end

      describe port(22) do
        it { should be_listening }
      end

      describe file('/etc/ssh/sshd_config') do
        its(:content) { should match /PermitRootLogin no/ }
        its(:content) { should match /PasswordAuthentication yes/ }
        its(:content) { should match /UseDNS no/ }
      end
    end
  end
end
</code></pre>

<p>We have a lot of roles and profiles that we would like to test. As you might imagine this could get quite verbose and repetitive pretty quickly. We are currently building
up <a href="https://www.relishapp.com/rspec/rspec-core/docs/example-groups/shared-context">shared_contexts</a> for each of our profiles which we can
then wrap up into roles to easily reflect our roles/profiles structure in the main codebase.</p>

<h2>Summary</h2>

<p>We are just at the very beginning of this journey with Beaker. As well as testing all our modules we are looking to scale our to test the roles and profiles in our whole
code base. These examples here are how we are doing it at the moment for our mixed-platform environment. We will continue to expand upon it and build it into our pipeline.
At this moment we are looking to expand beyond vagrant and run these against AWS instances but perhaps that is for the next post &hellip;</p>

<p>As usual for any questions or comments then please reach out to me on twitter <a href="https://twitter.com/liamjbennett">@liamjbennett</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/09/01/testing-puppet-with-beaker-pt-dot-2-the-windows-story/">Testing Puppet with Beaker pt.2 - The Windows story</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-09-01T09:43:10+01:00" pubdate data-updated="true">Sep 1<span>st</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">part one</a> we discussed our first steps into the world of acceptance testing our Puppet manifests.
By using Beaker we able to test managing local users on our Linux boxes. This was a positive experience for us. It allowed us to get to grips with the basics of configuring
Beaker to run tests and configuring our node sets to run those tests against. In this post, we will be discussing how we went about getting Beaker working with Windows.</p>

<p>As many of your reading this will be aware, OpenTable currently has quite a large Windows infrastructure and we are using Puppet extensively to maintain that environment.
We are also moving forward with releasing as many of our modules open source onto the <a href="https://forge.puppetlabs.com/opentable">Puppet Forge</a> as possible (11 out of 18 of which are Windows
exclusive). What this means is that there was no way that we could ignore trying to use Beaker to test our manifests against Windows. We knew that we would have to support
many different versions and editions of Windows out there in the community as well that the ones we have to support internally.</p>

<p>This was going to be a challenge (configuration management with Windows usually is) but we were up for it.</p>

<h2>The Preliminaries</h2>

<h3>Serverspec</h3>

<p>The first step was looking at Serverspec. Serverspec is a Ruby gem that provides extensions to RSpec that allow you to test the actual state of your servers, either locally
or from the outside in via SSH. What we needed to know was did it support Windows? The answer was thankfully a resounding &ldquo;Yes!&rdquo;. All the <a href="http://serverspec.org/resource_types.html">resource types</a> that you might want to test including file, service and user are available and supported on Windows. There are also a couple of Windows specific ones such as iis_website, Windows_feature and Windows_registry_key. We even added our own to support <a href="https://github.com/serverspec/serverspec/pull/403/files">Windows_scheduled_task</a>. Interestingly Serverspec also supports WinRM as an alternative to SSH when you are testing from the outside-in but we will go back into that later. As long as your using Serverspec > 1.6 you will have all the Windows support you might need.</p>

<h3>Packer</h3>

<p>Step two was to build some Windows Vagrant boxes to test against. The documentation on the <a href="http://github.com/puppetlabs/beaker/wiki">wiki</a> was (at the time) a bit slim when it came
to building test boxes but we knew we needed Cygwin so we went ahead and created the boxes that we needed. All our boxes are created with Packer <a href="http://github.com/opentable/packer-images">and are open sourced on GitHub</a>. They have also been <a href="http://vagrantcloud.com/opentable">published to Vagrant Cloud</a>
so you can download pre-built images and get up and running quickly (version 1.x images contain the Cygwin installation).</p>

<h3>Beaker</h3>

<p>So far, so good. We hit a couple of issues in our initial test runs with Beaker: missing module_path, installation using the msi and 32-bit Windows support &ndash; but these were very
small issues and we were happy to be able to contribute back some changes (<a href="https://github.com/puppetlabs/beaker/pull/234">234</a>,
<a href="https://github.com/puppetlabs/beaker/pull/235">235</a>, <a href="https://github.com/puppetlabs/beaker/pull/236">236</a>). We were very happy and managed to get out first module tested,
the <a href="http://github.com/opentable/puppet-puppetversion/">cross-platform module puppet-puppetversion</a> for doing Puppet upgrades.</p>

<h2>The First Example</h2>

<p>Let&rsquo;s take a more detailed look at those puppetversion tests, how we configured Beaker to run and how it changed for the Windows support. I am going to assume at this point that
you already have some familiarity with Beaker; if not and this is your first steps into the testing tool then I would suggest going back and read <a href="http://tech.opentable.co.uk/blog/2014/04/04/testing-puppet-with-beaker/">part one</a> of this series which contains a little bit of background to this and some useful resources for getting started.</p>

<p>The first thing that we needed to change for Windows was our <a href="https://raw.githubusercontent.com/opentable/puppet-puppetversion/master/spec/spec_helper_acceptance.rb">spec_accepentance.rb file</a>.</p>

<p>Step one was to include the appropriate Serverspec helpers. What this does is let Serverspec know that we are executing on Windows so that underlying resources work correctly. We are also telling
server spec here to communicate using WinRM.</p>

<pre><code>require 'beaker-rspec/spec_helper'
require 'beaker-rspec/helpers/serverspec'
require 'winrm'

hosts.each do |host|
  case host['platform']
    when /windows/
      include Serverspec::Helper::Windows
      include Serverspec::Helper::WinRM

      version = ENV['PUPPET_VERSION'] || '3.4.3'

      install_puppet(:version =&gt; version)

  else
    install_puppet
  end
end

...
</code></pre>

<p>The next step is to configure WinRM so that it can connect properly. In our case this meant connecting to Vagrant boxes.</p>

<pre><code>...

RSpec.configure do |c|
  ...
  hosts.each do |host|
    c.host = host

    if host['platform'] =~ /windows/
      endpoint = "http://127.0.0.1:5985/wsman"
      c.winrm = ::WinRM::WinRMWebService.new(endpoint, :ssl, :user =&gt; 'vagrant', :pass =&gt; 'vagrant', :basic_auth_only =&gt; true)
      c.winrm.set_timeout 300
    end

    ...
  end
end

...
</code></pre>

<p>Now let&rsquo;s look at one of the tests themselves.</p>

<p>The first part should look pretty familiar. We use the face(&lsquo;osfamily&rsquo;) helper in Beaker to make sure that the test itself is only ever executed for our Windows hosts. We are then running an apply_manifest two times in order to validate that the manifest is idempotent. The only different here is that we are specifying a custom Windows-specific module_path.</p>

<pre><code>...
require 'spec_helper_acceptance'

describe 'puppetversion', :unless =&gt; UNSUPPORTED_PLATFORMS.include?(fact('osfamily')) do
  ...

  context 'upgrade on windows', :if =&gt; fact('osfamily').eql?('windows') do

  it 'should install the new version' do
    pp = &lt;&lt;-PP
      class { 'puppetversion':
        version =&gt; '3.5.1',
        time_delay =&gt; 1
      }
    PP

    apply_manifest(pp, :modulepath =&gt; 'C:/ProgramData/PuppetLabs/puppet/etc/modules', :catch_failures =&gt; true)
    expect(apply_manifest(pp, :modulepath =&gt; 'C:/ProgramData/PuppetLabs/puppet/etc/modules', :catch_failures =&gt; true).exit_code).to be_zero
  end
</code></pre>

<p>The next part is where we actually perform the bulk of the tests. In the case of this module, we are testing
that the scheduled task has run and that Puppet has been upgraded to the appropriate version. We are making use
of the Windows_scheduled_task resource that we created earlier:</p>

<pre><code>  describe Windows_scheduled_task('puppet upgrade task') do
    it { should exist }
  end

  #This will fail if your laptop (and therefor the Vagrant vm) is not running on AC power
  describe package('puppet') do
    it {
      sleep(240) #Wait for the task to execute
      should be_installed.with_version('3.5.1')
    }
  end

  describe Windows_scheduled_task('puppet upgrade task') do
    it {
      pp = &lt;&lt;-PP
        class { 'puppetversion':
          version =&gt; '3.5.1'
        }
      PP

      apply_manifest(pp, :modulepath =&gt; 'C:/ProgramData/PuppetLabs/puppet/etc/modules', :catch_failures =&gt; true)

      should_not exist
    }
  end
end
</code></pre>

<p>The final part was to configure the nodeset file for the Windows box that we wanted to run our test against:</p>

<pre><code>HOSTS:
  windows-2008R2-serverstandard-x64:
  roles:
    - agent
  platform: windows-server-amd64
  box : opentable/win-2008r2-standard-amd64-nocm
  box_url : opentable/win-2008r2-standard-amd64-nocm
  hypervisor : vagrant
  user: vagrant
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>This was working well for us so we continued on to our next module.</p>

<p>The next module we chose to look at was <a href="http://github.com/opentable/puppet-Windowsfeature">puppet-Windowsfeature</a>.</p>

<p>The test we have implemented in this moudle looks like this:</p>

<pre><code>require 'spec_helper_acceptance'

describe 'Windowsfeature' do
  context 'windows feature should be installed' do
    it 'should install .net 3.5 feature' do

      pp = &lt;&lt;-PP
        Windowsfeature { 'as-net-framework': }
      PP

      apply_manifest(pp, :catch_failures =&gt; true)
      expect(apply_manifest(pp, :catch_failures =&gt; true).exit_code).to be_zero
    end

    describe Windows_feature('as-net-framework') do
      it { should be_installed.by('powershell') }
    end
  end
end
</code></pre>

<p>This module was a little more tricky. Why? Installing Windows features requires elevated permissions. What this meant is that when Beaker attempted to SSH into our Windows box and our Puppet module ran its underlying PowerShell we were faced with a harsh and non-descriptive <strong>&ldquo;Access is denied error&rdquo;</strong>.</p>

<h2>SSH</h2>

<p>Not all SSH daemons are created equal. To understand the &ldquo;Access is denied error&rdquo; we were seeing and why it happens you need to understand a little bit about how sshd on Cygwin works. You can read all about the details from the Cygwin forum archives (<a href="http://cygwin.com/ml/cygwin/2004-09/msg00087.html">[1]</a>, <a href="http://cygwin.com/ml/cygwin/2006-06/msg00862.html">[2]</a>, <a href="http://thread.gmane.org/gmane.os.cygwin/128472">[3]</a>, <a href="https://cygwin.com/cygwin-ug-net/ntsec.html#ntsec-nopasswd1">[4]</a>) but TLDR; is that you need to use a username and password rather than private key authentication in order to get reasonable admin permissions. Having said all that, and having read all the documentation about the issue above we were still facing the same problem so we had to look at alternative options.</p>

<p>There are several paths you can go down and I want to tell you about them here to save you a similar yak shave:</p>

<h3>OpenSSH for Windows</h3>

<p>A thinner alternative than having to install the full Cygwin stack using OpenSSH for Windows is the same OpenSSH implementation. The issue here however is that it doesn’t contain some of the Unix binaries required for Beaker to function. You can work around this if you have Git for Windows installed on your machine by putting its bin directory on your path but overall this doesn’t really solve any of the issues we were facing. We get a lighter footprint on the machine but still the same error as before</p>

<h3>Bitvise SSH Server</h3>

<p><a href="http://www.bitvise.com/ssh-server-download">Bitvise SSH Server</a> is an alternative SSH implementation (of which there are many more listed on <a href="http://en.wikipedia.org/wiki/Comparison_of_SSH_servers">Wikipedia</a>). It resolves the permissions issue (it deals with the elevation internally) and has the benefit that it provides a proper command shell rather than a emulated bash shell. It also means we don’t have to have any binaries on there that we don’t need &ndash; a big plus. It would mean that we needed to make a few small changes to Beaker in order to replace some of the internal bash command with Windows alternatives but this was not a big task to do and is something we could contribute back.</p>

<h3>WinRM</h3>

<p>Could we do away with SSH altogether? It eliminates the problem we were facing and also means we don’t have to install anything on our Windows boxes &ndash; it’s all built in already. This would be an ideal solution but does all our tooling support it? I’ll discuss this in a little bit more detail later.</p>

<h3>Not use Beaker at all</h3>

<p>The nuclear option. If we couldn’t get anything to work we could not use Beaker at all, we could try and use Test Kitchen (with <a href="https://github.com/neillturner/kitchen-puppet">test-kitchen-puppet</a>) or some hand-rolled solution. Not the best idea, for us or the community but we though we might have to go down this path at one point. We even added <a href="https://github.com/liamjbennett/kitchen-puppet/tree/Windows_support">Windows support to test-kitchen-puppet</a> as part our diversion in this direction.</p>

<h2>What worked in the end:</h2>

<p>So we went down all these avenues and decided that the best option for us was to use Bitvise. It fixed the problem we were facing but it meant that we had some work ahead of us:
1. We had to rebuild all our Windows images with Bitvise rather than Cygwin (<a href="https://vagrantcloud.com/opentable">vagrantcloud.com/opentable</a> &ndash; version 2.x images now have this)
2. We had to make some changes to Beaker to support using a standard Windows command shell rather than a Unix shell:</p>

<ul>
<li> <a href="https://github.com/puppetlabs/beaker/pull/419">https://github.com/puppetlabs/beaker/pull/419</a> – configure_puppet method</li>
<li> <a href="https://github.com/puppetlabs/beaker/pull/420">https://github.com/puppetlabs/beaker/pull/419</a> – host_entry method</li>
<li> <a href="https://github.com/puppetlabs/beaker/pull/418">https://github.com/puppetlabs/beaker/pull/419</a> – Vagrant box_version</li>
<li> <a href="https://github.com/puppetlabs/beaker/pull/424">https://github.com/puppetlabs/beaker/pull/419</a> – Bitvise SSH</li>
</ul>


<p>With it finally working we had something that looked like this:</p>

<h2>Second Example</h2>

<p>Well with all the changes we implemented there were actually very few changes that we needed to make to our actual tests code.</p>

<p>No adjustments are needed for our spec_acceptance.rb file.</p>

<p>No adjustments are required for our spec file (show above).</p>

<p>The main change we made was in the nodeset file:</p>

<pre><code>HOSTS:
  win-2008R2-std:
  roles:
    - default
    - agent
  platform: Windows-server-amd64
  box: opentable/win-2008r2-standard-amd64-nocm
  hypervisor: vagrant
  user: vagrant
  ip: '10.255.33.129'
  communicator: bitvise
CONFIG:
  log_level: verbose
  type: git
</code></pre>

<p>The biggest change you will see here is the addition of the &lsquo;communicator&rsquo; option. What this does is to allows us to actively select to use either Cygwin or Bitvise. This means that in our case we want to use Bitvise SSH as this fixes the error we were seeing and it&rsquo;s the version of SSH now installed on your newer Vagrant boxes. Bitvise is the only supported option at the moment (in our Beaker fork) but it is likely that this will soon support WinRM as well.</p>

<p>Things to note here:
* The name of the Windows host defined in the node set must be the same as the name of the Windows hostname &ndash; if it is not then when Vagrant boots up it will change the hostname
which will put Windows into a &ldquo;restart pending&rdquo; state.</p>

<h2>The Future and WinRM</h2>

<p>Most modern versions of Windows server have WinRM enabled by default but if you are using an older version or you are attempting to test a client then you will need to make sure
that this is enabled on your boxes. This is still the direction that we would like to go long-term as it is the most Windows-friendly approach but there are few road blocks in
the way of doing so right now:</p>

<ol>
<li><p>Packer doesn’t set support WinRM as a communication method. This is being actively worked on and you can follow the work here:</p>

<ul>
<li><a href="https://github.com/mitchellh/packer/issues/451">https://github.com/mitchellh/packer/issues/451</a></li>
<li><a href="https://github.com/dylanmei/packer-communicator-winrm">https://github.com/dylanmei/packer-communicator-winrm</a></li>
</ul>
</li>
<li><p>Beaker doesn’t yet support WinRM as a communication protocol. This is currently being discussed internally after we raised the idea. The work that we have completed for
Bitvise support will go some way it allowing other providers, such as wirm going forward and therefore WinRM support should be coming in the near future.</p></li>
</ol>


<h2>Summary</h2>

<p>Using Beaker to test modules for Windows has been a long and complicated journey. I have attempted to cover here all the problems that you might run into when trying to do this for yourselves and provide some good examples to get you going. You will soon see this being rolled out to all of the OpenTable open source modules shortly so you will have some complete working examples to reference. We will continue to work with PuppetLabs in improving Beaker (and its Windows support) in order to make this a easy process for everyone.</p>

<p>For any questions or comments then please reach out to me on twitter <a href="https://twitter.com/liamjbennett">@liamjbennett</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/08/08/what-can-i-do/">What can I do?</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-08-08T13:52:30+01:00" pubdate data-updated="true">Aug 8<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I&rsquo;ve noticed recently a lot of content in my social media network is based on the current escalating problems in the Middle East.</p>

<p>Whenever I see such content from a friend or follower, I&rsquo;m reminded of Stephen Covey&rsquo;s <a href="http://www.amazon.co.uk/gp/product/B00GOZV3TM/ref=as_li_tl?ie=UTF8&amp;camp=1634&amp;creative=19450&amp;creativeASIN=B00GOZV3TM&amp;linkCode=as2&amp;tag=helenpieli-21&amp;linkId=RTIUTRBYIE6APBJA">The 7 Habits of Highly Effective People</a> &ndash; Circle of Influence, Circle of Concern.</p>

<p>Covey says that in order to remain truly effective, we should focus our time and energy on situations we can influence. Don&rsquo;t worry about things we cannot control, such as the weather, the economy or indeed foreign conflicts.</p>

<p>Covey&rsquo;s work implies that if we identify the areas that are inside our Circle of Influence we will be more productive and make more of an impact;</p>

<p><img src="http://www.chowamigo.co.uk/images/what_can_i_do.png" alt="image" /></p>

<p>If I&rsquo;m honest I find this work quite harsh in its approach. Indeed it might not sit well with everyone. However, I personally found myself much less burdened once I could identify which problems I had control over.</p>

<h2>Expand your influence</h2>

<p>I still find myself watching the news, reading press reports and shaking my head at the global problems we&rsquo;ve yet to solve. Covey&rsquo;s stance is a tough one to take &ndash; don&rsquo;t waste energy on X, your time and resources are better utilised by focusing on what you can change.</p>

<p>Some examples:</p>

<ul>
<li><p>I&rsquo;m concerned about global warming => recycle waste as best you can, be energy efficient with your devices, insulate your home more effectively.</p></li>
<li><p>I&rsquo;m concerned about my finances => learn new skills, look for a better role, try to control your spending, reduce unnecessary outgoings.</p></li>
<li><p>I&rsquo;m concerned about my health => get a check up, start to eat healthily, take regular exercise, reduce tobacco and alcohol intake.</p></li>
</ul>


<p>What I like about Covey&rsquo;s advice is that it&rsquo;s sending a clear message &ndash; are we able to make the sun shine at weekends? Can we solve the UK economic crisis by next month? Is there really anything we can do to bring about peace in the Middle East? The likely answer for most of us is no. We  simply have no control on areas outside of our power.</p>

<p>But if you step back into your circle of influence and be pro-active, you can make a real difference.</p>

<h2>Further reading</h2>

<p><a href="http://www.ndoherty.com/circle-of-influence-circle-of-concern">http://www.ndoherty.com/circle-of-influence-circle-of-concern</a></p>

<p><a href="http://www.upcyclededucation.com/2012/09/circle-of-concern-circle-of-influence.html">http://www.upcyclededucation.com/2012/09/circle-of-concern-circle-of-influence.html</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/20/acceptance-now/">Acceptance Now</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-05-20T15:00:00+01:00" pubdate data-updated="true">May 20<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><em>When is acceptance-only testing a good idea, and how can its problems be overcome?</em></p>

<p>In <a href="/blog/2014/04/16/look-ma-no-unit-tests">a recent post</a>, I espoused some of the benefits my team enjoyed by reducing our test-base to a single layer of acceptance tests, with no separate unit or integration tests. It caused <a href="http://www.reddit.com/r/programming/comments/237fr1/look_ma_no_unit_tests/">some minor controversy</a>, which was not to be unexpected. At the time, I knew I had left out some details for brevity&rsquo;s sake. In this post—spurred on by some interesting <a href="https://twitter.com/NathanGloyn/status/456756552092098561">questions</a> and <a href="http://www.reddit.com/r/programming/comments/237fr1/look_ma_no_unit_tests/cgujuv7">commentary</a>—I&rsquo;d like to offer a more constructive view on the subject, and dig a little deeper into the nitty gritty of how we made it work.</p>

<p>I&rsquo;ll also point out some other hidden benefits of moving to acceptance-only testing, and suggest synergistic practices that can help decide if this is the right approach for your project.</p>

<h2>Seams</h2>

<p>After-the-fact unit testing requires us to <a href="https://www.youtube.com/watch?v=wEhu57pih5w">find the seams</a> along which code can be isolated and tested. Where those seams don&rsquo;t exist,  the temptation is to refactor code until they do, using patterns like <a href="http://martinfowler.com/articles/injection.html">dependency injection</a>, and following <a href="http://en.wikipedia.org/wiki/Single_responsibility_principle">SRP</a> and other <a href="http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod">SOLID</a> patterns.</p>

<p><a href="http://en.wikipedia.org/wiki/Test-driven_development">Test-first unit testing</a> aka TDD naturally tends to maximise seams, and results in highly decoupled code with small, specific tests. TDD must result in 100% unit test coverage, if practiced according to <a href="http://en.wikipedia.org/wiki/Test-Driven_Development_by_Example">the gospel</a>. In other words, TDD results in the best kind of code for later <a href="http://sourcemaking.com/refactoring">modification without risk</a>, and the best tests for detailed, granular feedback. Unit tests also tend to run very quickly, sometimes fast enough to <a href="http://misko.hevery.com/2009/05/07/configure-your-ide-to-run-your-tests-automatically/">run every time you hit &ldquo;Save&rdquo;</a>.</p>

<p><strong>Acceptance testing has far fewer seams available.</strong> Typically only one: the application boundary. Yes: <em>Your tests must invoke the entire application each time they are run.</em> Thus running acceptance tests is potentially very slow for even moderately sized projects with few dependencies—let alone large ones with many. Another problem is that, when an acceptance test fails, the failure could be at <em>any</em> layer in the stack. You immediately lose the pinpoint specificity afforded by unit tests.</p>

<p>Why, then, is it sometimes a good idea to forgo unit- in favour of acceptance-tests?</p>

<p>As we will see, the first issue, performance, can be mitigated. The second, granularity, is more difficult to overcome. But sometimes <em>that might be okay.</em></p>

<h2>Performance</h2>

<p>In most cases I have seen, acceptance tests are <em>really</em> slow. In some cases, there might be nothing you can do, but usually there is. Front-end automation suites using <a href="http://docs.seleniumhq.org/">Selenium</a> are temporally incorrigible, but they can be parallelised. Complex transactional pieces might be irreducible, but they can be helped with mock data. No matter which box your code is in, there is probably an escape route. Following are some of the techniques we used to overcome performance bottlenecks in our acceptance test suite&hellip;</p>

<h3>Sandbox Data</h3>

<p><em>This is probably a topic that deserves its own post, but I&rsquo;ll try to give a high-level treatment here.</em>
In our project, we took on the overhead of providing mock &ldquo;sandbox&rdquo; data. For our consumers this was a required feature anyway, so implementing it began early in the project, well before we <a href="/blog/2014/04/16/look-ma-no-unit-tests">deleted all the unit tests</a>. It turned out this was an important enabler in moving to acceptance-only, since it allowed us to run tests much faster by <em>sometimes</em> circumventing data access.</p>

<p>Since this project was written in C# using strict TDD, our data layer already had <a href="http://en.wikipedia.org/wiki/Interface_(computer_science)#Software_interfaces_in_object-oriented_languages">an interface</a> for each data source. In the <em>unit</em> tests this allowed us to easily stub out the data. We reused the same interfaces to build up our sandbox, with dependency injection at runtime to choose between real and mock data. (I like to call this a &lsquo;pseudoseam&rsquo; in that it allows us to isolate data access at runtime, just like an ordinary seam allows you to isolate classes and methods in test.)</p>

<p><em>Sandbox data is hard to implement. A lot of the effort that would have gone into unit tests and their maintenance was instead pumped into writing good, wholesome, fake data for the sandbox.</em> However, sandbox data, unlike unit tests, solves three problems at once:</p>

<ul>
<li>It lets your <em>consumers</em> test in a predictable way without making real transactions;</li>
<li>It enables you to record specific data conditions from The Real World™, increasing your understanding of that data;</li>
<li>It lets you test internal business logic independently from real data, fast.</li>
</ul>


<p><em>Snip!</em> I went into too much detail on sandbox data here, saving that for a future post.</p>

<h3>Loosen Isolation</h3>

<p>Isolation between test runs is really important. If the order you run tests in can ever alter the results, then you have shared state, and you can no longer trust that your tests are testing the same thing each time they are run.</p>

<p>Usually, in unit testing, we rely on the test runner to respect directives in code that enforce isolation in this way. In NUnit with C# we use attributes for <a href="http://www.nunit.org/index.php?p=setup&amp;r=2.2.10">set-up</a> and <a href="http://www.nunit.org/index.php?p=teardown&amp;r=2.2.10">tear-down</a>, for example. We usually throw away the entire object graph before each test. <em>Sometimes before each assertion.</em> For acceptance testing, where spinning up your test subject tends to take longer, it can be helpful to bend the rules somewhat.</p>

<p>In an ideal world, each test run would begin on a new, freshly installed OS, thus eliminating any possibility of differing test results due to environmental issues—the system clock and network state notwithstanding. For unit testing we rarely if ever take this extremist approach. More usual is to rely on the test runner to enforce &ldquo;similar enough&rdquo; initial conditions each time a test is run–commonly relying on the developer to write this set-up and tear-down code correctly.</p>

<p>In acceptance testing it can be very beneficial for performance to loosen this one step further and re-use the same application instance (process) between test runs. Sandboxed data, especially if it is immutable, can help enormously to avoid the pitfalls of shared state. When running your acceptance tests against real data, where shared state is a real concern, you may discover interesting bugs that would otherwise have gone unnoticed if you were using only unit tests. <strong>Upon the discovery of issues with real data, you must implement the same failing condition in your sandbox data so that you don&rsquo;t accidentally introduce regressions later.</strong></p>

<p>The way we initially achieved this in our acceptance tests was by using the <code>TestFixtureSetup</code> attribute from NUnit to invoke the application, and run it to a point where it had generated an interesting result. Then, each &lsquo;test&rsquo; is in fact a single assertion on the state of the world at that point. Like this:</p>

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='csharp'><span class='line'><span class="na">[TestFixture]</span>
</span><span class='line'><span class="k">public</span> <span class="k">class</span> <span class="nc">my_acceptance_tests</span>
</span><span class='line'><span class="p">{</span>
</span><span class='line'><span class="na">    [TestFixtureSetUp]</span>
</span><span class='line'>    <span class="k">public</span> <span class="k">void</span> <span class="nf">do_stuff</span><span class="p">()</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="n">_result</span> <span class="p">=</span> <span class="n">InvokeTheApplicationsWithSomeGivenParams</span><span class="p">();</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'><span class="na">    [Test]</span>
</span><span class='line'>    <span class="k">public</span> <span class="k">void</span> <span class="nf">it_is_cool</span><span class="p">()</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="n">Assert</span><span class="p">.</span><span class="n">That</span><span class="p">(</span><span class="n">_result</span><span class="p">.</span><span class="n">Coolness</span><span class="p">,</span> <span class="n">Is</span><span class="p">.</span><span class="n">GreaterThan</span><span class="p">(</span><span class="m">1337</span><span class="p">));</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'><span class="na">    [Test]</span>
</span><span class='line'>    <span class="k">public</span> <span class="k">void</span> <span class="nf">it_has_2_bananas</span><span class="p">()</span>
</span><span class='line'>    <span class="p">{</span>
</span><span class='line'>        <span class="n">Assert</span><span class="p">.</span><span class="n">That</span><span class="p">(</span><span class="n">_result</span><span class="p">.</span><span class="n">Bananas</span><span class="p">,</span> <span class="n">Is</span><span class="p">.</span><span class="n">EqualTo</span><span class="p">(</span><span class="m">2</span><span class="p">));</span>
</span><span class='line'>    <span class="p">}</span>
</span><span class='line'>    <span class="p">.</span>
</span><span class='line'>    <span class="p">.</span>
</span><span class='line'>    <span class="p">.</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure>


<p>We eventually refined this to use constructors to set up the initial state, and did some other not-necessarily-normal things to coax our acceptance tests into a reasonably elegant suite. More on this in an upcoming post on sandbox data.</p>

<h2>Granularity</h2>

<p>Granularity, in this case, refers to the level of detail revealed by a failing test. When a <em>unit</em> test fails, if it&rsquo;s written correctly, you should immediately know which method in which class went wrong. Often, the call stack in the exception message will tell you exactly which line of code was at fault. You can immediately jump to the offending code.</p>

<p>With acceptance tests, when something goes wrong, it could be anywhere in your program. Any of the tens, hundreds, thousands, or even millions of lines of code in your program could be at fault. This is clearly less than ideal, however there are ways to mitigate the pain:</p>

<h3>Minimise Code, Maximise Seams</h3>

<p>Clearly, the fewer lines of code in your app, the easier it will be to hunt down obscure test failures. However some problems are too big to be solved with few lines of code. What then to do? One answer, which we are beginning to explore in a new project, may be to write many small programs, each of which solves only a small part of the problem domain. This approach is known as <a href="http://martinfowler.com/articles/microservices.html">microservices</a>, and certainly has its own complexities in threading together many small pieces at OS or network level. However, a microservices architecture has additional potential benefits tangential to its affinity with acceptance-only testing. I&rsquo;m planning a blog post on this subject soon, once we have more data.</p>

<h3>Debugfu</h3>

<p>This isn&rsquo;t really a way to make your tests more granular, but it can help mitigate the problems of low granularity in acceptance tests. If the test won&rsquo;t tell you which line of code went wrong, attach a debugger to find out! If you&rsquo;re using Visual Studio, you are blessed with a best-of-breed debugger. Use it, trace through the execution and try to spot what went wrong. Use bookmarks and breakpoints to index your code. Does one area of code cause problems time and time again? There is probably something wrong with it, see if it can be rewritten more clearly. Users of  IntelliJ IDEA, Eclipse, or a myriad other IDEs, will also have access to usable debuggers.</p>

<h3>Sandbox Data (Again)</h3>

<p>Sandbox data allows you to run your tests against very specific data conditions. Often your code will have a different execution path depending on data, and this is really valuable knowledge when trying to nail down the cause of a test failure. Sandbox data, that can be selected by your tests, will improve the percieved granularity of such, by limiting the potential execution paths.</p>

<h2>Benefits</h2>

<p>I mentioned a few of the benefits of moving to acceptance-only testing in my last post. However, a few more have come to light since then which are worth mentioning.</p>

<h3>Acceptance Tests Are Language-Agnostic</h3>

<p>After writing v1 of our API, and acceptance-testing the living daylights out of it, we realised that we were still probably maintaining too much code, this time in the application itself. We decided to port the whole thing to JavaScript using Node to see what it would be like.</p>

<p>It worked, and <em>we didn&rsquo;t have to touch a single test,</em> even though those tests were written in C#, and the application was in JavaScript.</p>

<p>Just imagine the overhead of porting hundreds of unit tests over to a different language, along with the application. If that had been a requirement of our experiment, it would not have happened, and we would not have learned what we did. <em>(In the end we did keep using the C# implementation in production, but the speedy rewrite was still a valuable learning aid.)</em></p>

<h3>Acceptance Tests Behave Exactly Like Your Users</h3>

<p>When an acceptance-level test passes, you can be confident that a whole user journey using your application is working correctly. That&rsquo;s a huge win. When one fails, you can be pretty confident that something important to your users is not working properly and needs attention. Also, very valuable knowledge. This contrasts somewhat with unit-level tests that might tell you something internal is awry with your application, but its real impact to consumers will still often be unknown. Should you fix it? If there are multiple failures, which are the most important? Unit tests will rarely answer these questions for you.</p>

<p>Of course, you will fix it, or else be unable to confidently release your software–but surely, at times, you will be fixing something that does not matter, or is no longer relevant to your consumers. Unit tests in this way can encourage code rot, making it very difficult to unpick dependencies that are no longer needed. With acceptance tests, you only need to unpick the dependencies in your application, not also in the tests.</p>

<h2>Synergy</h2>

<p>Much of this is new to me, and certainly isn&rsquo;t without contention. However, the problems with acceptance-only testing, and specifically the solutions to those problems, indicate certain synergistic practices that may improve its viability:</p>

<h3>Thin Layers</h3>

<p>The project we first tried moving to acceptance-only testing on was a very thin layer–a facade over a collection of internal services. It had minimal business logic, and thus few potential execution paths. This certainly allowed us to keep the number of acceptance tests lower than might be expected for a large, complex application, that might branch off into numerous modes of operation. Of course one should probably try to minimise the <a href="http://en.wikipedia.org/wiki/Cyclomatic_complexity">cyclomatic complexity</a> of a code-base anyway, for one&rsquo;s own sanity.</p>

<h3>Statelessness</h3>

<p>If your system maintains a lot of state, acceptance testing can be much harder. This is because you will likely have to set-up much of that state for each test, increasing both developer effort and execution time. Both of which are bad. However, my new favourite thing, immutable sandbox data, may well be your friend in this case.</p>

<h3>Microservices</h3>

<p>We have only just begun experimenting with microservices ourselves, however I think it stands to reason that if each application is small overall, then the number of test cases for each will be small as well. This means the whole suite will run faster, and give you more granular feedback. I differentiate microservices from &lsquo;thin layers&rsquo; in that a microservice may well do data access, input parsing, validation, HTTP handling, and a bit of business logic–but over a very narrow domain–i.e. a thin vertical. A thin layer, on the other hand will perform only one kind of function–e.g. HTTP handling–but across multiple facets of the system. If thin layers are the lines of latitude, then microservices can be sections of the lines of longitude.</p>

<h2>Too Short; Read Also</h2>

<p>I hope this article has been a little more useful than the previous one. I have tried to explain more specifically what we actually did, from end-to-end, and how we overcame problems along the way. However, I&rsquo;ve really only scratched the surface. I will hopefully get the chance flesh out some of the ideas here in the coming months. In the mean time, there are plenty of <a href="http://www.shino.de/2012/07/02/atdd-by-example/">books</a> and <a href="http://jonkruger.com/blog/2012/02/20/when-acceptance-tests-are-better-than-unit-tests/">blog posts</a> on the subject of acceptance testing. In addition, Martin Fowler has writen <a href="http://martinfowler.com/articles/microservices.html">a great primer on microservices</a> that&rsquo;s really got me thinking about their utility alongside acceptance-only testing and sandbox data.</p>

<p>Thanks for reading :)</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/19/continuous-delivery-automating-deployment-visibility/">Continuous Delivery: Automating Deployment Visibility</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-05-19T17:17:40+01:00" pubdate data-updated="true">May 19<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In our continued effort to drive towards a service oriented architecture each of our teams are continuously improving their deployment processes. Recently our team has focussed on automating as much as possible, putting as much into chat as we can and improving our logging/metrics.</p>

<p>The image below shows at a high level what our teams current deployment pipeline looks like and this post will attempt to summarise some recent changes that have allowed us to automate visibility.</p>

<p><img src="/images/posts/release-pipeline.png" width="900" height="350" title="image" alt="images"></p>

<h2>Kicking off a deployment</h2>

<p>I wrote previously that we started <a href="http://tech.opentable.co.uk/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot/">using chatops</a> to increase visibility operationally. Hubot is central to this and we wrote a small script to kick off deployments within <a href="https://www.hipchat.com/">Hipchat</a></p>

<p><img src="/images/posts/hubot-deploy-restaurant.png" width="350" height="350" title="image" alt="images"></p>

<p>We have two TeamCity instances. The first is used as a build and deployment system to our pre-production servers. The second is used as a deployment system to our production servers. Artifacts from our non-production instance are stored in <a href="http://www.jfrog.com/home/v_artifactory_opensource_overview">Artifactory</a> and our production deployment makes an API call to non-production TeamCity to ask for the last successfully pinned build. Pinning a build only occurs when we&rsquo;re happy that the build is ready to be shipped (passing unit and acceptance tests). The above Hubot command will pin the non-production build, given that the build succeeded, and add a build to the queue in production.</p>

<p>To configure Hubot to do this we wrote a command to setup aliases providing the build id of the build to pin (non-production) and the build id of the build to kick off (production).</p>

<p><img src="/images/posts/hubot-deploy-alias.png" width="350" height="350" title="image" alt="images"></p>

<h2>Deployment visibility</h2>

<p>Our production deployments must be auditable and it&rsquo;s important that we know what went out with each release and keep a log of this for our Risk Management team. We do this by creating a ticket in <a href="https://www.atlassian.com/software/jira">JIRA</a>, internally known as a CCB, and this gives us a central store of all deployments by all teams.</p>

<p>In the past these tickets were manually created for each release. We soon realised that this was something we could automate. To do so we created a new &ldquo;deployment-info&rdquo; endpoint for our service. This simply contains the SHA of the last commit released along with a time stamp. The first step of our production deploy is to query this endpoint and then using the Github API to get all the commits since that last SHA. These commits are then logged to JIRA to create a CCB ticket using the JIRA API. Each of these steps are automated from TeamCity using grunt tasks. You can find information of the grunt tasks on github as follows:</p>

<ul>
<li><a href="https://github.com/opentable/grunt-ccb">https://github.com/opentable/grunt-ccb</a></li>
<li><a href="https://github.com/opentable/grunt-github-manifest">https://github.com/opentable/grunt-github-manifest</a></li>
<li><a href="https://github.com/opentable/grunt-package-github">https://github.com/opentable/grunt-package-github</a></li>
</ul>


<h2>Build Notifications to Kibana</h2>

<p>Once we have a CCB we fire a start and end event from TeamCity containing the build number to Redis which is then piped into <a href="http://logstash.net/">Logstash</a>. An event is sent before and after deploying the code to all nodes. This is hugely beneficial because it allows us to plot releases against our graphs in <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a>. Kibana recently added a new feature called Markers. Essentially these are tags that display at the bottom of a graph.</p>

<p><img src="/images/posts/kibana-tags.png" width="350" height="350" title="image" alt="images"></p>

<p>You can find information on this Markers module on github &ndash; <a href="https://github.com/opentable/grunt-deployment-logger">https://github.com/opentable/grunt-deployment-logger</a></p>

<p>This has already proved incredibly useful for the team and has allowed us to visually correlate issues or changes in key metrics (response times/requests per second) to releases. The following image shows how these look over several graphs.</p>

<p><img src="/images/posts/kibana-dashboard.png" width="900" height="900" title="image" alt="images"></p>

<h2>Hipchat build complete notification</h2>

<p><img src="/images/posts/hubot-notification.png" width="350" height="200" title="image" alt="images"></p>

<p>Once our deployment pipeline has completed we send a notification to our teams room in Hipchat (as a final step in TeamCity) to inform the team that the release has completed. It&rsquo;s great to see a deployment start and end in chat. Having a central log of key operations in our team means that we don&rsquo;t have to go and find information when it&rsquo;s baked into chat.</p>

<h2>Conclusion</h2>

<p>We&rsquo;ve come along way with improving our pipeline and automating visibility. Our team is made up of 4 members; 3 in the office and 1 remote. The ultimate goal is to improve speed of deployment and visibility of events not just within the team but for everyone who is interested. Equally we want to continue to open source by as much as possible, allowing us to share our process with teams inside and outside of our organization. We can release code anywhere in the world and the process is completely centralised in chat. We want to continue to move fast and fix faster.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/15/managing-windows-features-with-puppet/">Managing Windows Features with Puppet</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-05-15T16:12:38+01:00" pubdate data-updated="true">May 15<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Back in June 2013, I wrote about <a href="http://tech.opentable.co.uk/blog/2013/06/14/windows-feature-management-with-powershell/">Windows Feature Management with PowerShell</a>. We have since released a Puppet module that will do this for us. We originally wrote PowerShell:</p>

<pre><code>Import-Module ServerManager
Add-WindowsFeature Web-Asp-Net
</code></pre>

<p>The Puppet module now wraps this code as follows:</p>

<pre><code>windowsfeature { 'Web-Asp-Net': }
</code></pre>

<p>The declaration windowsfeature is a specific Puppet type called a <a href="http://docs.puppetlabs.com/learning/definedtypes.html">define</a>. In developer terms, this is the equivalent of a helper method that can be reused. We can also make sure that Windows Features are <em>not</em> installed on the server as follows:</p>

<pre><code>windowsfeature { 'Telnet-Server': 
  ensure =&gt; absent 
}
</code></pre>

<p>In the backing code for the module, we do a check before we install / uninstall any windows features. This means that we will only make the changes we really need to. This ensures idempotency of the script. By using this class, we can build up a list of what features a server should have enabled / installed on it. As example manifest would look as follows:</p>

<pre><code>class my_windows_features {
  windowsfeature { 'Web-Asp-Net': }
  windowsfeature { 'Web-Net-Ext': }
  windowsfeature { 'Web-ISAPI-Ext': }
  windowsfeature { 'Web-ISAPI-Filter': }
  windowsfeature { 'Web-Mgmt-Tools': }
  windowsfeature { 'Web-Mgmt-Console': }
  windowsfeature { 'Telnet-Server': ensure =&gt; absent }
}
</code></pre>

<p>The server will have it&rsquo;s shipping list of Windows Features checked every 30 minutes by Puppet. We are sure that any changes encountered during that time will be applied as expected.  You can find more about our WindowsFeature module on the <a href="http://github.com/opentable/puppet-windowsfeature">github repo</a>. If you want to use the module, then you can install it using the Puppet Module tool via the <a href="http://forge.puppetlabs.com/opentable/windowsfeature">Puppet Forge</a></p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/05/13/managing-windows-web-applications-with-puppet/">Managing Windows Web Applications with Puppet</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-05-13T15:25:16+01:00" pubdate data-updated="true">May 13<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>As part of our move towards a configuration management tool, we really wanted to start automating as much of our infrastructure as possible. This included our application configuration stack. IIS management is pretty easy with PowerShell. It would look something like this</p>

<pre><code>Import-Module WebAdministration
New-WebSite -Name "DemoSite" -Port 80 -IP * -PhysicalPath "c:\inetpub\wwwroot" -ApplicationPool "MyAppPool"
</code></pre>

<p>This would of course set up a website called &lsquo;DemoSite&rsquo; running on port 80 on the local machine. The cmdlets that come with PowerShell make this pretty easy. This is great if it is a one-off job to set up a site. We run our websites from a number of webservers, therefore, it would be silly to have to RDP into each webserver and run a script on it. This is why tools like Puppet, Chef, Ansible etc. exist. We needed a configuration management tool to do this work for us. It has a number of benefits:</p>

<ul>
<li>Orchestration</li>
<li>Idempotency</li>
<li>Makes sure that each server is configured in &lsquo;exactly&rsquo; the same way as no human intervention is needed</li>
<li>Developers can help the operations team by creating the scripts needed. This is great for collaboration between teams</li>
</ul>


<p>On investigating how we would do this with Puppet, we noticed that there were not many other people managing their site in this way. Therefore, we would have to turn our PowerShell scripts into Puppet modules to manage our system.</p>

<p>We have since created a Puppet module to manage IIS. To manage IIS with Puppet, we can now write the following code:</p>

<pre><code>iis::manage_site { 'DemoSite:
   site_path     =&gt; 'c:\inetpub\wwwroot',
   port          =&gt; '80',
   ip_address    =&gt; '*',
   app_pool      =&gt; 'MyAppPool'
}
</code></pre>

<p>This would produce <strong>exactly</strong> the same results as the code from above. But it has 1 difference. There are checks in the code behind this module that will mean the code will only execute when it is needed, i.e. when the site_path isn&rsquo;t correct or the app_pool isn&rsquo;t correct. This is idempotency. The script can be run again and again and again&hellip;.</p>

<p>To create an application binding, we used to do this in PowerShell:</p>

<pre><code>Import-Module WebAdministration
New-WebBinding -Name 'DemoSite' -Port '8080' -IPAddress '*'
</code></pre>

<p>This would set up an extra binding on port 8080 for the site, DemoSite. We replaced this code with our puppet equivalent:</p>

<pre><code>iis::manage_binding { 'DemoSite-8080':
  site_name   =&gt; 'DemoSite',
  protocol    =&gt; 'http',
  port        =&gt; '8080',
  ip_address  =&gt; '*',
}
</code></pre>

<p>To create a virtual application, we would write the PowerShell:</p>

<pre><code>Import-Module WebAdministration
New-WebApplication -Name 'VirtualApp' -Site 'DemoSite' -PhysicalPath 'c:\inetpub\wwwroot\MyVirtualApp' -ApplicationPool 'MyAppPool'
</code></pre>

<p>This will create a VirtualApp folder on the DemoSite, use the same application pool and then set the path of the folder. I can do the same thing in Puppet as follows:</p>

<pre><code>iis::manage_virtual_application {'VirtualApp':
  site_name   =&gt; 'DemoSite',
  site_path   =&gt; 'C:\inetpub\wwwroot\MyVirtualApplication',
  app_pool    =&gt; 'MyAppPool'
 }  
</code></pre>

<p>We can therefore, chain a manifest together that does all this for us in 1 go. It would look as follows:</p>

<pre><code>class mywebsite {
  iis::manage_app_pool {'MyAppPool':
    enable_32_bit           =&gt; true,
    managed_runtime_version =&gt; 'v4.0',
  } -&gt;

  iis::manage_site {'DemoSite':
    site_path   =&gt; 'C:\inetpub\wwwroot',
    port        =&gt; '80',
    ip_address  =&gt; '*',
    app_pool    =&gt; 'MyAppPool'
  } -&gt;

  iis::manage_virtual_application {'VirtualApp':
    site_name  =&gt; 'DemoSite',
    site_path  =&gt; 'C:\inetpub\wwwroot\MyVirtualApp',
    app_pool   =&gt; 'MyAppPool'
  } -&gt; 

  iis::manage_binding {'DemoSite-8080':
    site_name  =&gt; 'DemoSite',
    protocol   =&gt; 'http',
    port       =&gt; '8080',
    ip_address =&gt; '*'
  }
}
</code></pre>

<p>The module does more than just these tasks and I could give more and more examples of what we wrote, but you can find more about our IIS module on the <a href="http://github.com/opentable/puppet-iis">github repo</a>. If you want to use the module, then you can install it using the Puppet Module tool via the <a href="http://forge.puppetlabs.com/opentable/iis">Puppet forge</a>.</p>

<p>We love to hear feedback on things that the module should support. We like Pull Requests even more :)</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/29/remote-worker-notes-tools-and-setup/">Remote Worker Notes &ndash; Tools and Setup</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-29T10:56:49+01:00" pubdate data-updated="true">Apr 29<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>For the last couple months I&rsquo;ve been working remotely with our search team at OpenTable. Let me share our remote working setup and some rationale for our choices. I will shy away from judging how it&rsquo;s all worked out and leave that for subsequent blog posts.</p>

<h2>The deal</h2>

<p>I started as an on-site engineer and subsequently became a remote worker and my challenge was to make remote working as similar to on-site working as possible. Thus it is natural that <strong>I work the same working hours</strong> as my colleagues in the office (despite the time difference) and I take <strong>public holidays</strong> at the same time that there are bank holidays in UK (even though in Poland they are different).</p>

<p>We also decided that every two months I will <strong>visit the office for a week</strong> as well as arranging visits to coincide with other office guests (like contractors, overseas colleagues and so on). All this is to try to keep me as close to the team as possible and disrupt the normal work-flow as little as possible.</p>

<h2>The workplace setup</h2>

<p>Once home in Poland I had to carefully consider my options for a workplace. I tried three obvious possibilities:</p>

<ul>
<li>Dining room in my own one bedroom flat</li>
<li>Co-working space</li>
<li>Private room in my parents&#8217; house</li>
</ul>


<p>The <strong>dining room</strong> worked pretty well, for a time&hellip;  I would &lsquo;arrive&rsquo; really early in the office (just after taking a shower), and then be &lsquo;home&rsquo; straight after the laptop lid was closed. My wife kindly respected that I am focused at work and distracted me only to share lunch (or to kick me out of house to bring the lunch in).</p>

<p>A couple of things made it only a short-term solution &ndash; the first being my back complaining about working on a chair that&rsquo;s nice for a dinner, but awful for working on computer. Then I realised that I really miss a second monitor and separate keyboard &ndash; I guess anybody who coded in the middle of summer on a laptop knows the pain of a hot computer under your fingers.  Finally, my child was born, and that was it &ndash; a crying child in the same room when you are trying to pair with somebody is a deal-breaker.</p>

<p>In the meantime I evaluated a local <strong>co-working space</strong>. The problem was that it was quiet, <em>really quiet</em>, up to the point that I was embarrassed to pair with somebody remotely. The way we work fluctuates over time; there are weeks when I do stuff alone and in silence, there are weeks when I spend whole days on conversations or pairing. I just cannot be too quiet. For that matter I tried also a coffee shop, but it failed me for exactly the same reasons as the dining room table.</p>

<p><strong>My little own office is currently the winner</strong> and the only way it could be better is if behind the closed doors there was my house, not my parents&#8217; one. But even this one is pretty good &ndash; I have access to an always full fridge and a decent coffee machine. I can bring my family with me and it is not awkward.</p>

<p>Finally, I can share a lunch with whoever is at home. With a decent chair and desk I cannot complain about anything. Actually, I found that even after work if we stay for a coffee or dinner I am tempted to pop in into the office and look at our dashboards or chat room.</p>

<h2>The tools setup</h2>

<p>To facilitate continuous communication <em>you first need a good attitude, and then good tools</em>. With the attitude two things matter the most:</p>

<ul>
<li>A quick response</li>
<li>Patience</li>
</ul>


<p>By nature all remote communication channels are asynchronous, which is hugely different to face-to-face communication. It is much easier this way for an important question to go unnoticed by you. Conversely it is easy to get annoyed that somebody is not responding, when in fact they may just be talking to the person at the desk next to them.</p>

<p>Our primary tool is <strong>Google Hangout</strong>. It works quite well for a team, being nice for stand-ups, though it fails miserably as a constant communication tool. There are two reasons for that; the first is that its messages often get lost and the second is it makes our MacBook radiators spin like crazy when open for too long. <em>Unfortunately we had to rule out an online window to the office after a couple of days trying</em>.</p>

<p>We also extensively use <a href="http://www.hipchat.com"><strong>HipChat</strong></a>, and that brought one of best breakthroughs in our communication patterns. It has a few features which make it great as a team collaboration tool:</p>

<ol>
<li>You get messages whether you pay attention or not</li>
<li>You can interrupt other people by mentioning them</li>
<li>It makes sharing links and images really easy</li>
<li>When you type you can collaborate with multiple persons at the same time</li>
</ol>


<p>I also mentioned that pairing is a big part of our daily work. I have to admit that I haven&rsquo;t yet found a tool that would make the experience seamless. There is always a bit of delay on the line, or shortcuts not working, or problems of mismatch between screen resolutions that would never appear when pairing on the same machine.</p>

<p>On the other hand when you have a keyboard just under your fingers it is much easier to swap sides and while discussing actually write code constructs. For this we use combination of <strong>Hangout</strong> screen share feature (for quick debug help) and <strong>RDP</strong>.</p>

<p>One last tool that I like to use quite extensively is <strong>Google Drive</strong> as a shared whiteboard. It has proved to be especially useful for retrospectives or architecture discussion. The nice thing about the Drive draw tool is that it forces you to use shapes, lines and text. Those three primitives allow for an unlimited number of possible drawings. Being vector means you can easily move stuff around (nothing will fall on the floor), you never run out of space and you have an immediate and lasting backup of your board. I also found that a distributed retrospective facilitates a more equal participation of those attending.</p>

<p>The best feature of the most of those tools is that they allow to <strong>easily save and store the outcomes of your conversations.</strong> Hangouts can be recorded, HipChat naturally creates history of chats and Google Drive&rsquo;s drawings become a persistent track of team discussions.</p>

<h2>Get Involved</h2>

<p>If you have a different setup or tools that help with having a distributed team member, or you want to share your experiences be sure to tweet us <a href="http://www.twitter.com/OpenTableTechUK">@OpenTableTechUK</a> or me personally <a href="http://www.twitter.com/mbazydlo">@mbazydlo</a>.​</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/28/effective-prioritisation/">Effective prioritisation</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-28T14:51:51+01:00" pubdate data-updated="true">Apr 28<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Prioritisation is a huge part of modern life.</p>

<p>No.</p>

<p>Prioritisation is an ESSENTIAL part of modern life.</p>

<p>I&rsquo;m not talking solely about Agile here either. Yes we plan stories, yes we prioritise them to get them into the backlog but I&rsquo;m aiming at a higher level. Life lessons that are useful to all of us.</p>

<p>How do we know what to do next? Perhaps you lean on your gut feeling to order your to-do list. Maybe it is real-time cost and budget implications that influence your priorities or does it come down to who shouts loudest?</p>

<h2>Why is prioritisation important?</h2>

<p>The experts would have us believe we are living through the Digital Age, also know as the Computer Age. Increasingly this has been more accurately redefined as the Information Age.</p>

<p>The society we live in today truly bombardes us with information. Whether it be TV, mobile, emails, blog posts, status updates, tweets, likes and check-in&rsquo;s we are rarely unconnected to modern day life and have a vast amount of data and information at our disposal.</p>

<p>This flood of information leads to a problem: we often confuse the importance of everyday tasks and activities.</p>

<p>For instance how many times have we said &ldquo;I don&rsquo;t have time to read&rdquo; or &ldquo;I&rsquo;d love to be able to go to the gym more&rdquo;. Perhaps a more honest way to say this is &ldquo;reading is not a high priority for me&rdquo; or &ldquo;I prioritise other activities higher than going to the gym&rdquo;.</p>

<p>The decision not to spend time reading or working-out in the gym means you are spending time on other activities you consider more important. Consciously or unconsciously, we prioritise our life. The key is to prioritise effectively.</p>

<h2>The Focus Quadrant</h2>

<p>I&rsquo;ve been introduced to a framework that helped me to grab this idea of effective prioritisation and certainly opened my eyes to some of the mistakes I made previously.</p>

<p>Stephen Covey, a sadly deceased educator, author and businessman, designed a matrix called The Focus Quadrant that categorises activities and helps us deal with the struggle of prioritisation by clearly outlining the conflicts involved.</p>

<p><img src="http://www.chowamigo.co.uk/images/focus.jpg" alt="image" /></p>

<table style="font-size: 80%;margin-bottom:20px;">
    <tr>
        <th style="padding:3px;"></th>
        <th style="padding:3px;"><b>Description</b></th>
        <th style="padding:3px;"><b>Examples</b></th>
    </tr>
<tr style="background-color: #E5E5E5">
    <td style="white-space:nowrap;padding:3px;vertical-align:top;">A. Important/Urgent</td>
    <td style="padding:3px;vertical-align:top;">Tasks that are necessary usually deadline driven or time sensitive. This usually this means panic and problems.</td>
    <td style="padding:3px;vertical-align:top;">Fixing the server crash, firefighting a site outage, entering expenses before a deadline</td>
</tr>
<tr>
    <td style="white-space:nowrap;padding:3px;vertical-align:top;">B. Important/Not urgent</td>
    <td style="padding:3px;vertical-align:top;">Activities in direct alignment to your goals. Typically these tasks involve planning ahead. Prevention rather than cure.</td>
    <td style="padding:3px;vertical-align:top;">Relationship building, researching your next API, learning a new coding language, taking time out to exercise</td>
</tr>
<tr style="background-color: #E5E5E5">
    <td style="white-space:nowrap;padding:3px;vertical-align:top;">C. Not important/Urgent</td>
    <td style="padding:3px;vertical-align:top;">Activities that seem important but in reality are not. It&#8217;s thought these tasks will make you popular as you are responding to requests for your time.</td>
    <td style="padding:3px;vertical-align:top;">Going to a meeting, answering the phone, replying instantly on IMs or email</td>
</tr>
<tr>
    <td style="white-space:nowrap;padding:3px;vertical-align:top;">D. Not important/Not urgent</td>
    <td style="padding:3px;vertical-align:top;">While it would seem pleasant enough to live here permentantly, in reality these activities are time wasters that should be eliminated as much as possible.</td>
    <td style="padding:3px;vertical-align:top;">Playing video games, surfing the net, getting a tea or coffee, flicking through the magazine on your desk</td>
</tr>
</table>


<h2>How does this help?</h2>

<p>Covey&rsquo;s point is that if we have a long list of items and activities and we are struggling to prioritise them, spending 20 minutes writing the list down and assigning each task A, B, C or D can help clarify importance.</p>

<p>For me the interesting part is that we should not be spending too much time in A. This goes against our natural reaction that A is good &ndash; probably the fault of our education system. In fact category A is considered beneficial only in the short term. We are not overly productive even if we spend a lot of time on A category tasks. We really want to shift our attention to more longer term plans.</p>

<p>Category B on the other hand is considered the sweet spot. This is where we are doing our best work. It is where we will be ticking off a lot of our important long term goals. Yes we all have different goals but the majority of the goals we want to achieve fit into B &ndash; non urgent and important.</p>

<p>For category C there is a great saying &lsquo;The quadrant of deception&rsquo;. These are tasks we THINK are urgent but in reality they are not. Considered to be a distraction, get used to politely saying no to any activities in the C category. We should look to regain control of our time and not be forced to respond when it suits others.</p>

<p>Finally category D is the worst place to spend your time. It&rsquo;s okay to roam across each of these four quadrants but if we identify we are spending too much time in quadrant D we need to step back, reassess and pick up a task more aligned to our goals &ndash; ideally something from category B.</p>

<h2>Money => mouth</h2>

<p>While this is useful in theory, we of course need to be realistic. You should factor in time for impromptu interruptions &ndash; saying no to every meeting isn&rsquo;t possible and this isn&rsquo;t an excuse to push back difficult tasks. However, the next time your to-do list gets unmanageable, take time out to clarify your priorities.</p>

<p>Now my current my to-do list looks like this&hellip;</p>

<ol>
<li>Investigate a blip in our API performance <strong><code>(A)</code></strong></li>
<li>Scope out our next API <strong><code>(B)</code></strong></li>
<li>Improve the way our current API deploys to production <strong><code>(B)</code></strong></li>
<li>Send out a team update to interested parties <strong><code>(C)</code></strong></li>
<li>Go and grab a cup of tea :) <strong><code>(D)</code></strong></li>
</ol>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2014/04/16/look-ma-no-unit-tests/">Look ma, no unit tests!</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-04-16T17:00:00+01:00" pubdate data-updated="true">Apr 16<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p><em><strong>UPDATE:</strong> I&rsquo;ve written a <a href="/blog/2014/05/19/acceptance-now">follow-up to this post</a> with a bit more detail into how we made acceptance-only testing work in practice.</em></p>

<p>At OpenTable we strive to deliver change as quickly and correctly as possible. To do this effectively we are always looking for <a href="/blog/2014/02/28/api-benchmark/">new</a> <a href="/blog/2013/08/16/grunt-plus-vagrant-equals-acceptance-test-heaven/">tools</a> <a href="/blog/2014/04/07/upgrading-puppet-with-puppet/">and</a> <a href="/blog/2014/02/10/the-adoption-of-configuration-management/">methods</a> that allow us, the developers, to respond quickly and accurately to changing requirements and environments.</p>

<p>There are a number of practices that we already make use of, helping us to be the most effective team I&rsquo;ve ever worked in:</p>

<ul>
<li>We operate in small teams who each own <em>most</em> of their own vertical.</li>
<li>We use continuous delivery to ship code to production within minutes.</li>
<li>We have a high degree of high-quality test coverage.</li>
<li>We are getting better and better at monitoring All The Things.</li>
<li><a href="/blog/2013/11/22/beginning-a-journey-to-chatops-with-hubot/">We use Chatops</a>, so communication is central to our work, and keeps remote workers/teams in the loop.</li>
</ul>


<p>All of the above are truly empowering for the dev team, and are conducive to an amazingly stress-free working environment. However, these practices only address the infrastructure, culture, and ceremony surrounding our work. What if there was something else? Something about the way we write the code itself, that could increase our velocity yet further, without compromising our integrity&hellip;</p>

<blockquote><p>There are a number of practices that we already make use of, helping us to be the most effective team I&rsquo;ve ever worked in&hellip; What if there was something else?</p></blockquote>

<p>Well, on a recent project, we found one such way: <em><em>we decided to delete all of the unit and integration tests</em></em>.</p>

<p><em>What?! Are we quite mad?</em> You may be thinking&hellip; Well, it took me a little time to get used to this idea as well, but read on and you&rsquo;ll see that it was actually the most sane thing we could have possibly done  .</p>

<h2>Survival of the testedest</h2>

<p>In the beginning, the project had 100% unit test coverage, there were no external dependencies, and the world was Good.</p>

<p>Soon afterwards, a tall shadow appeared in the glorious unit-tested sunset. External dependencies had arrived. Like good little developers we added integration tests. It hurt, our codebase grew, we had occasional false-failures, but we were travelling the path well trodden. We had evaded the First Menace, and surrounded ourselves with heavy armour, we were safe. Things seemed to be Good.</p>

<p><em>Meanwhile&hellip;</em></p>

<p>We realised that some of the things that would be important to our consumers were still not covered by our tests. Things like actual HTTP responses, serialisation, and the like. These are things that don&rsquo;t always need to be tested explicitly, but since this was a third-party-developer-facing system, we really wanted to be sure that the interface worked exactly as we wanted, HTTP headers, character encoding, date formatting, the lot.</p>

<p>So, playing the role of our consumers, we engineered high-level acceptance tests, behaving byte-for-byte as we expected our customers to do.</p>

<p>Now, with the triple-action protection of three layers of tests, we felt our project was the most minty-fresh piece of haute engineering we had ever laid keyboards on.</p>

<p>We were wrong.</p>

<h2>Tests, tests, tests, duplication.</h2>

<p>Up to this point, we had operated in a near-vacuum. That was fine, we had been working quickly to implement a sub-set of an existing and well-used API, so we knew which were the most important features that needed porting. We continued, largely happy with our creation, for some time.</p>

<p><em>Then, gazing up from the receding tide of the third trimester were the hungry eyes of the Second Menace. Our users were upon us!</em></p>

<p>Our early adopters were great, giving us a lot of helpful feedback and helping us shape the API into a genuinely usable v1. However, responding to this change required a greater degree of flexibility in the code than we had required up to this point. Our triple-chocolate-crunch of pithy tests was starting to really slow us down, and rot our teeth. The main reason for this: duplication.</p>

<p>We had tried from the start to avoid any duplication in our tests, but this was all but impossible to achieve. You just can&rsquo;t test an API call end-to-end in an acceptance-test style, without inadvertently testing all of the underlying logic for that call. Code which was already covered by unit tests, and often integration tests as well. Therefore each move we made came with the burden of updating multiple tests. Often materially very similar tests, but written to test a different layer of the same cake. We were between an immovable monolith and a very heavy boulder&mdash;and had a hoarde of features we still wanted to smash, who were freely bounding over the mountain tops, and out of reach.</p>

<p>It was time to cut ourselves free.</p>

<h2>Ripping off the plaster</h2>

<p>The idea that we might not need all these layers of tests was first mooted by fellow OpenTable engineer Arnold Zokas. My initial reaction was one of slight incredulity. Delete all those tests that we&rsquo;ve so carefully caressed and cajoled into a thing of beauty?! Strip off the armour?! I wasn&rsquo;t immediately convinced. However, the pain of implementing new features was starting to burn, so I was interested.</p>

<blockquote><p>I wasn&rsquo;t immediately convinced.</p></blockquote>

<p>We talked about it&mdash;what was necessary about the unit tests? What was their real worth? We had to test many of those things from the outside-in anyway, with the acceptance tests, so why test them twice? The logic started to stack up. I was convinced this was the right thing to do.</p>

<p>Take a deep breath. <em>RIP!</em> Aah, there, done.</p>

<p>There was a little bleeding, some gaps in our acceptance tests that had to be filled, some complex set-up logic from the integration tests that had to be ported to work with the acceptance tests. A few days&#8217; worth of cleanup and patching in the background, and&hellip; tentatively&hellip; we were done.</p>

<p>For me at least, this was a bold move. But it shouldn&rsquo;t have seemed so, we knew all of our endpoints were acceptance-tested, including every supported API call. My primary worry was how we were going to nail down the exact cause of bugs with no code-level testing. This turned out to be nowhere near as bad as I expected.</p>

<blockquote><p>We knew all of our endpoints were acceptance-tested, including every supported API call.</p></blockquote>

<h2>What just happened?</h2>

<p>I like to visualise this as if we were building <a href="http://en.wikipedia.org/wiki/Gateway_Arch">a giant arch</a>. At first, you build a temporary structure with scaffolding (the unit and integration tests). As time goes on you construct a hardened permanent structure (the software). On top of the software, you layer your <a href="http://en.wikipedia.org/wiki/Structural_health_monitoring">structural integrity monitors</a> (acceptance tests). Eventually, there is no need for the scaffolding any more; the structure is self-supporting, and future modifications can rely on this&mdash;time to punch out the middle!</p>

<p><em>Of course, there are other considerations, like logging, monitoring, and providing sandbox data, which all contributed to making this feasable&mdash;but that&rsquo;s for another post.</em></p>

<h2>Was it worth it?</h2>

<p>Unequivocally, yes. Since making this decision, we have been unhindered by our tests, and they are back to being a much loved part of the project. We have had no problems that would have been caught by unit tests, and we can still do TDD with our acceptance tests. In addition, I think removing the crutch of unit tests may have improved our discipline somewhat: <em>it keeps us thinking in the context of the end-user at all times, so we never spend time working on a feature that isn&rsquo;t directly useful to our consumers.</em></p>

<blockquote><p>It keeps us thinking in the context of the end-user at all times, so we never spend time working on a feature that isn&rsquo;t directly useful to our consumers.</p></blockquote>

<h2>YMWMCV</h2>

<p>Of course, every project is unique (just like every other project), so <em>your mileage will most certainly vary.</em> We were working on a stateless facade over a small but crucial subset of the business&mdash;making reservations. For relatively small, stateless projects, this approach has worked brilliantly. However, when things do go wrong at development time, they could be at any layer in the stack, and you often need to attach a debugger to find out what happened. This is less than ideal, but in our case was a very cost-effective compromise.</p>

<p>The upshot, for me at least, is that you shouldn&rsquo;t be afraid to shirk convention when the project demands it. By really analysing what each part of your project is doing, you can cut the cruft, helping you move faster <em>without</em> breaking stuff.</p>

<blockquote><p>By really analysing what each part of your project is doing, you can cut the cruft, helping you move faster <em>without</em> breaking stuff.</p></blockquote>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/4/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/2/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/04/27/opencomponents-microservices-in-the-front-end-world/">OpenComponents - microservices in the front-end world</a>
      </li>
    
      <li class="post">
        <a href="/blog/2016/01/07/react-testing/">Testing React Components</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/05/06/puppet-community/">Puppet-Community</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/05/the-dns-abc/">The DNS ABC</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/16/hapi-js-and-sigterm/">Hapi.js and SIGTERM</a>
      </li>
    
  </ul>
</section>
<section>
	<a class="twitter-timeline" data-dnt="true" href="https://twitter.com/opentabletechuk"  data-widget-id="351711375858466817">Tweets by @opentabletechuk</a>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
</section>
<section>
  <h1>GitHub Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/opentable">@opentable</a> on GitHub
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'opentable',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>Copyright &copy; 2016 - OpenTable</p></footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
