<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: REST | OpenTable Tech UK Blog]]></title>
  <link href="http://tech.opentable.co.uk/blog/categories/rest/atom.xml" rel="self"/>
  <link href="http://tech.opentable.co.uk/"/>
  <updated>2015-03-05T14:36:34+00:00</updated>
  <id>http://tech.opentable.co.uk/</id>
  <author>
    <name><![CDATA[OpenTable]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A Beginner's guide to REST services]]></title>
    <link href="http://tech.opentable.co.uk/blog/2015/02/02/a-beginners-guide-to-rest-services/"/>
    <updated>2015-02-02T11:53:25+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2015/02/02/a-beginners-guide-to-rest-services</id>
    <content type="html"><![CDATA[<h2>Why this post?</h2>

<p>As a junior, I always find it easier to just sit and write code than actually stop to think about the theoretical basis that lie under the applications I work on. <strong>REST</strong>  is one of those terms I heard a lot about, so I decided to try to sum up what it means and how it affects the choices we make everyday as software engineers.</p>

<h2>Introduction to REST</h2>

<p>REST stands for Representational State Transfer, and it can be defined as an architectural style used to build Web Services that are lightweight, maintainable, and scalable. A service that is designed by REST principles can be called a <strong>RESTful service</strong>.</p>

<p>It has been described first in 2000 by Roy Fielding, in a <a href="http://www.ics.uci.edu/~fielding/pubs/webarch_icse2000.pdf">dissertation</a> called &ldquo;Architectural Styles and the Design of Network-based Software Architectures&rdquo;. The basic idea was to describe the interactions between the components of a distributed system, putting constraints on them and emphasizing the importance of an uniform interface, that is abstracted from the single components.</p>

<p>REST is often applied to the design and development of web services, which is the scenario I&rsquo;ll try to address in this post.</p>

<p>The purpose of a web service can be summed up as follows: it exposes <strong>resources</strong> to a <strong>client</strong> so that it can have access to them (examples of typical resources include pictures, video files, web pages and business data).</p>

<p>Common features of a service that is built in a REST style are:</p>

<ul>
<li>Representations</li>
<li>Messages</li>
<li>URIs</li>
<li>Uniform Interface</li>
<li>Statelessness</li>
<li>Links between resources</li>
<li>Caching</li>
</ul>


<h2>Representations &ndash; what are they?</h2>

<p>REST style does not put a constraint into the way resources are represented, as long as their format is understandable by the client.</p>

<p>Good examples of data formats in which a resource could be returned from a service are <a href="http://www.json.org/"><strong>JSON</strong></a> (JavaScript Object Notation, which nowadays is the coolest one) and <a href="http://www.w3.org/XML/"><strong>XML</strong></a> (Extensible Markup Language, used for more complex data structures). Say for instance a REST service has to expose the data related to a song, with its attributes. A way of doing it in JSON could be:</p>

<p>```json
{</p>

<pre><code>"ID": 1,
"title": "(You gotta) Fight for your right (To party)",
"artist": "Beastie Boys",
"album": "Licensed To Ill",
"year": 1986,
"genre": "Hip-Hop"
</code></pre>

<p>}
```</p>

<p>Easy, huh?</p>

<p>Anyway, a service can represent a resource in a number of ways at the same time, leaving the client to choose which one is better suited for its needs. The important thing is that there is agreement on what format to send/expect.</p>

<p>The format that the client needs will be part of the <strong>request</strong> sent by the client.</p>

<p>The resource will be eventually sent by the service as part of what we call a <strong>response</strong>.</p>

<p>It has to be kept in mind that a resource should be completely described by the representation, since this is the only information the client will have. It has to be exaustive, but without exposing classified or useless information about the entity at the same time.</p>

<h2>Messages A.K.A. client and service chatting</h2>

<p>Q: So, how exactly do client and service exchange requests and responses?</p>

<p>A: They send messages.</p>

<p>In fact, to be more specific, the client will send an <strong>HTTP request</strong> to the service, specifying the following details:</p>

<ul>
<li>The <strong>method</strong> that is called on the resource. It can correspond to a <em>GET</em>, a <em>POST</em>, a <em>PUT</em>, a <em>DELETE</em>, an <em>OPTIONS</em> or a <em>HEAD</em> operation.</li>
<li>The <strong>URI</strong> of the request. It identifies what is the resource on which the client wants to use the method. More on that later. For now let&rsquo;s say it is the only way the client knows how to call the needed resource.</li>
<li>The <strong>HTTP version</strong>, which is usually <a href="http://tools.ietf.org/html/rfc2616"><em>HTTP/1.1</em></a>.</li>
<li>The <strong>request headers</strong>, which are the additional information passed, with the request, to the service. These fields are basically request modifiers, similar to the parameters sent to a programming language method, and they depend on the type of request sent. More on that later.</li>
<li>The <strong>request body</strong>: is the actual content of a message. In a RESTful service, it’s where the representation of resources sit. A body will not be present in a GET request, for instance, since it is a request to retrieve a resource rather than to create one, whereas a POST request will most likely have one.</li>
</ul>


<p>The request will then generate an <strong>HTTP response</strong> to the client, that will contain the following elements:</p>

<ul>
<li>The <strong>HTTP version</strong>, same as above.</li>
<li>The <strong>response code</strong>: which is a three-digit status code sent back to the client. Can be of the <strong>1xx</strong> format (informational), <strong>2xx</strong> (success), <strong>3xx</strong> (redirect), <strong>4xx</strong> (client error), <strong>5xx</strong> (server error).</li>
<li>The <strong>response header</strong>, which contains metadata and settings related to the message.</li>
<li>The <strong>response body</strong>: contains the representation (if the request was successful).</li>
</ul>


<h2>URIs, home of the resources</h2>

<p>A requirement of REST is that each resource has to correspond to an <a href="http://en.wikipedia.org/wiki/Uniform_resource_identifier">URI</a> address, which unsurprisingly stands for Uniform Resource Identifier. Having URIs associated to resources is key, because they are the addresses on which the client is allowed to perform the operations on the resources. It is important to stress that according to REST an URI should describe a resource, but never the operation performed on it.</p>

<p>The addresses are usually constructed hierarchically, to allow readability. A typical resource URL could be written as: <code>http://serviceName/resourceName/resourceID</code></p>

<p>Basic guidelines to build well-structured URIs are:</p>

<ul>
<li>Resources should be named with plural nouns, no verbs, using conventions throughout the whole service.</li>
<li>Query URIs <code>http://serviceName/resourceName?id=resourceID</code> should be used only when really necessary. They are not deprecated by REST style, but they are less readable than the normal URIs, and are ignored by search engines. On the upside, they allow the client to send parameters to the service, to refine the request for a specific subset of resources, or resources in a specific format.</li>
</ul>


<h2>Uniform interface, various operations</h2>

<p>Ok, so now that a client knows where a resource is reachable, how is it going to handle the resource? What are the operations that it can perform?</p>

<p>HTTP provides a set of methods that allow the client to perform standard operations on the service:</p>

<table style="margin-bottom:16px;">
    <tr>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Method</th>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Operation performed</th> 
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Quality</th>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">GET</td>
        <td style="padding:5px 10px;">Read a resource</td> 
        <td style="padding:5px 10px;">Safe</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">POST</td>
        <td style="padding:5px 10px;">Insert a new resource, or update an existing one</td> 
        <td style="padding:5px 10px;">Not idempotent</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">PUT</td>
        <td style="padding:5px 10px;">Insert a new resource, or update an existing one</td> 
        <td style="padding:5px 10px;">Idempotent (see below)</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">DELETE</td>
        <td style="padding:5px 10px;">Delete a resource</td> 
        <td style="padding:5px 10px;">Idempotent</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">OPTIONS</td>
        <td style="padding:5px 10px;">List allowed operations on a resource</td> 
        <td style="padding:5px 10px;">Safe</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">HEAD</td>
        <td style="padding:5px 10px;">Return only the response header, no body</td> 
        <td style="padding:5px 10px;">Safe</td>
    </tr>
</table>


<p>The key difference between <em>POST</em> and <em>PUT</em> is that no matter how many times a <em>PUT</em> operation is performed, the result will be the same (this is what <em>idempotent</em> means), whereas with a <em>POST</em> operation a resource will be added or updated multiple times.</p>

<p>Another difference is that a client that sends a <em>PUT</em> request always need to know the exact URI to operate on, I.E. assigning a name or an ID to a resource. If the client is not able to do so, it has no choice but to use a POST request.</p>

<p>Finally, if the resource already exists, <em>POST</em> and <em>PUT</em> will update it in an identical fashion.</p>

<p>These operations, according to REST, should be available to the client as hyperlinks to the above described URIs, and that is how the client/service interface is constrained to be <em>uniform</em>.</p>

<h2>Statelessness of the client side</h2>

<p>A RESTful service does not maintain the application state client-side. This only allows the client to perform requests that are resource specific, and does not allow the client to perform operations that assume prior knowledge of past requests. The client only knows what to do based on the ability to read the hypertext it receives, knowing its media type.</p>

<p>This leads me to mention an important constraint of REST, that was also <a href="http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven">enforced by Fielding</a> after publishing his dissertation: hyperlinks within hypertext are the only way for the client to make state transitions and perform operations on resources. This constraint is also known as <strong>HATEOAS</strong> (Hypermedia As The Engine Of Application State).</p>

<h2>Links between resources</h2>

<p>In the case of a resource that contains a list of resources, REST suggests to include links to the single resources on the representation, to keep it compact and avoid redundant data.</p>

<h2>Caching to optimize time and efficiency</h2>

<p>Allows to store responses and return them if the same request is performed again. It has to be handled carefully to avoid returning stale results. The headers that allow us to perform controls over caching are:</p>

<table style="margin-bottom:16px;">
    <tr>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Header</th>
        <th style="font-weight:bold;padding:5px 10px;border-bottom:1px solid #ccc;">Application</th>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Date</td>
        <td style="padding:5px 10px;">Finding out when this representation was generated</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Last Modified</td>
        <td style="padding:5px 10px;">Date and time when the server modified the representation</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Cache-Control</td>
        <td style="padding:5px 10px;">HTTP 1.1 header used to control caching, can contain directives</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Expires</td>
        <td style="padding:5px 10px;">Expiration date (supports HTTP 1.0)</td>
    </tr>
    <tr>
        <td style="padding:5px 10px;font-weight:bold;">Age</td>
        <td style="padding:5px 10px;">Duration since the resource was fetched from server</td>
    </tr>
</table>


<p>Cache-Control values can be tweaked to control if a cached result is still valid or stale. For example, the <em>max-age</em> value indicates for how many seconds from the moment expressed by the Date header a cached result will be valid.</p>

<h2>Conclusion</h2>

<p>REST is a language-agnostic style that abstracts over components and allows to build scalable, reusable and relatively lightweight web services. Thinking about it, it seems that REST is very close to an accurate description of the characteristics that made the World Wide Web so popular.</p>

<p>That of course is encouraging developers from all over the world to comply to these very basic ideas, owned by no one but at the same time used by everyone. Fascinating!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Internationalisation in a RESTful world]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/04/02/internationalisation-in-a-restful-world/"/>
    <updated>2014-04-02T14:11:00+01:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/04/02/internationalisation-in-a-restful-world</id>
    <content type="html"><![CDATA[<p>I18n is often a painful afterthought when serving content from a http-api. It can be taken for granted and tacked on using nasty query string values. But thankfully HTTP provides us with a solid gold opportunity. If you can look past the mire of content negotiation you can see the nuggets that lie inside.</p>

<p>The accept-language header is used by most browsers and allows websites to serve content in a language that the user can (hopefully) understand. When we expose content from an api (in most of our use cases, at least), this content eventually ends up in front of a human (in some shape or form). Having our service-service communication serve localised resources can be invaluable because it frees the clients from having to think about i18n of the resources being served from our api.</p>

<p>It is a simple part of the HTTP specification and is widely used and supported.</p>

<p><code>
GET /product/123
Accept-Language: en-US
</code></p>

<p>The accept-language header is specifically designed to allow the server to provide a representation of the resource which approximates something the client can understand.</p>

<p>The really useful bit comes from the quality value.</p>

<p><code>
GET /product/123
Accept-Language: en-US,en;q=0.8
</code></p>

<p>This header asks the service to provide en-US, and if it&rsquo;s unavailable then fall back to <strong>any</strong> english representation. The quality value (<code>q=0.8</code>) is a decimal value between 0 and 1 which indicates order of preference when specifying multiple languages. The server should pick the <strong>first</strong> available match. If there are multiple matches with the same quality value, then the server can pick any. If the client wants to specify some fierce preferences then they can crank out something like this:</p>

<p><code>
GET /product/123
Accept-Language: fr-CA,fr-FR;q=0.8,fr;q=0.6,en-US;q=0.4,en;q=0.2,*;q=0.1
</code></p>

<p>If you decipher this it&rsquo;s pretty simple, you can see the quality headers giving the order in which the languages are preferred. What it does is give the client fantastic flexibility. For service-service communication you might have a use-case which will <em>never</em> serve a representation that doesn&rsquo;t match the request, or you might need to <em>always</em> provide some representation (i.e. for cases where some content is always better than none).</p>

<p>The accept-language header gives you that flexibility. In my opinon, if your http-api&rsquo;s are serving content that <em>can</em> be internationalised, the server should always support this type of behaviour because it can shift the control from the server to the client. It allows the server&rsquo;s behaviour to be incredibly explicit and the clients get all that lovely flexibility.</p>

<p><strong>What happens when there is no matching representation?</strong></p>

<p>Well, the specification is (intentionally) vague. In other words, it is up to the server to decide. I myself always prefer to be explicit. Thankfully the HTTP specification provides for just such an eventuality.</p>

<p><a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html">HTTP 406 Not Acceptable</a> <em>&ldquo;The resource identified by the request is only capable of generating response entities which have content characteristics not acceptable according to the accept headers sent in the request.&rdquo;</em></p>

<p>The 406 response <em>should</em> contain a list of characteristics which the resource does support. In this case, a list of available languages. The specification does allow the server to automatically select a representation to return, however in my opinion, the server should be explicit, rather than implicit.</p>

<p>If the client has a use case where it <em>always</em> requires some sort of response (i.e. where any content is better than no content), then the client can append a wildcard to the end of the accept-language header, which will instruct the server to fall back to <em>any</em> language, in the event that there are none matching.</p>

<p><strong>Parsing the Accept-Language header</strong></p>

<p>I wrote a little npm module to help us in <a href="https://github.com/andyroyle/accept-language-parser">parsing the accept-language header</a>. Once you get past the (somewhat hairy) regex, it&rsquo;s a simple little bit of code. (Disclaimer, I&rsquo;m not a regex god, so there are a couple of little bugs in it).</p>

<p>Parsing an accept-language string such as <code>en-US,en;q=0.8</code> gives an object looking like this:</p>

<p>```
[
  {</p>

<pre><code>code: "en",
region: "GB",
quality: 1.0
</code></pre>

<p>  },
  {</p>

<pre><code>code: "en",
region: undefined,
quality: 0.8
</code></pre>

<p>  }
];
```</p>

<p>Output is always sorted in quality order from highest &ndash;> lowest. as per the http spec, omitting the quality value implies 1.0.</p>

<p>We can pass this around our application and use it to select the representation which best matches the client&rsquo;s request.</p>

<p><strong>Using it</strong></p>

<p>We use <a href="http://hapijs.com">hapi.js</a> for some of our api&rsquo;s (and I&rsquo;m very much in love), we use this module in a pre-requisite handler in our route:</p>

<p>```
var alparser = require(&lsquo;accept-language-parser&rsquo;);
server.route({
  method: &ldquo;GET&rdquo;,
  path: &ldquo;/v5/restaurants/{id}&rdquo;,
  config: {</p>

<pre><code>pre: [
  {
    method: function(req, next){
      next(alparser.parse(req.raw.req.headers["accept-language"] || ""));
    },
    assign: "language",
    mode: "parallel"
  }
],
handler: function(req, reply){
    ...
}
</code></pre>

<p>  }
});
```</p>

<p>For those of you that don&rsquo;t know, the prerequisites run before the handler, and assign their values to the request object. You can now get hold of the parsed language object here:</p>

<p><code>
req.pre.language
</code></p>

<p><strong>Content-Negotiation is hard</strong></p>

<p>Yes, it is. But suck it up. In my opinion the benefits outweigh the costs. Besides, the Accept-Language header is part of the HTTP specification and is well understood. If you have doubts, start small, and always try to be <em>explicit</em> rather than implicit.</p>

<p><strong>Gotchas</strong></p>

<p>Caching (both client-side and intermediate) can be picky. By default, most caches won&rsquo;t respect the header content (i.e. the resource is cached by url only). You can get around this by using vary-headers:</p>

<p><code>
GET /product/123
Accept-Language: en-GB,en;q=0.8
Vary: Accept-Language
</code></p>

<p>This instructs the cache that the response will vary with the value of Accept-Language, so when this changes it should be cached as a separate resource. Vary headers <strong>should</strong> be applied by the client to the request, however the server can apply them to the response if necessary.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Performance testing our Search API]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/03/19/performance-testing-our-search-api/"/>
    <updated>2014-03-19T15:53:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/03/19/performance-testing-our-search-api</id>
    <content type="html"><![CDATA[<p>It was midnight on the Friday before Christmas, my seven-week old child was tucked up asleep and I was pretty chilled. All was calm and it was time for bed. Two minutes later I had a phone call, followed by a series of Nagios email alerts, and a need to put my work hat on quickly. The search API was having trouble; as the manager of the team who developed it, I was not looking forward to this. We were having a real slowdown at one of our busiest times of the year &ndash; this was going to be fun.</p>

<p>Once the dust settled and we were back up and running we realised we needed to do a better job of performance testing and actually solve any performance issues. We <em>had</em> done some performance testing but clearly not the right kind.</p>

<h2>What had gone wrong?</h2>

<p>We were indexing our data too frequently, and under load it started to take a long time. We created a race condition where multiple indexing operations started happening. As each operation assumed the previous one had either failed or finished a vicious circle occurred making the situation worse and worse.</p>

<p>The simple fix was just to index the data less often or not at all if another operation was running, however we wanted to get to the bottom of why we slowed down under load which exposed this race condition, improve that speed and understand what is the maximum load the system can take.</p>

<h2>Getting a benchmark</h2>

<p>In order to know if we were actually making improvements we needed to be able to recreate load and see the impact. In order to do this and truly appreciate how it was going we needed to use our live environment &ndash; the only environment I felt we could truly understand. My next post will explain why we felt this was the best environment for the job.</p>

<p>With search and availability it is actually quite hard to use response times as a benchmark and the name of this exercise was really to cope with greater load. Response times are still nice to improve though so we were tracking them, but not using them as our main benchmark.</p>

<h2>Some perspective on our problem domain</h2>

<p>We do not have a large index, in the tens of thousands of restaurants rather than millions of tweets for example. We have to merge in table availability (the fantastic thing about the OpenTable system) to the more static restaurant information. We also have large page sizes, needing to get up to 2,500 results out of one Elastic Search request. This proved to be relevant.</p>

<p>We also have an autocomplete search endpoint served out of the same infrastructure. That was not heavy in terms of resource usage but we still noted it slowed down when peak load was happening.</p>

<h2>Initial ideas to investigate</h2>

<p>We brainstormed a few things to investigate and assess as potential performance improvements. As we were still relatively new to Elastic Search (ES), a lot of these were related to the configuration of ES itself.</p>

<ul>
<li>Improved sharding strategy (we were using the default)</li>
<li>Check we were <a href="http://elasticsearch-users.115913.n3.nabble.com/Elasticsearch-Filter-And-Query-td4027675.html">filtering before querying</a></li>
<li>Check the <a href="http://elasticsearch-users.115913.n3.nabble.com/Performance-of-term-query-with-sorting-td4032901.html">sorting was efficient</a></li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-optimize.html">Optimise the index</a> as part of the indexing process</li>
<li><a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-optimize.html">Slow log</a> checking</li>
<li>Check how we were using the <a href="http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-source-field.html">source</a> in the index</li>
<li>Warm-up the index after it is built</li>
<li>Gzip between API boxes and ES</li>
<li>Check connection pool is a singleton, internal detail</li>
<li>Delete old indexes more rigorously (we have a new index each release)</li>
<li>Check <a href="http://grokbase.com/t/gg/elasticsearch/13bezebw1p/garbage-collector-issues">garbage collection</a> on ES boxes</li>
<li>Check <a href="https://groups.google.com/forum/#!topic/logstash-users/gfTTbRABk1M">swappiness</a> settings on ES boxes</li>
<li>Split different search types to different hardware</li>
</ul>


<h2>Now, we learned, we were a bit stupid.</h2>

<p>Once we worked through a few things, our main findings showed that we had made some rookie errors.</p>

<p>We had a sharding policy that is great for large indexes but we only needed one shard replicated on each ES server in the cluster. Changing that helped reduce the load across servers as each server could do our queries entirely.</p>

<p>Through Elastic HQ (<a href="http://www.elastichq.org/">the useful plugin to ES</a>) we got the rather crude red boxes for various metrics on the diagnostics page. The two that stood out were high Garbage Collection and disk swaps. The main thing causing this was that we were using OpenJDK instead of the Oracle JVM. If you are using OpenJDK, change it now! The swappiness setting was less impactful but resolved some of the issues caused by the frequent writing to disk.</p>

<p>Now we increased scale well, until we moved our bottleneck from our Elastic Search cluster to our API boxes.</p>

<h2>And some things are limits of the technology</h2>

<p>Now the API boxes were the problem we realised that basically serialisation and deserialisation is the bottleneck. The number of boxes can be scaled out (more servers) or serialisation removed. Also your programming language can be the limiting factor here. So we are now looking around at the best language as an option for speeding things up.</p>

<h2>So what now</h2>

<p>We have tweaked the ES set-up and scaled out our API servers and our benchmark improved (the amount of traffic can we serve). We have a roadmap for how to take more and more traffic but the response time is probably as fast as we can go if we keep the same methodology. As a result we are actually looking at using Elastic Search a lot less than we originally planned. We need to take serialisation out of the game where possible, using in-memory filtering seems a candidate again.</p>

<h2>That&rsquo;s it for Elastic Search?</h2>

<p>Not at all, we love it. We are definitely still going to use for autocomplete, free-text search and also other indexing we want to do with future feature plans we have.</p>

<p>It was our fault we made some stupid errors with it, but our architecture and technology decisions just prevent us using it for our one, currently most important, use case, right now.</p>

<p>Also solving serialisation issues creatively might mean we can use ES again.</p>

<h2>Summary</h2>

<p>Get performance testing into your deployment pipeline, consider testing in production and expect the worst. The worst being that you might have got something stupid wrong and it is only exposed when you really don&rsquo;t want it to be.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking APIs - why it’s important, and how]]></title>
    <link href="http://tech.opentable.co.uk/blog/2014/02/28/api-benchmark/"/>
    <updated>2014-02-28T09:00:00+00:00</updated>
    <id>http://tech.opentable.co.uk/blog/2014/02/28/api-benchmark</id>
    <content type="html"><![CDATA[<p>Since I joined OpenTable I’ve been experimenting with performance monitoring, specifically on web services. One of the projects my team is responsible for is a REST API that provides UI elements for HTML5 applications, shaped as HTML snippets and static resources. Our consumers are websites deployed in multiple parts of the world, so our service needs to be fast and reliable.</p>

<h2>The why</h2>

<p>A couple of weeks after joining the company I decided, as part of my <a href="http://tech.opentable.co.uk/blog/2014/02/06/20-percent-time/">innovation time</a>, to rebuild the core of a .NET WebApi project in node.js in order to have a working prototype that could do exactly the same job as the original one, and could help me to observe how the two applications could react with similar volumes of traffic. After managing to make the two apis run on two clean VMs with the same configuration, I wrote a little node.js script to start performing some requests and test the response times. After seeing the results I thought that something was going wrong:</p>

<p>```sh
.NET/route1 x 9.16 ops/sec ±12.71% (17 runs sampled)</p>

<h1>nodeJS/route1 x 106 ops/sec ±1.19% (180 runs sampled)</h1>

<p>Fastest is nodeJS/route1</p>

<p>.NET/route2 x 10.70 ops/sec ±8.54% (19 runs sampled)</p>

<h1>nodeJS/route2 x 118 ops/sec ±1.22% (175 runs sampled)</h1>

<p>Fastest is nodeJS/route2</p>

<p>======================================
Fastest Service is nodeJS
```</p>

<p>After trying to microbenchmark different layers of the software, I found the problem. On the .NET side I was reading a file synchronously, for every request; the file system’s library used with the node.js app, instead, was automatically caching the reads as default. After setting up a basic caching mechanism in the .NET app and running my script again the node.js API was only 1.4 times faster. After finding and solving that issue I thought how badly the application could have handled concurrency when deployed in production, even if it was heavily unit tested, the specs were well defined, and it was built using all the best techniques we all love.</p>

<p>As developers we rely on technologies that, with a minimum effort, can guarantee some pretty decent results in terms of performance. Modern web frameworks handle concurrency and thread management without requiring much plumbing code. Sophisticated and relatively cheap cloud services help us monitor our applications, providing dashboards, reports and alerting systems. Deploying on the cloud we can run our services and even auto-scale them depending on how much power we need. Even with these tools we must still own the responsibility of writing good quality code, testing it properly, and deploying as fast as possible in order to optimise the delivery process of our products.</p>

<p>But what about performance? I mean, what about the relationship between the code we write every day, and the way we impact overall performance? Are we sure that we are not deploying to production something that is degrading our services’ performance?</p>

<h2>The how</h2>

<p>Talking about the ‘why’ could be relatively easy, but the ‘how’ is a controversial topic. In my experience there are three important steps to any dish. First, we need the right tools to manipulate the ingredients. I tried many different tools but I couldn&rsquo;t find a good fit for all my benchmarking requirements. It always makes sense to start with something to get you going but after a while it is important to find what’s the best for you, your team, and your project.</p>

<p>Then, you start cooking: personally, the number of things I’ve learned by just starting to benchmark some services is incredible. Nevertheless, as with every time we talk about metrics, it is key to know what is important about the data we are analysing, recognise the false positives/negatives and be aware of vanity metrics that could emphasize something irrelevant, or, more importantly, hide something significant.</p>

<p>The last step is simple: react and persist. If you discover something relevant, you can do something to improve the quality of your software.  With the right tools you can write some benchmark tests to target different layers of your software and execute them each and every time you contribute to that repository.  Doing this helps you keep your system performance under control which is really valuable.</p>

<h2>The how, for me</h2>

<p>When I found that little bug in the application (and it wasn’t actually a bug in the way we usually define them, as it wasn’t breaking any test or any software’s feature), I decided to spend some time to make my benchmark script better, in order to support different HTTP verbs, HTTPS, and a few things I needed to test all the routes of the API in a easy way. The goal was to wrap my little script as a <a href="http://www.gruntjs.com">grunt.js</a> module, (we use Teamcity as a CI platform and we already use grunt to run various tasks during our release process). I wanted to run this benchmarks externally to avoid interfering with the performance of the application, and to have a configuration-based simple-to-use tool.</p>

<p>So after some refactoring I started working on <a href="https://github.com/matteofigus/api-benchmark">api-benchmark</a> and its grunt wrapper <a href="https://github.com/matteofigus/grunt-api-benchmark">grunt-api-benchmark</a>, in order to make performance testing part of our continuous delivery process. A couple of days later my team was using it to run benchmark tests on our pre-prod environments against our APIs, running some hundreds of requests on each route every time we made a single commit to Github. What I managed to do was to break the build if response times were not good enough (stopping the production deployment), and producing a tiny report with some graphs, in order to have something useful to observe and eventually collect. Now, a couple of months later, a lot of functionality has been added and other teams are using it with success.</p>

<p>In case of RESTful services, it is possible to make series of requests to test response times, find peaks and classify errors; it is possible to perform concurrent calls to see how many parallel requests your service can handle (when deployed in single boxes, or when load balanced and globalised); and every time grunt runs everything is saved and plotted to readable and shareable graphs, so the knowledge can be shared between people that belong to different backgrounds.</p>

<p><img class="center" src="/images/posts/api_benchmark.png"></p>

<p>A lot of other features are still under development, including support for SOAP services and historical analysis (compare results from previous benchmarks and create historical graphs to represent the evolution of your software).</p>

<h2>How it works &amp; how to use it</h2>

<p>The way it works is simple. A configuration file contains the list of all the routes of the API. For each route it is possible to set different parameters such as headers, methods, expected status code, and expected response times. Other options that can be set such as the minimum number of samples, the maximum time for collecting the results, the number of concurrent requests, etc. It should be something like:</p>

<p>```json
{
  &ldquo;service&rdquo;: {</p>

<pre><code>"My api": "http://localhost:3007/api/"
</code></pre>

<p>  },
  &ldquo;endpoints&rdquo;: {</p>

<pre><code>"simpleRoute": "v1/getJson",
"postRoute": {
  "route": "v1/postJson",
  "method": "post",
  "data": {
    "test": true,
    "someData": "someStrings"
  },
  "expectedStatusCode": 200
},
"deleteRoute": {
  "route": "v1/deleteMe?test=true",
  "method": "delete",
  "maxMean": 0.06,
  "maxSingleMean": 0.003
}
</code></pre>

<p>  },
  &ldquo;options&rdquo;: {</p>

<pre><code>"minSamples": 1000,
"runMode": "parallel",
"maxConcurrentRequests": 20,
"debug": true,
"stopOnError": false
</code></pre>

<p>  }
}
```</p>

<p>Then it is possible to include the script in a project that is written in any language and runs on any platform. The only requirement is to <a href="http://www.nodejs.org">install node.js</a> on that machine. If the project doesn’t have a package.json file in the root of your project, it’s as easy as doing:</p>

<p><code>sh
$ npm init
</code></p>

<p>Once this is complete, include and install grunt-api-benchmark as a dependency (if you are not on Windows, you may want to sudo it depending on how you’ve installed node.js):</p>

<p><code>sh
$ npm install grunt-api-benchmark --save-dev
</code></p>

<p>The last thing to do is to create a task inside your Gruntfile.js. Create one if you already don’t have one, and then add the ‘api_benchmark’ task in order to have something like this:</p>

<p>```js
module.exports = function(grunt) {</p>

<p>  grunt.initConfig({</p>

<pre><code>pkg: grunt.file.readJSON('package.json'),
api_benchmark: {
  myApi: {
    options: {
      output: 'output_folder'
    },
    files: {
      'report.html': 'config.json',
      'export.json': 'config.json'
    }
  }
}
</code></pre>

<p>  });</p>

<p>  grunt.loadNpmTasks(&lsquo;grunt-api-benchmark&rsquo;);
  grunt.registerTask(&lsquo;benchmark&rsquo;, [&lsquo;api_benchmark&rsquo;]);
};
```</p>

<p>Where “generated” is the output folder, “config.json” is your configuration file, and “report.html” (or “export.json”) is the output’s filename. To run it just:</p>

<p><code>sh
$ grunt benchmark
</code></p>

<p>If you use TravisCI, TeamCity, or any other CI platform, all you’ll have to do is to make it run after being sure the dependencies are resolved:</p>

<p><code>sh
$ npm install
</code></p>

<h2>Let’s benchmark &ndash; some lessons learned</h2>

<p>I think this is the most important part of the whole process, however I don’t think there are any general rules that are applicable for every context. I believe that after testing and stressing your system you will find out what matters to you and to your business. Nevertheless, I want to share some of the lessons I’ve learned.</p>

<p>First, set-up everything correctly.</p>

<ul>
<li>Benchmarks need to run always on the same machine, same agent, and same configuration to be reliable and comparable.</li>
<li>The network should be tested to be sure there aren’t any particular limits that would affect the benchmarks. It should be tested each time before running any benchmarks and could include things like bandwidth, host name correctness, and OS limitations.</li>
<li>Don’t run the tests from the same machine that hosts the application. Run it from the outside, and if you deploy in different regions, keep that in mind when you look at the results.</li>
</ul>


<p>When you benchmark, remember that stress and performance are two different things.</p>

<ul>
<li>You should test both to learn about performance but also your limits, in order to have an idea on how to scale your application or how to fix it when necessary.</li>
<li>10 seconds is not enough. 1 minute is nice, 5 is better.</li>
<li>One route is not enough. Testing all the routes allow us to see the difference between different response lengths.</li>
<li>Sometimes your application needs a warm-up, especially if you test it after a deployment. Set up a script to do that or set a proper time-out to be sure you are retrieving some valuable numbers back.</li>
<li>Don&rsquo;t benchmark the live production environment. Your results are affected by too many variables. If possible, set-up a staging environment with exactly the same configuration to run benchmarks.</li>
</ul>


<p>If necessary, adapt your API to be more testable through some very basic design patterns.</p>

<ul>
<li>Performance could depend on synchronous calls to third-party APIs or databases. Ideally routes should have an optional parameter to mock external dependencies so we should test that as well.</li>
<li>Ensure that changes to data or the operating environment are not persisted after the benchmarks complete. This is important to ensure no side effects on subsequent runs and will allow you to  benchmark production boxes if needed (after the deployment and obviously before directing any traffic to them).</li>
</ul>


<p>Last but not least, let’s analyse the data</p>

<ul>
<li>Averages are not enough, peaks are important, investigate them.</li>
<li>When something unexpected happens, try to reproduce it in order to fix it.</li>
<li>If wildly different numbers come up every time you run the tests, your API is depending on too many unpredictable events. Try to fix it. Try to run benchmarks locally and microbenchmark your software until you find the element that is causing the unpredictability. Then, fix it or find a way to mock it if you have no other option.</li>
<li>Numbers should be readable and shareable by everyone. Find a tool that dashboards your results and easily allow you to share that data.</li>
</ul>


<h2>‘benchmarking’ != ‘monitoring’;</h2>

<p>Benchmarking doesn’t equal and doesn’t replace monitoring. Once you start having an extensive knowledge about your system’s performance, you can find useful and easy to establish correlations between your benchmarks and your monitoring metrics. Depending on the scale of your system, it could be something very important.</p>

<h2>Conclusions</h2>

<p>I believe that taking care of performance is our responsibility, as developers. We can and should do more, and I hope this subject will gain more interest. In the meanwhile, if <a href="https://github.com/matteofigus/api-benchmark">api-benchmark</a> sounds interesting for you and you are interested in trying it or contributing (it is totally open-source), don’t hesitate to <a href="http://www.twitter.com/matteofigus">get in touch with me</a>.</p>
]]></content>
  </entry>
  
</feed>
